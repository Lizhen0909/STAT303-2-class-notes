[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science II with python (Class notes)",
    "section": "",
    "text": "Preface\nThese are class notes for the course STAT303-2-Sec20&Sec21 in Winter 2025. This is not the course text-book. You are required to read the relevant sections of the book as mentioned on the course website.\nThis book serves as the course notes for [Course Name], and it is an evolving resource developed to support the learning objectives of the course. It builds upon the foundational work of the original iteration, authored and maintained by Professor Arvind Krishna. We are deeply grateful for Professor Krishna’s contributions, as his work has provided a robust framework and valuable content upon which this version of the book is based.\nAs the course progresses during this quarter, the notes will be continually updated and refined to reflect the content taught in real time. The modifications aim to enhance the clarity, depth, and relevance of the material to better align with the current teaching objectives and methodologies.\nThis book is a living document, and we welcome feedback, suggestions, and contributions from students, instructors, and the broader academic community to help improve its quality and utility.\nThank you for being part of this journey, and we hope this resource serves as a helpful guide throughout the course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "1.1 Simple Linear Regression\nRead section 3.1 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nfrom matplotlib.lines import Line2D\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nDevelop a simple linear regression model that predicts car price based on engine size. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n# We are reading training data ONLY at this point.\n# Test data is already separated in another file\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv') # Predictors\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv') # Response\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "href": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "title": "1  Simple Linear Regression",
    "section": "",
    "text": "1.1.1 Training with statsmodels\nHere, we will use the statsmodels.formula.api module of the statsmodels library. The use of “API” here doesn’t refer to a traditional external web API but rather an interface within the library for users to interact with and perform specific tasks. The statsmodels.formula.api module provides a formulaic interface to the statsmodels library. A formula is a compact way to specify statistical models using a formula language. This module allows users to define statistical models using formulas similar to those used in R.\nSo, in summary, the statsmodels.formula.api module provides a formulaic interface as part of the statsmodels library, allowing users to specify statistical models using a convenient and concise formula syntax.\n\n# Let's create the model\n    \n# ols stands for Ordinary Least Squares - the name of the algorithm that optimizes Linear Regression models\n\n# data input needs the dataframe that has the predictor and the response\n# formula input needs to:\n    # be a string\n    # have the following syntax: \"response~predictor\"\n    \n# Using engineSize to predict price\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model, i.e., train the model\nmodel = ols_object.fit()\n\n\n#Printing model summary which contains among other things, the model coefficients\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.390\n\n\nModel:\nOLS\nAdj. R-squared:\n0.390\n\n\nMethod:\nLeast Squares\nF-statistic:\n3177.\n\n\nDate:\nTue, 16 Jan 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n16:46:33\nLog-Likelihood:\n-53949.\n\n\nNo. Observations:\n4960\nAIC:\n1.079e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.079e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-4122.0357\n522.260\n-7.893\n0.000\n-5145.896\n-3098.176\n\n\nengineSize\n1.299e+04\n230.450\n56.361\n0.000\n1.25e+04\n1.34e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1271.986\nDurbin-Watson:\n0.517\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n6490.719\n\n\nSkew:\n1.137\nProb(JB):\n0.00\n\n\nKurtosis:\n8.122\nCond. No.\n7.64\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation is: \\(\\hat{price}\\) = -4122.0357 + 12990 * engineSize\n\nR-squared is 39%. This is the proportion of variance in car price explained by engineSize.\nThe coef of engineSize (\\(\\hat{\\beta}_1\\)) is statistically significant (\\(p\\)-value = 0). There is a linear relationship between X and Y.\nThe 95% CI of \\(\\hat{\\beta}_1\\) is [1.25e+04, 1.34e+04].\nPI is not shown here.\n\nThe coefficient of engineSize is 1.299e+04. - Unit change in engineSize increases the expected price by \\(\\$\\) 12,990. - An increase of 3 increases the price by \\(\\$\\) (3*1.299e+04) = \\(\\$\\) 38,970.\nThe coefficients can also be returned directly usign the params attribute of the model object returned by the fit() method of the ols class:\n\nmodel.params\n\nIntercept     -4122.035744\nengineSize    12988.281021\ndtype: float64\n\n\nVisualize the regression line\n\nsns.set(font_scale=1.25)\nax = sns.scatterplot(x = train.engineSize, y = train.price,color = 'orange')\nsns.lineplot(x = train.engineSize, y = model.fittedvalues,color = 'blue')\nplt.xlim(-1,7)\nplt.xlabel('Engine size (in litres)')\nplt.ylabel('Car price')\nlegend_elements = [Line2D([0], [0], color='blue', lw=4, label='Predicted (Model)'),\n                   Line2D([0], [0], marker='o', color='w', label='Actual',\n                          markerfacecolor='orange', markersize=10)]\nax.legend(handles=legend_elements, loc='upper left');\n\n\n\n\n\n\n\n\nNote that the above plot can be made directly using the seaborn function regplot(). The function regplot() fits a simple linear regression model with y as the response, and x as the predictor, and then plots the model over a scatteplot of the data.\n\nax = sns.regplot(x = 'engineSize', y = 'price', data = train, color = 'orange',line_kws={\"color\": \"blue\"})\nplt.xlim(-1,7)\nplt.xlabel('Engine size (in litres)')\nplt.ylabel('Car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.legend(handles=legend_elements, loc='upper left');\n#Note that some of the engineSize values are 0. They are incorrect, and should ideally be imputed before developing the model.\n\n\n\n\n\n\n\n\nThe light shaded region around the blue line in the above plot is the confidence interval.\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\nNow that the model has been trained, let us evaluate it on unseen data. Make sure that the columns names of the predictors are the same in train and test datasets.\n\n# Read the test data\ntestf = pd.read_csv('./Datasets/Car_features_test.csv') # Predictors\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv') # Response\ntest = pd.merge(testf, testp)\n\n\n#Using the predict() function associated with the 'model' object to make predictions of car price on test (unknown) data\npred_price = model.predict(testf)#Note that the predict() function finds the predictor 'engineSize' in the testf dataframe, and plugs its values in the regression equation for prediction.\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.scatterplot(x = testp.price, y = pred_price, color = 'orange')\n#In case of a perfect prediction, all the points must lie on the line x = y.\nax = sns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='blue') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\nplt.xticks(rotation=20);\n\n\n\n\n\n\n\n\nThe prediction doesn’t look too good. This is because we are just using one predictor - engine size. We can probably improve the model by adding more predictors when we learn multiple linear regression.\nWhat is the RMSE of the predicted car price on unseen data?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n12995.106451548696\n\n\nThe root mean squared error in predicting car price is around $13k.\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n12810.109175214138\n\n\nThe residual standard error on the training data is close to the RMSE on the test data. This shows that the performance of the model on unknown data is comparable to its performance on known data. This implies that the model is not overfitting, which is good! In case we overfit a model on the training data, its performance on unknown data is likely to be worse than that on the training data.\nFind the confidence and prediction intervals of the predicted car price\n\n#Using the get_prediction() function associated with the 'model' object to get the intervals\nintervals = model.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n1\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n2\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n3\n8866.245277\n316.580850\n8245.606701\n9486.883853\n-16254.905974\n33987.396528\n\n\n4\n47831.088340\n468.949360\n46911.740050\n48750.436631\n22700.782946\n72961.393735\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2667\n47831.088340\n468.949360\n46911.740050\n48750.436631\n22700.782946\n72961.393735\n\n\n2668\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n2669\n8866.245277\n316.580850\n8245.606701\n9486.883853\n-16254.905974\n33987.396528\n\n\n2670\n21854.526298\n184.135754\n21493.538727\n22215.513869\n-3261.551421\n46970.604017\n\n\n2671\n21854.526298\n184.135754\n21493.538727\n22215.513869\n-3261.551421\n46970.604017\n\n\n\n\n2672 rows × 6 columns\n\n\n\nShow the regression line predicting car price based on engine size for test data. Also show the confidence and prediction intervals for the car price.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nax = sns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\n\nlegend_elements = [Line2D([0], [0], color='red', label='Mean prediction'),\n                   Line2D([0], [0], color='blue', label='Confidence interval'),\n                  Line2D([0], [0], color='green', label='Prediction interval')]\nax.legend(handles=legend_elements, loc='upper left')\nplt.xlabel('Engine size (in litres)')\nplt.ylabel('Car price')\nax.yaxis.set_major_formatter('${x:,.0f}');\n\n\n\n\n\n\n\n\n\n\n1.1.2 Training with sklearn\n\n# Create the model as an object\n\nmodel = LinearRegression() # No inputs, this will change for other models\n\n# Train the model - separate the predictor(s) and the response for this!\nX_train = train[['engineSize']]\ny_train = train[['price']]\n\n# Note that both are dfs, NOT series - necessary to avoid errors\n\nmodel.fit(X_train, y_train)\n\n# Check the slight syntax differences\n    # predictors and response separate\n    # We need to manually slice the predictor column(s) we want to include\n    # No need to assign to an output\n    \n# Return the parameters\nprint(\"Coefficient of engine size = \", model.coef_) # slope\nprint(\"Intercept = \", model.intercept_) # intercept\n\n# No .summary() here! - impossible to do much inference; this is a shortcoming of sklearn\n\nCoefficient of engine size =  [[12988.28102112]]\nIntercept =  [-4122.03574424]\n\n\n\n# Prediction\n\n# Again, separate the predictor(s) and the response of interest\nX_test = test[['engineSize']]\ny_test = test[['price']].to_numpy() # Easier to handle with calculations as np array\n\ny_pred = model.predict(X_test)\n\n# Evaluate\nmodel_rmse = np.sqrt(np.mean((y_pred - y_test)**2)) # RMSE\nmodel_mae = np.mean(np.abs(y_pred - y_test)) # MAE\n\nprint('Test RMSE: ', model_rmse)\n\nTest RMSE:  12995.106451548696\n\n\n\n# Easier way to calculate metrics with sklearn tools\n\n# Note that we have imported the functions 'mean_squared_error' and 'mean_absolute_error'\n# from the sklearn.metrics module (check top of the code)\n\nmodel_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\nmodel_mae = mean_absolute_error(y_test,y_pred)\nprint('Test RMSE: ', model_rmse)\nprint('Test MAE: ', model_mae)\n\nTest RMSE:  12995.106451548696\nTest MAE:  9411.325912951994\n\n\n\ny_pred_train = model.predict(X_train)\nprint('Train R-squared:', r2_score(y_train, y_pred_train))\nprint('Test R-squared:', r2_score(y_test, y_pred))\n\nTrain R-squared: 0.39049842625794573\nTest R-squared: 0.3869900378620146\n\n\nNote: Why did we repeat the same task in two different libraries?\n\nstatsmodels and sklearn have different advantages - we will use both for our purposes\n\nstatsmodels returns a lot of statistical output, which is very helpful for inference (coming up next) but it has a limited variety of models.\nWith statsmodels, you may have columns in your DataFrame in addition to predictors and response, while with sklearn you need to make separate objects consisting of only the predictors and the response.\nsklearn includes many models (Lasso and Ridge this quarter, many others next quarter) and helpful tools/functions (like metrics) that statsmodels does not but it does not have any inference tools.\n\n\n\n\n1.1.3 Training with statsmodels.api\nEarlier we had used the statsmodels.formula.api module, where we had to put the regression model as a formula. We can also use the statsmodels.api module to develop a regression model. The syntax of training a model with the OLS() function in this module is similar to that of sklearn’s LinearRegression() function. However, the order in which the predictors and response are specified is different. The formula-style syntax of the statsmodels.formula.api module is generally preferred. However, depending on the situation, the OLS() syntax of statsmodels.api may be preferred.\nNote that you will manually need to add the predictor (a column of ones) corresponding to the intercept to train the model with this method.\n\n# Create the model as an object\n\n# Train the model - separate the predictor(s) and the response for this!\nX_train = train[['engineSize']]\ny_train = train[['price']]\n\nX_train_with_intercept = np.concatenate((np.ones(X_train.shape[0]).reshape(-1,1), X_train), axis = 1)\n\nmodel = sm.OLS(y_train, X_train_with_intercept).fit()\n    \n# Return the parameters\nprint(model.params) \n\nconst    -4122.035744\nx1       12988.281021\ndtype: float64\n\n\nThe model summary and all other attributes and methods of the model object are the same as that with the object created using the statsmodels.formula.api module.\n\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.390\n\n\nModel:\nOLS\nAdj. R-squared:\n0.390\n\n\nMethod:\nLeast Squares\nF-statistic:\n3177.\n\n\nDate:\nMon, 08 Jan 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n11:17:55\nLog-Likelihood:\n-53949.\n\n\nNo. Observations:\n4960\nAIC:\n1.079e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.079e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-4122.0357\n522.260\n-7.893\n0.000\n-5145.896\n-3098.176\n\n\nx1\n1.299e+04\n230.450\n56.361\n0.000\n1.25e+04\n1.34e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1271.986\nDurbin-Watson:\n0.517\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n6490.719\n\n\nSkew:\n1.137\nProb(JB):\n0.00\n\n\nKurtosis:\n8.122\nCond. No.\n7.64\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "2.1 Multiple Linear Regression\nRead section 3.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\n# importing libraries \nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nDevelop a multiple linear regression model that predicts car price based on engine size, year, mileage, and mpg. Datasets to be used: Car_features_train.csv, Car_prices_train.csv\n# Reading datasets\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf,trainp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "href": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "title": "2  Multiple Linear Regression",
    "section": "",
    "text": "2.1.1 Training the model\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nMon, 29 Jan 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n03:10:20\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.661e+06\n1.49e+05\n-24.593\n0.000\n-3.95e+06\n-3.37e+06\n\n\nyear\n1817.7366\n73.751\n24.647\n0.000\n1673.151\n1962.322\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe model equation is: estimated car price = -3.661e6 + 1818 * year -0.15 * mileage - 79.31 * mpg + 12180 * engineSize\nThe procedure to fit the model using sklearn will be similar to that in simple linear regression.\n\nmodel = LinearRegression()\n\nX_train = train[['year','engineSize','mpg','mileage']] # Slice out the predictors\ny_train = train[['price']]\n\nmodel.fit(X_train,y_train)\n\n\n\n2.1.2 Hypothesis test for a relationship between the response and a subset of predictors\nLet us test the hypothesis if there is relationship between car price and the set of predictors: mpg and year.\n\nhypothesis = '(mpg = 0, year = 0)'     \n    \nmodel.f_test(hypothesis) # the F test of these two predictors is stat. sig.\n\n&lt;class 'statsmodels.stats.contrast.ContrastResults'&gt;\n&lt;F test: F=325.9206432972666, p=1.0499509223096256e-133, df_denom=4.96e+03, df_num=2&gt;\n\n\nAs the \\(p\\)-value is low, we reject the null hypothesis, i.e., at least one of the predictors among mpg and year has a statistically significant relationship with car price.\nPredict the car price for the cars in the test dataset. Datasets to be used: Car_features_test.csv, Car_prices_test.csv\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\n\n\n2.1.3 Prediction\n\npred_price = model.predict(testf)\n\nMake a visualization that compares the predicted car prices with the actual car prices\n\nsns.set(font_scale=1.25)\nsns.scatterplot(x = testp.price, y = pred_price, color = 'orange')\n#In case of a perfect prediction, all the points must lie on the line x = y.\nax = sns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='blue') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\nplt.ylim([-10000, 160000])\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\nplt.xticks(rotation=20);\n\n\n\n\n\n\n\n\nThe prediction looks better as compared to the one with simple linear regression. This is because we have four predictors to help explain the variation in car price, instead of just one in the case of simple linear regression. Also, all the predictors have a significant relationship with price as evident from their p-values. Thus, all four of them are contributing in explaining the variation. Note the higher values of \\(R^2\\) as compared to the one in the case of simple linear regression.\nWhat is the RMSE of the predicted car price?\n\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9956.82497993548\n\n\nWhat is the residual standard error based on the training data?\n\nnp.sqrt(model.mse_resid)\n\n9563.74782917604\n\n\n\ntrainp.describe()\n\n\n\n\n\n\n\n\ncarID\nprice\n\n\n\n\ncount\n4960.000000\n4960.000000\n\n\nmean\n15832.446169\n23469.943750\n\n\nstd\n2206.717006\n16406.714563\n\n\nmin\n12002.000000\n450.000000\n\n\n25%\n13929.250000\n12000.000000\n\n\n50%\n15840.000000\n18999.000000\n\n\n75%\n17765.750000\n30335.750000\n\n\nmax\n19629.000000\n145000.000000\n\n\n\n\n\n\n\n\nsns.scatterplot(x = model.fittedvalues, y=model.resid,color = 'orange')\nax = sns.lineplot(x = [pred_price.min(),pred_price.max()],y = [0,0],color = 'blue')\nplt.xlabel('Predicted price')\nplt.ylabel('Residual')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\nplt.xticks(rotation=20);\n\n\n\n\n\n\n\n\n\n\n2.1.4 Effect of adding noisy predictors on \\(R^2\\)\nWill the explained variation (R-squared) in car price always increase if we add a variable?\nShould we keep on adding variables as long as the explained variation (R-squared) is increasing?\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nnp.random.seed(1)\ntrain['rand_col'] = np.random.rand(train.shape[0])\nols_object = smf.ols(formula = 'price~year+mileage+mpg+engineSize+rand_col', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.661\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n1928.\n\n\nDate:\nTue, 27 Dec 2022\nProb (F-statistic):\n0.00\n\n\nTime:\n01:07:38\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4954\nBIC:\n1.050e+05\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.662e+06\n1.49e+05\n-24.600\n0.000\n-3.95e+06\n-3.37e+06\n\n\nyear\n1818.1672\n73.753\n24.652\n0.000\n1673.578\n1962.756\n\n\nmileage\n-0.1474\n0.009\n-16.809\n0.000\n-0.165\n-0.130\n\n\nmpg\n-79.2837\n9.338\n-8.490\n0.000\n-97.591\n-60.976\n\n\nengineSize\n1.218e+04\n189.972\n64.109\n0.000\n1.18e+04\n1.26e+04\n\n\nrand_col\n451.1226\n471.897\n0.956\n0.339\n-474.004\n1376.249\n\n\n\n\n\n\n\n\nOmnibus:\n2451.728\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31040.331\n\n\nSkew:\n2.046\nProb(JB):\n0.00\n\n\nKurtosis:\n14.552\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nAdding a variable with random values to the model (rand_col) increased the explained variation (\\(R^2\\)). This is because the model has one more parameter to tune to reduce the residual squared error \\((RSS)\\). However, the \\(p\\)-value of rand_col suggests that its coefficient is zero. Thus, using the model with rand_col may give poorer performance on unknown data, as compared to the model without rand_col. This implies that it is not a good idea to blindly add variables in the model to increase \\(R^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html",
    "href": "Lec3_VariableTransformations_and_Interactions.html",
    "title": "3  Extending Linear Regression (statsmodels)",
    "section": "",
    "text": "3.1 Variable interactions\nRead sections 3.3.1 and 3.3.2 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntrain = pd.merge(trainf,trainp)\ntest = pd.merge(testf,testp)\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\nUntil now, we have assumed that the association between a predictor \\(X_j\\) and response \\(Y\\) does not depend on the value of other predictors. For example, the multiple linear regression model that we developed in Chapter 2 assumes that the average increase in price associated with a unit increase in engineSize is always $12,180, regardless of the value of other predictors. However, this assumption may be incorrect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending Linear Regression (statsmodels)</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-interactions",
    "title": "3  Extending Linear Regression (statsmodels)",
    "section": "",
    "text": "3.1.1 Variable interaction between continuous predictors\nWe can relax this assumption by considering another predictor, called an interaction term. Let us assume that the average increase in price associated with a one-unit increase in engineSize depends on the model year of the car. In other words, there is an interaction between engineSize and year. This interaction can be included as a predictor, which is the product of engineSize and year. Note that there are several possible interactions that we can consider. Here the interaction between engineSize and year is just an example.\n\n#Considering interaction between engineSize and year\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg', data = train)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.682\n\n\nModel:\nOLS\nAdj. R-squared:\n0.681\n\n\nMethod:\nLeast Squares\nF-statistic:\n2121.\n\n\nDate:\nTue, 24 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n15:28:11\nLog-Likelihood:\n-52338.\n\n\nNo. Observations:\n4960\nAIC:\n1.047e+05\n\n\nDf Residuals:\n4954\nBIC:\n1.047e+05\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n5.606e+05\n2.74e+05\n2.048\n0.041\n2.4e+04\n1.1e+06\n\n\nyear\n-275.3833\n135.695\n-2.029\n0.042\n-541.405\n-9.361\n\n\nengineSize\n-1.796e+06\n9.97e+04\n-18.019\n0.000\n-1.99e+06\n-1.6e+06\n\n\nyear:engineSize\n896.7687\n49.431\n18.142\n0.000\n799.861\n993.676\n\n\nmileage\n-0.1525\n0.008\n-17.954\n0.000\n-0.169\n-0.136\n\n\nmpg\n-84.3417\n9.048\n-9.322\n0.000\n-102.079\n-66.604\n\n\n\n\n\n\n\n\nOmnibus:\n2330.413\nDurbin-Watson:\n0.524\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n29977.437\n\n\nSkew:\n1.908\nProb(JB):\n0.00\n\n\nKurtosis:\n14.423\nCond. No.\n7.66e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.66e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model in Chapter 2 since we added a predictor.\nThe model equation is:\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\beta_2*engineSize + \\beta_3*(year * engineSize) + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + (\\beta_2+\\beta_3*year)*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]or\n\\[\\begin{equation}\nprice = \\beta_0 + \\beta_1*year + \\tilde \\beta*engineSize + \\beta4*mileage + \\beta_5*mpg,\n\\end{equation}\\]\nSince \\(\\tilde \\beta\\) is a function of year, the association between engineSize and price is no longer a constant. A change in the value of year will change the association between price and engineSize.\nSubstituting the values of the coefficients:\nprice = 5.606e5 - 275.3833year + (-1.796e6+896.7687year)engineSize -0.1525mileage -84.3417mpg\nThus, for cars launched in the year 2010, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687 * 2010 \\(\\approx\\) \\$6,500, assuming all the other predictors are constant. However, for cars launched in the year 2020, the average increase in price for one liter increase in engine size is -1.796e6 + 896.7687*2020 \\(\\approx\\) \\$15,500 , assuming all the other predictors are constant.\nSimilarly, the equation can be re-arranged as:\nprice = 5.606e5 +(-275.3833+896.7687engineSize)year -1.796e6engineSize -0.1525mileage -84.3417*mpg\nThus, for cars with an engine size of 2 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 2 \\(\\approx\\) \\$1500, assuming all the other predictors are constant. However, for cars with an engine size of 3 litres, the average increase in price for a one year newer model is -275.3833+896.7687 * 3 \\(\\approx\\) \\$2400, assuming all the other predictors are constant.\n\n#Computing the RMSE of the model with the interaction term\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9423.598872501092\n\n\nNote that the RMSE is lower than that of the model in Chapter 2. This is because the interaction term between engineSize and year is significant and relaxes the assumption of constant association between price and engine size, and between price and year. This added flexibility makes the model better fit the data. Caution: Too much flexibility may lead to overfitting!\nNote that interaction terms corresponding to other variable pairs, and higher order interaction terms (such as those containing 3 or 4 variables) may also be significant and improve the model fit & thereby the prediction accuracy of the model.\n\n\n3.1.2 Including qualitative predictors in the model\nLet us develop a model for predicting price based on engineSize and the qualitative predictor transmission.\n\n#checking the distribution of values of transmission\ntrain.transmission.value_counts()\n\nManual       1948\nAutomatic    1660\nSemi-Auto    1351\nOther           1\nName: transmission, dtype: int64\n\n\nNote that the Other category of the variable transmission contains only a single observation, which is likely to be insufficient to train the model. We’ll remove that observation from the training data. Another option may be to combine the observation in the Other category with the nearest category, and keep it in the data.\n\ntrain_updated = train[train.transmission!='Other']\n\n\nols_object = smf.ols(formula = 'price ~ engineSize + transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.459\n\n\nModel:\nOLS\nAdj. R-squared:\n0.458\n\n\nMethod:\nLeast Squares\nF-statistic:\n1400.\n\n\nDate:\nTue, 24 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n15:28:21\nLog-Likelihood:\n-53644.\n\n\nNo. Observations:\n4959\nAIC:\n1.073e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.073e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3042.6765\n661.190\n4.602\n0.000\n1746.451\n4338.902\n\n\ntransmission[T.Manual]\n-6770.6165\n442.116\n-15.314\n0.000\n-7637.360\n-5903.873\n\n\ntransmission[T.Semi-Auto]\n4994.3112\n442.989\n11.274\n0.000\n4125.857\n5862.765\n\n\nengineSize\n1.023e+04\n247.485\n41.323\n0.000\n9741.581\n1.07e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1575.518\nDurbin-Watson:\n0.579\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n11006.609\n\n\nSkew:\n1.334\nProb(JB):\n0.00\n\n\nKurtosis:\n9.793\nCond. No.\n11.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that there is no coefficient for the Automatic level of the variable Transmission. If a car doesn’t have Manual or Semi-Automatic transmission, then it has an Automatic transmission. Thus, the coefficient of Automatic will be redundant, and the dummy variable corresponding to Automatic transmission is dropped from the model.\nThe level of the categorical variable that is dropped from the model is called the baseline level. Here Automatic transmission is the baseline level. The coefficients of other levels of transmission should be interpreted with respect to the baseline level.\nQ: Interpret the intercept term\nAns: For the hypothetical scenario of a car with zero engine size and Automatic transmission, the estimated mean car price is \\(\\approx\\) \\$3042.\nQ: Interpret the coefficient of transmission[T.Manual]\nAns: The estimated mean price of a car with manual transmission is \\(\\approx\\) \\$6770 less than that of a car with Automatic transmission.\nLet us visualize the developed model.\n\n#Visualizing the developed model\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], color = 'red')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], color = 'blue')\nsns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept']+model.params['transmission[T.Manual]'], color = 'green')\nplt.legend(labels=[\"Automatic\",\"Semi-Automatic\", \"Manual\"])\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nBased on the developed model, for a given engine size, the car with a semi-automatic transmission is estimated to be the most expensive on average, while the car with a manual transmission is estimated to be the least expensive on average.\nChanging the baseline level: By default, the baseline level is chosen as the one that comes first if the levels are arranged in alphabetical order. However, you can change the baseline level by specifying one explicitly.\nInternally, statsmodels uses the patsy package to convert formulas and data to the matrices that are used in model fitting. You may refer to this section in the patsy documentation to specify a particular level of the categorical variable as the baseline.\nFor example, suppose we wish to change the baseline level to Manual transmission. We can specify this in the formula as follows:\n\nols_object = smf.ols(formula = 'price~engineSize+C(transmission, Treatment(\"Manual\"))', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.459\n\n\nModel:\nOLS\nAdj. R-squared:\n0.458\n\n\nMethod:\nLeast Squares\nF-statistic:\n1400.\n\n\nDate:\nTue, 24 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n15:28:39\nLog-Likelihood:\n-53644.\n\n\nNo. Observations:\n4959\nAIC:\n1.073e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.073e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3727.9400\n492.917\n-7.563\n0.000\n-4694.275\n-2761.605\n\n\nC(transmission, Treatment(\"Manual\"))[T.Automatic]\n6770.6165\n442.116\n15.314\n0.000\n5903.873\n7637.360\n\n\nC(transmission, Treatment(\"Manual\"))[T.Semi-Auto]\n1.176e+04\n473.110\n24.867\n0.000\n1.08e+04\n1.27e+04\n\n\nengineSize\n1.023e+04\n247.485\n41.323\n0.000\n9741.581\n1.07e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1575.518\nDurbin-Watson:\n0.579\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n11006.609\n\n\nSkew:\n1.334\nProb(JB):\n0.00\n\n\nKurtosis:\n9.793\nCond. No.\n8.62\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n3.1.3 Including qualitative predictors and their interaction with continuous predictors in the model\nNote that the qualitative predictor leads to fitting 3 parallel lines to the data, as there are 3 categories.\nHowever, note that we have made the constant association assumption. The fact that the lines are parallel means that the average increase in car price for one litre increase in engine size does not depend on the type of transmission. This represents a potentially serious limitation of the model, since in fact a change in engine size may have a very different association on the price of an automatic car versus a semi-automatic or manual car.\nThis limitation can be addressed by adding an interaction variable, which is the product of engineSize and the dummy variables for semi-automatic and manual transmissions.\n\n#Using the ols function to create an ols object. 'ols' stands for 'Ordinary least squares'\nols_object = smf.ols(formula = 'price~engineSize*transmission', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.479\n\n\nModel:\nOLS\nAdj. R-squared:\n0.478\n\n\nMethod:\nLeast Squares\nF-statistic:\n909.9\n\n\nDate:\nSun, 22 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n22:55:55\nLog-Likelihood:\n-53550.\n\n\nNo. Observations:\n4959\nAIC:\n1.071e+05\n\n\nDf Residuals:\n4953\nBIC:\n1.072e+05\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3754.7238\n895.221\n4.194\n0.000\n1999.695\n5509.753\n\n\ntransmission[T.Manual]\n1768.5856\n1294.071\n1.367\n0.172\n-768.366\n4305.538\n\n\ntransmission[T.Semi-Auto]\n-5282.7164\n1416.472\n-3.729\n0.000\n-8059.628\n-2505.805\n\n\nengineSize\n9928.6082\n354.511\n28.006\n0.000\n9233.610\n1.06e+04\n\n\nengineSize:transmission[T.Manual]\n-5285.9059\n646.175\n-8.180\n0.000\n-6552.695\n-4019.117\n\n\nengineSize:transmission[T.Semi-Auto]\n4162.2428\n552.597\n7.532\n0.000\n3078.908\n5245.578\n\n\n\n\n\n\n\n\nOmnibus:\n1379.846\nDurbin-Watson:\n0.622\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n9799.471\n\n\nSkew:\n1.139\nProb(JB):\n0.00\n\n\nKurtosis:\n9.499\nCond. No.\n30.8\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe model equation for the model with interactions is:\nAutomatic transmission: price = 3754.7238 + 9928.6082 * engineSize,\nSemi-Automatic transmission: price = 3754.7238 + 9928.6082 * engineSize + (-5282.7164+4162.2428*engineSize),\n\nManual transmission: price = 3754.7238 + 9928.6082 * engineSize +(1768.5856-5285.9059 * engineSize),\nor\nAutomatic transmission: price = 3754.7238 + 9928.6082 * engineSize,\nSemi-Automatic transmission: price = -1527 + 7046 * engineSize,\nManual transmission: price = 5523 + 4642 * engineSize\nQ: Interpret the coefficient of manual tranmission, i.e., the coefficient of transmission[T.Manual].\nA: For a hypothetical scenario of zero engine size, the estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$1768 more than the estimated mean price of a car with Automatic transmission.\nQ: Interpret the coefficient of the interaction between engine size and manual transmission, i.e., the coefficient of engineSize:transmission[T.Manual].\nA: For a unit (or a litre) increase in engineSize , the increase in estimated mean price of a car with Manual transmission is \\(\\approx\\) \\$5285 less than the increase in estimated mean price of a car with Automatic transmission.\n\n#Visualizing the developed model with interaction terms\nplt.rcParams[\"figure.figsize\"] = (9,6)\nsns.set(font_scale = 1.3)\nx = np.linspace(train_updated.engineSize.min(),train_updated.engineSize.max(),100)\nax = sns.lineplot(x = x, y = model.params['engineSize']*x+model.params['Intercept'], label='Automatic', color = 'red')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Semi-Auto]'])*x+model.params['Intercept']+model.params['transmission[T.Semi-Auto]'], '-b', label='Semi-Automatic')\nplt.plot(x, (model.params['engineSize']+model.params['engineSize:transmission[T.Manual]'])*x+model.params['Intercept']+model.params['transmission[T.Manual]'], '-g', label='Manual')\nplt.legend(loc='upper left')\nplt.xlabel('Engine size (in litre)')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\n\n\n\n\n\n\n\n\nNote the interaction term adds flexibility to the model.\nThe slope of the regression line for semi-automatic cars is the largest. This suggests that increase in engine size is associated with a higher increase in car price for semi-automatic cars, as compared to other cars.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending Linear Regression (statsmodels)</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "href": "Lec3_VariableTransformations_and_Interactions.html#variable-transformations",
    "title": "3  Extending Linear Regression (statsmodels)",
    "section": "3.2 Variable transformations",
    "text": "3.2 Variable transformations\nSo far we have considered only a linear relationship between the predictors and the response. However, the relationship may be non-linear.\nConsider the regression plot of price on mileage.\n\nax = sns.regplot(x = train_updated.mileage, y =train_updated.price,color = 'orange', line_kws = {'color':'blue'})\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\n\n#R-squared of the model with just mileage\nmodel = smf.ols('price~mileage', data = train_updated).fit()\nmodel.rsquared\n\n0.22928048993376182\n\n\nFrom the first scatterplot, we see that the relationship between price and mileage doesn’t seem to be linear, as the points do not lie on a straight line. Also, we see the regression line (or the curve), which is the best fit line doesn’t seem to fit the points well. However, price on average seems to decrease with mileage, albeit in a non-linear manner.\n\n3.2.1 Quadratic transformation\nSo, we guess that if we model price as a quadratic function of mileage, the model may better fit the points (or the curve may better fit the points). Let us transform the predictor mileage to include \\(mileage^2\\) (i.e., perform a quadratic transformation on the predictor).\n\n#Including mileage squared as a predictor and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.271\n\n\nModel:\nOLS\nAdj. R-squared:\n0.271\n\n\nMethod:\nLeast Squares\nF-statistic:\n920.6\n\n\nDate:\nSun, 22 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:26:05\nLog-Likelihood:\n-54382.\n\n\nNo. Observations:\n4959\nAIC:\n1.088e+05\n\n\nDf Residuals:\n4956\nBIC:\n1.088e+05\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.44e+04\n332.710\n103.382\n0.000\n3.37e+04\n3.5e+04\n\n\nmileage\n-0.5662\n0.017\n-33.940\n0.000\n-0.599\n-0.534\n\n\nI(mileage ** 2)\n2.629e-06\n1.56e-07\n16.813\n0.000\n2.32e-06\n2.94e-06\n\n\n\n\n\n\n\n\nOmnibus:\n2362.973\nDurbin-Watson:\n0.325\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n22427.952\n\n\nSkew:\n2.052\nProb(JB):\n0.00\n\n\nKurtosis:\n12.576\nCond. No.\n4.81e+09\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.81e+09. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that in the formula specified within the ols() function, the I() operator isolates or insulates the contents within I(…) from the regular formula operators. Without the I() operator, mileage**2 will be treated as the interaction of mileage with itself, which is mileage. Thus, to add the square of mileage as a separate predictor, we need to use the I() operator.\nLet us visualize the model fit with the quadratic transformation of the predictor - mileage.\n\n#Visualizing the regression line with the model consisting of the quadratic transformation of the predictor - mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nThe above model seems to better fit the data (as compared to the model without transformation) at least upto mileage around 125,000. The \\(R^2\\) of the model with the quadratic transformation of mileage is also higher than that of the model without transformation indicating a better fit.\n\n\n3.2.2 Cubic transformation\nLet us see if a cubic transformation of mileage can further improve the model fit.\n\n#Including mileage squared and mileage cube as predictors and developing the model\nols_object = smf.ols(formula = 'price~mileage+I(mileage**2)+I(mileage**3)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.283\n\n\nModel:\nOLS\nAdj. R-squared:\n0.283\n\n\nMethod:\nLeast Squares\nF-statistic:\n652.3\n\n\nDate:\nSun, 22 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:33:27\nLog-Likelihood:\n-54340.\n\n\nNo. Observations:\n4959\nAIC:\n1.087e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.087e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3.598e+04\n371.926\n96.727\n0.000\n3.52e+04\n3.67e+04\n\n\nmileage\n-0.7742\n0.028\n-27.634\n0.000\n-0.829\n-0.719\n\n\nI(mileage ** 2)\n6.875e-06\n4.87e-07\n14.119\n0.000\n5.92e-06\n7.83e-06\n\n\nI(mileage ** 3)\n-1.823e-11\n1.98e-12\n-9.199\n0.000\n-2.21e-11\n-1.43e-11\n\n\n\n\n\n\n\n\nOmnibus:\n2380.788\nDurbin-Watson:\n0.321\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n23039.307\n\n\nSkew:\n2.065\nProb(JB):\n0.00\n\n\nKurtosis:\n12.719\nCond. No.\n7.73e+14\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.73e+14. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Visualizing the model with the cubic transformation of mileage\npred_price = model.predict(train_updated)\nax = sns.scatterplot(x = 'mileage', y = 'price', data = train_updated, color = 'orange')\nsns.lineplot(x = train_updated.mileage, y = pred_price, color = 'blue')\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nNote that the model fit with the cubic transformation of mileage seems slightly better as compared to the models with the quadratic transformation, and no transformation of mileage, for mileage up to 180k. However, the model should not be used to predict car prices of cars with a mileage higher than 180k.\nLet’s update the model created earlier (in the beginning of this chapter) to include the transformed predictor.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'price~year*engineSize+mileage+mpg+I(mileage**2)', data = train_updated)\nmodel = ols_object.fit()\nmodel.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.702\n\n\nModel:\nOLS\nAdj. R-squared:\n0.702\n\n\nMethod:\nLeast Squares\nF-statistic:\n1947.\n\n\nDate:\nSun, 22 Jan 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n23:42:13\nLog-Likelihood:\n-52162.\n\n\nNo. Observations:\n4959\nAIC:\n1.043e+05\n\n\nDf Residuals:\n4952\nBIC:\n1.044e+05\n\n\nDf Model:\n6\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.53e+06\n2.7e+05\n5.671\n0.000\n1e+06\n2.06e+06\n\n\nyear\n-755.7419\n133.791\n-5.649\n0.000\n-1018.031\n-493.453\n\n\nengineSize\n-2.022e+06\n9.72e+04\n-20.803\n0.000\n-2.21e+06\n-1.83e+06\n\n\nyear:engineSize\n1008.6993\n48.196\n20.929\n0.000\n914.215\n1103.184\n\n\nmileage\n-0.3548\n0.014\n-25.973\n0.000\n-0.382\n-0.328\n\n\nmpg\n-54.7450\n8.896\n-6.154\n0.000\n-72.185\n-37.305\n\n\nI(mileage ** 2)\n1.926e-06\n1.04e-07\n18.536\n0.000\n1.72e-06\n2.13e-06\n\n\n\n\n\n\n\n\nOmnibus:\n2355.448\nDurbin-Watson:\n0.562\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n38317.404\n\n\nSkew:\n1.857\nProb(JB):\n0.00\n\n\nKurtosis:\n16.101\nCond. No.\n6.40e+12\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 6.4e+12. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the R-squared has increased as compared to the model with just the interaction term.\n\n#Computing RMSE on test data\npred_price = model.predict(testf)\nnp.sqrt(((testp.price - pred_price)**2).mean())\n\n9074.494088619422\n\n\nNote that the prediction accuracy of the model has further increased, as the RMSE has reduced. The transformed predictor is statistically significant and provides additional flexibility to better capture the trend in the data, leading to an increase in prediction accuracy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending Linear Regression (statsmodels)</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#polynomialfeatures",
    "href": "Lec3_VariableTransformations_and_Interactions.html#polynomialfeatures",
    "title": "3  Extending Linear Regression (statsmodels)",
    "section": "3.3 PolynomialFeatures()",
    "text": "3.3 PolynomialFeatures()\nThe function PolynomialFeatures() from the sklearn library can be used to generate a predictor matrix that includes all interactions and transformations upto a degree d.\n\nX_train = train[['mileage', 'engineSize', 'year', 'mpg']]\ny_train = train[['price']]\nX_test = test[['mileage', 'engineSize', 'year', 'mpg']]\ny_test = test[['price']]\n\n\n3.3.1 Generating polynomial features\nLet us generate polynomial features upto degree 2. This will include all the two-factor interactions, and all squared terms of degree 2.\n\npoly = PolynomialFeatures(2, include_bias = False) # Create the object - degree is 2\n\n# Generate the polynomial features\nX_train_poly = poly.fit_transform(X_train) \n\nNote that the LinearRegression() function adds the intercept by default (check the fit_intercept argument). Thus, we have put include_bias = False while generating the polynomial features, as we don’t need the intercept. The term bias here refers to the intercept (you will learn about bias in detail in STAT303-3). Another option is to include the intercept while generating the polynomial features, and put fit_intercept = False in the LinearRegression() function.\nBelow are the polynomial features generated by the PolynomialFeatures() functions.\n\npoly.get_feature_names_out()\n\narray(['mileage', 'engineSize', 'year', 'mpg', 'mileage^2',\n       'mileage engineSize', 'mileage year', 'mileage mpg',\n       'engineSize^2', 'engineSize year', 'engineSize mpg', 'year^2',\n       'year mpg', 'mpg^2'], dtype=object)\n\n\n\n\n3.3.2 Fitting the model\n\nmodel = LinearRegression() \nmodel.fit(X_train_poly, y_train) \n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n3.3.3 Testing the model\n\nX_test_poly = poly.fit_transform(X_test)\n\n\n#RMSE\nnp.sqrt(mean_squared_error(y_test, model.predict(X_test_poly)))\n\n8896.175508213777\n\n\nNote that the polynomial features have helped reduced the RMSE further.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Extending Linear Regression (statsmodels)</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html",
    "href": "polynominal_features.html",
    "title": "4  Extending Linear Regression (PolynomialFeatures in Sklearn)",
    "section": "",
    "text": "4.0.1 Simulate Data\nimport numpy as np\nimport pandas as pd\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Number of samples\nN = 5000\n\n# Generate features from uniform distributions\nx1 = np.random.uniform(-5, 5, N)\nx2 = np.random.uniform(-5, 5, N)\n\n# Define the nonlinear relationship and add noise\ny = 1.5 * (x1 ** 2) + 0.5 * (x2 ** 3) + np.random.normal(loc=3, scale=3, size=N)\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\n\n# Save to CSV (optional)\ndf.to_csv('nonlinear_dataset.csv', index=False)\n\ndf.head(10)  # Display the first 10 rows\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-1.254599\n-1.063645\n0.295770\n\n\n1\n4.507143\n-0.265643\n30.086577\n\n\n2\n2.319939\n3.545474\n34.523622\n\n\n3\n0.986585\n-1.599956\n-1.109427\n\n\n4\n-3.439814\n3.696497\n49.341010\n\n\n5\n-3.440055\n-4.118656\n-14.395442\n\n\n6\n-4.419164\n2.767984\n43.154083\n\n\n7\n3.661761\n3.475476\n43.267654\n\n\n8\n1.011150\n-3.181823\n-9.254211\n\n\n9\n2.080726\n-0.696535\n11.674644\n# Create X and y arrays\nX = df[['x1', 'x2']].values\ny = df['y'].values",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Extending Linear Regression (PolynomialFeatures in Sklearn)</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html#key-takeaway",
    "href": "polynominal_features.html#key-takeaway",
    "title": "4  Extending Linear Regression (PolynomialFeatures in Sklearn)",
    "section": "4.1 Key takeaway:",
    "text": "4.1 Key takeaway:\nIn scikit-learn, the built-in PolynomialFeatures transformer is somewhat “all or nothing”: by default, it generates all polynomial terms (including interactions) up to a certain degree. You can toggle:\n\ninteraction_only=True to generate only cross-terms\ninclude_bias=False to exclude the constant (bias) term,\ndegree to control how high the polynomial powers go.\n\nHowever, if you want fine-grained control over exactly which terms get generated (for example, only certain interaction terms, or only a subset of polynomial terms), you will need to create those features manually or write a custom transformer (skipped for beginner level)\nUse interaction_only for Cross Terms Only\nIf your goal is only to capture interaction terms (i.e., $ x_1 x_2 $, but no squares, cubes, etc.), you can set:\n\npoly_int = PolynomialFeatures(degree=6, \n                             interaction_only=True, \n                             include_bias=False)\n\nX_transformed = poly_int.fit_transform(X)\n\nprint(\"\\nTransformed Feature Names:\")\nprint(poly_int.get_feature_names_out())\n\n\nTransformed Feature Names:\n['x0' 'x1' 'x0 x1']\n\n\nIf you want to be very selective—say, just add \\(x_1^2\\) and $ x_1 x_2 $ but not \\(x_2^2\\) —the simplest approach is to create columns by hand. For example:\n\nimport numpy as np\n\nX1 = X[:, 0].reshape(-1, 1)  # feature 1\nX2 = X[:, 1].reshape(-1, 1)  # feature 2\n\n# Manually create specific transformations\nX1_sq = X1**2\nX1X2 = X1 * X2\n\n# Combine them as you like\nX_new = np.hstack([X1, X2, X1_sq, X1X2])\n\nprint(\"\\nTransformed Feature Names:\")\nprint(['x1', 'x2', 'x1^2', 'x1*x2'])\n\nX_new[:5]  # Display the first 5 rows\n\n\nTransformed Feature Names:\n['x1', 'x2', 'x1^2', 'x1*x2']\n\n\narray([[ -1.25459881,  -1.0636448 ,   1.57401818,   1.3344475 ],\n       [  4.50714306,  -0.26564341,  20.3143386 ,  -1.19729284],\n       [  2.31993942,   3.54547393,   5.3821189 ,   8.22528473],\n       [  0.98658484,  -1.59995614,   0.97334965,  -1.57849247],\n       [ -3.4398136 ,   3.69649685,  11.83231757, -12.71526011]])\n\n\nWhen using PolynomialFeatures (or any other scikit-learn transformer), the fitting step is always done on the training data—not on the test data. This is a fundamental principle of machine learning pipelines: we do not use the test set for any part of model training (including feature encoding, feature generation, scaling, etc.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Extending Linear Regression (PolynomialFeatures in Sklearn)</span>"
    ]
  },
  {
    "objectID": "potential_issue_3.html",
    "href": "potential_issue_3.html",
    "title": "5  Beyond Fit (implementation)",
    "section": "",
    "text": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n# Load the Boston Housing dataset (for demonstration purposes)\ndf = pd.read_csv('datasets/Housing.csv')\ndf.head()\n\n\n\n\n\n\n\n\nprice\narea\nbedrooms\nbathrooms\nstories\nmainroad\nguestroom\nbasement\nhotwaterheating\nairconditioning\nparking\nprefarea\nfurnishingstatus\n\n\n\n\n0\n13300000\n7420\n4\n2\n3\nyes\nno\nno\nno\nyes\n2\nyes\nfurnished\n\n\n1\n12250000\n8960\n4\n4\n4\nyes\nno\nno\nno\nyes\n3\nno\nfurnished\n\n\n2\n12250000\n9960\n3\n2\n2\nyes\nno\nyes\nno\nno\n2\nyes\nsemi-furnished\n\n\n3\n12215000\n7500\n4\n2\n2\nyes\nno\nyes\nno\nyes\n3\nyes\nfurnished\n\n\n4\n11410000\n7420\n4\n1\n2\nyes\nyes\nyes\nno\nyes\n2\nno\nfurnished\n\n\n\n\n\n\n\n\n# build a formular api model using price as the target, the rest of the variables as predictors\nmodel = smf.ols('price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement + hotwaterheating + airconditioning + parking + prefarea + furnishingstatus', data=df)\nmodel = model.fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.682\nModel:                            OLS   Adj. R-squared:                  0.674\nMethod:                 Least Squares   F-statistic:                     87.52\nDate:                Wed, 05 Feb 2025   Prob (F-statistic):          9.07e-123\nTime:                        08:46:02   Log-Likelihood:                -8331.5\nNo. Observations:                 545   AIC:                         1.669e+04\nDf Residuals:                     531   BIC:                         1.675e+04\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n======================================================================================================\n                                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------\nIntercept                           4.277e+04   2.64e+05      0.162      0.872   -4.76e+05    5.62e+05\nmainroad[T.yes]                     4.213e+05   1.42e+05      2.962      0.003    1.42e+05    7.01e+05\nguestroom[T.yes]                    3.005e+05   1.32e+05      2.282      0.023    4.18e+04    5.59e+05\nbasement[T.yes]                     3.501e+05    1.1e+05      3.175      0.002    1.33e+05    5.67e+05\nhotwaterheating[T.yes]              8.554e+05   2.23e+05      3.833      0.000    4.17e+05    1.29e+06\nairconditioning[T.yes]               8.65e+05   1.08e+05      7.983      0.000    6.52e+05    1.08e+06\nprefarea[T.yes]                     6.515e+05   1.16e+05      5.632      0.000    4.24e+05    8.79e+05\nfurnishingstatus[T.semi-furnished] -4.634e+04   1.17e+05     -0.398      0.691   -2.75e+05    1.83e+05\nfurnishingstatus[T.unfurnished]    -4.112e+05   1.26e+05     -3.258      0.001   -6.59e+05   -1.63e+05\narea                                 244.1394     24.289     10.052      0.000     196.425     291.853\nbedrooms                            1.148e+05   7.26e+04      1.581      0.114   -2.78e+04    2.57e+05\nbathrooms                           9.877e+05   1.03e+05      9.555      0.000    7.85e+05    1.19e+06\nstories                             4.508e+05   6.42e+04      7.026      0.000    3.25e+05    5.77e+05\nparking                             2.771e+05   5.85e+04      4.735      0.000    1.62e+05    3.92e+05\n==============================================================================\nOmnibus:                       97.909   Durbin-Watson:                   1.209\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              258.281\nSkew:                           0.895   Prob(JB):                     8.22e-57\nKurtosis:                       5.859   Cond. No.                     3.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using studentized residuals)\n# ------------------------------------------\n# Outliers can be detected using studentized residuals\noutliers_studentized = model.get_influence().resid_studentized_external\noutlier_threshold = 3  # Common threshold for studentized residuals\n\n\n# Plot studentized residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(outliers_studentized)), outliers_studentized, alpha=0.7)\nplt.axhline(y=outlier_threshold, color='r', linestyle='--', label='Outlier Threshold')\nplt.axhline(y=-outlier_threshold, color='r', linestyle='--')\nplt.title('Studentized Residuals for Outlier Detection')\nplt.xlabel('Observation Index')\nplt.ylabel('Studentized Residuals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Identify observations with high studentized residuals\noutlier_indices_studentized = np.where(np.abs(outliers_studentized) &gt; outlier_threshold)[0]\nprint(f\"Outliers detected at indices: {outlier_indices_studentized}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using standardized residuals)\n# ------------------------------------------\n# Outliers can be detected using standardized  residuals\noutliers_standardized = model.get_influence().resid_studentized_internal\noutlier_threshold = 3  # Common threshold for standardized residuals\n\n\n# Identify observations with high standardized residuals\noutlier_indices_standardized = np.where(np.abs(outliers_standardized) &gt; outlier_threshold)[0]\nprint(f\"Outliers detected at indices: {outlier_indices_standardized}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# Plot studentized residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(outliers_standardized)), outliers_standardized, alpha=0.7)\nplt.axhline(y=outlier_threshold, color='r', linestyle='--', label='Outlier Threshold')\nplt.axhline(y=-outlier_threshold, color='r', linestyle='--')\nplt.title('standardized Residuals for Outlier Detection')\nplt.xlabel('Observation Index')\nplt.ylabel('standardized Residuals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using boxplot)\n# ------------------------------------------\n# Outliers can be detected using boxplot of standardized residuals\nplt.figure(figsize=(10, 6))\nsns.boxplot(model.resid)\nplt.title('Boxplot of Standardized Residuals');\n\n\n\n\n\n\n\n\n\n# use 3 standard deviation rule to identify outliers\noutlier_indices = np.where(np.abs(model.resid) &gt; 3 * model.resid.std())[0]\nprint(f\"Outliers detected at indices: {outlier_indices}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# ------------------------------------------\n# 2. Identifying High Leverage Points\n# ------------------------------------------\n# High leverage points can be detected using the hat matrix (leverage values)\nleverage = model.get_influence().hat_matrix_diag\nleverage_threshold = 2 * (df.shape[1] / df.shape[0])  # Common threshold for leverage\n\n\n5.0.0.1 Identifying High Leverage Points\nA common threshold for identifying high leverage points in regression analysis is:\n\\(h_i &gt; \\frac{2p}{n}\\)\nwhere:\n- \\(h_i\\) is the leverage value for the ( i )-th observation,\n- \\(p\\) is the number of predictors (including the intercept), and\n- \\(n\\) is the total number of observations.\n\n# Plot leverage values\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(leverage)), leverage, alpha=0.7)\nplt.axhline(y=leverage_threshold, color='r', linestyle='--', label='Leverage Threshold')\nplt.title('Leverage Values for High Leverage Points')\nplt.xlabel('Observation Index')\nplt.ylabel('Leverage')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Identify observations with high leverage\nhigh_leverage_indices = np.where(leverage &gt; leverage_threshold)[0]\nprint(f\"High leverage points detected at indices: {high_leverage_indices}\")\n\nHigh leverage points detected at indices: [  1   5   7  11  13  20  28  36  66  73  74  75  80  84  89 109 112 125\n 143 165 196 247 270 298 321 334 350 356 363 364 378 395 403 464 490 499\n 530]\n\n\n\n# ------------------------------------------\n# 3. Cook's Distance for Influential Observations\n# ------------------------------------------\n# Cook's distance measures the influence of each observation on the model\ncooks_distance = model.get_influence().cooks_distance[0]\n\n# Plot Cook's distance\nplt.figure(figsize=(10, 6))\nplt.stem(range(len(cooks_distance)), cooks_distance, markerfmt=\",\")\nplt.title(\"Cook's Distance for Influential Observations\")\nplt.xlabel('Observation Index')\nplt.ylabel(\"Cook's Distance\")\nplt.show()\n\n\n\n\n\n\n\n\nCook’s distance is considered high if it is greater than 0.5 and extreme if it is greater than 1.\n\n# Identify influential observations\ninfluential_threshold = 4 / (df.shape[1] - 1 ) # Common threshold for Cook's distance\ninfluential_indices = np.where(cooks_distance &gt; influential_threshold)[0]\nprint(f\"Influential observations detected at indices: {influential_indices}\")\n\nInfluential observations detected at indices: []\n\n\n\n# =======================================\n# 4. Checking Multicollinearity (VIF)\n# =======================================\n# VIF calculation\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif(X):\n    vif_data = pd.DataFrame()\n    vif_data[\"Variable\"] = X.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return vif_data\n\nX = df[['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']]\n\n# one-hot encoding for categorical variables\nX = pd.get_dummies(X, drop_first=True, dtype=float)\n\n\nvif_data = calculate_vif(X)\nprint(\"\\nVariance Inflation Factors:\")\nprint(vif_data.sort_values('VIF', ascending=False))\n\n\nVariance Inflation Factors:\n                           Variable        VIF\n1                          bedrooms  16.652387\n2                         bathrooms   9.417643\n0                              area   8.276447\n3                           stories   7.880730\n5                      mainroad_yes   6.884806\n11  furnishingstatus_semi-furnished   2.386831\n7                      basement_yes   2.019858\n12     furnishingstatus_unfurnished   2.008632\n4                           parking   1.986400\n9               airconditioning_yes   1.767753\n10                     prefarea_yes   1.494211\n6                     guestroom_yes   1.473234\n8               hotwaterheating_yes   1.091568\n\n\n\n# Rule of thumb: VIF &gt; 10 indicates significant multicollinearity\nmulticollinear_features = vif_data[vif_data['VIF'] &gt; 10]['Variable']\nprint(f\"Features with significant multicollinearity: {multicollinear_features.tolist()}\")\n\nFeatures with significant multicollinearity: ['bedrooms']\n\n\n\n# =======================================\n# 4. Checking Multicollinearity (Correlation Matrix)\n# =======================================\n# Correlation matrix\ncorrelation_matrix = X.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix');\n\n\n\n\n\n\n\n\n\n#output the correlation of other predictors with the bedrooms\nX.corr()['bedrooms'].abs().sort_values(ascending=False)\n\nbedrooms                           1.000000\nstories                            0.408564\nbathrooms                          0.373930\nairconditioning_yes                0.160603\narea                               0.151858\nparking                            0.139270\nfurnishingstatus_unfurnished       0.126252\nbasement_yes                       0.097312\nguestroom_yes                      0.080549\nprefarea_yes                       0.079023\nfurnishingstatus_semi-furnished    0.050040\nhotwaterheating_yes                0.046049\nmainroad_yes                       0.012033\nName: bedrooms, dtype: float64\n\n\n\n# ------------------------------------------\n# 5. Analyzing Residual Patterns\n# ------------------------------------------\n# Residuals vs Fitted Values Plot\nfitted_values = model.fittedvalues\nresiduals = model.resid\n\nplt.figure(figsize=(10, 6))\nsns.residplot(x=fitted_values, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 1})\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals');\n\n# QQ Plot for Normality of Residuals\nplt.figure(figsize=(10, 6))\nsm.qqplot(residuals, line='s')\nplt.title('QQ Plot of Residuals');\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.histplot(residuals, kde=True, bins=20)\nplt.axvline(residuals.mean(), color='red', linestyle='--', label=\"Mean Residual\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Density\")\nplt.title(\"Histogram of Residuals\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Beyond Fit (implementation)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html",
    "href": "Lec5_Potential_issues.html",
    "title": "6  Beyond Fit (statistical theory)",
    "section": "",
    "text": "6.1 Outliers\nRead section 3.3.3 (4, 5, & 6) of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLet us continue with the car price prediction example from the previous chapter.\nAn outlier is a point for which the true response (\\(y_i\\)) is far from the value predicted by the model. Residual plots can be used to identify outliers.\nIf the the response at the \\(i^{th}\\) observation is \\(y_i\\), the prediction is \\(\\hat{y}_i\\), then the residual \\(e_i\\) is:\n\\[e_i = y_i - \\hat{y_i}\\]\n#Plotting residuals vs fitted values\nsns.set(rc={'figure.figsize':(10,6)})\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals');\nSome of the errors may be high. However, it is difficult to decide how large a residual needs to be before we can consider a point to be an outlier. To address this problem, we have standardized residuals, which are defined as:\n\\[r_i = \\frac{e_i}{RSE(\\sqrt{1-h_{ii}})},\\] where \\(r_i\\) is the standardized residual, \\(RSE\\) is the residual standard error, and \\(h_{ii}\\) is the leverage (introduced in the next section) of the \\(i^{th}\\) observation.\nStandardized residuals, allow the residuals to be compared on a standard scale.\nIssue with standardized residuals:, If the observation corresponding to the standardized residual has a high leverage, then it will drag the regression line / plane / hyperplane towards it, thereby influencing the estimate of the residual itself.\nStudentized residuals: To address the issue with standardized residuals, studentized residual for the \\(i^{th}\\) observation is computed as the standardized residual, but with the \\(RSE\\) (residual standard error) computed after removing the \\(i^{th}\\) observation from the data. Studentized residual, \\(t_i\\) for the \\(i^{th}\\) observation is given as:\n\\[t_i = \\frac{e_i}{RSE_{i}(\\sqrt{1-h_{ii}})},\\] where \\(RSE_{i}\\) is the residual standard error of the model developed on the data without the \\(i^{th}\\) observation.\nDistribution of studentized residuals: If the regression model is appropriate such that no case is outlying because of a change in the model, then each studentized residual will follow a \\(t\\) distribution with \\((n–p–1)\\) degrees of freedom.\nAs the studentized residuals follow a \\(t\\) distribution, we can conduct a hypothesis test to identify whether an observation is an outlier or not for a given sigificance level. Note that the test will be two-sided since we are not concerned with the sign of the residuals, but only their absolute values.\nIn the current example, for a signficance level of 5%, the critical \\(t\\)-statistic is \\(t\\big(1 - \\frac{\\alpha}{2}, n - p - 1\\big)\\), as calculated below.\nn = train.shape[0]\np = model_log.df_model\nalpha = 0.05\n\n# Critical value\nstats.t.ppf(1 - alpha/2, n - p - 1)\n\n1.9604435402730618\nIf we were conducting the test for a single observation, we’ll compare the studentized residual for that observation with the critical \\(t\\)-statistic, and if the residual is greater than the critical value, we’ll consider that observation as an outlier.\nHowever, typically, we’ll be interested in conducting this test for all observations, and thus we’ll need a more conservative critical value for the same signficance level. This critical value is given by the Bonferroni correction as \\(t\\big(1 - \\frac{\\alpha}{2n}, n - p - 1\\big)\\).\nThus, the minimum value of studentized residual for which the observation will be classified as an outlier is:\ncritical_value = stats.t.ppf(1-alpha/(2*n), n - p - 1)\ncritical_value\n\n4.4200129981725365\nThe studentized residuals can be obtained using the outlier_test() method of the object returned by the fit() method of an OLS object. Let us find the studentized residuals in our car price prediction model.\n#Studentized residuals\nout = model_log.outlier_test()\nout\n\n\n\n\n\n\n\n\nstudent_resid\nunadj_p\nbonf(p)\n\n\n\n\n0\n-1.164204\n0.244398\n1.0\n\n\n1\n-0.801879\n0.422661\n1.0\n\n\n2\n-1.263820\n0.206354\n1.0\n\n\n3\n-0.614171\n0.539131\n1.0\n\n\n4\n0.027929\n0.977720\n1.0\n\n\n...\n...\n...\n...\n\n\n4955\n-0.523361\n0.600747\n1.0\n\n\n4956\n-0.509538\n0.610398\n1.0\n\n\n4957\n-1.718808\n0.085712\n1.0\n\n\n4958\n-0.077594\n0.938154\n1.0\n\n\n4959\n-0.482388\n0.629551\n1.0\n\n\n\n\n4960 rows × 3 columns\nStudentized residuals are in the first column of the above table. Let us plot the studentized residuals against fitted values. In the figure below, the studentized residuals above the top dotted green line and below the bottom dotted green line are outliers.\n#Plotting studentized residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nax = sns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [critical_value, critical_value],\n             color = 'green')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [-critical_value, -critical_value],\n             color = 'green')\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n\nplt.xlabel('Fitted values')\nplt.ylabel('Studentized Residuals');\nOutliers: Observations whose studentized residuals have a magnitude greater than \\(t\\big(1 - \\frac{\\alpha}{2n}, n - p - 1\\big)\\).\nImpact of outliers: Outliers do not have a large impact on the OLS line / plane / hyperplane as long as they don’t have a high leverage (discussed in the next section). However, outliers do inflate the residual standard error (RSE). RSE in turn is used to compute the standard errors of regression coefficients. As a result, statistically significant variables may appear to be insignificant, and \\(R^2\\) may appear to be lower.\nAre there outliers in our example?\n#Number of points with absolute studentized residuals greater than critical_value\nnp.sum(np.abs(out.student_resid) &gt; critical_value)\n\n19\nLet us analyze the outliers.\nind = (np.abs(out.student_resid) &gt; critical_value)\npd.concat([train.loc[ind,:], np.exp(model_log.fittedvalues[ind])], axis = 1)\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n0\n\n\n\n\n2042\n18228\nbmw\ni3\n2017\nAutomatic\n24041\nHybrid\n0\n78.2726\n0.0\n21495\n5537.337460\n\n\n2046\n17362\nbmw\ni3\n2016\nAutomatic\n68000\nHybrid\n0\n78.0258\n0.0\n15990\n4107.811771\n\n\n2050\n19224\nbmw\ni3\n2016\nAutomatic\n20013\nHybrid\n0\n77.9310\n0.0\n19875\n4784.986021\n\n\n2051\n13913\nbmw\ni3\n2014\nAutomatic\n34539\nHybrid\n0\n78.3838\n0.0\n14495\n3269.686113\n\n\n2055\n16512\nbmw\ni3\n2017\nAutomatic\n28169\nHybrid\n0\n77.9799\n0.0\n23751\n5454.207333\n\n\n2059\n15844\nbmw\ni3\n2016\nAutomatic\n19995\nHybrid\n0\n78.2825\n0.0\n19850\n4773.707307\n\n\n2060\n12107\nbmw\ni3\n2016\nAutomatic\n8421\nHybrid\n0\n77.9125\n0.0\n19490\n5028.048305\n\n\n2061\n18215\nbmw\ni3\n2014\nAutomatic\n37161\nHybrid\n0\n77.7505\n0.0\n14182\n3259.101789\n\n\n2063\n15617\nbmw\ni3\n2017\nAutomatic\n41949\nHybrid\n140\n78.1907\n0.0\n19998\n5173.402125\n\n\n2064\n18020\nbmw\ni3\n2015\nAutomatic\n9886\nHybrid\n0\n78.1810\n0.0\n17481\n4214.053932\n\n\n2143\n12972\nbmw\ni8\n2017\nAutomatic\n9992\nHybrid\n135\n69.2767\n1.5\n59950\n14675.519883\n\n\n2144\n13826\nbmw\ni8\n2015\nAutomatic\n43323\nHybrid\n0\n69.2683\n1.5\n44990\n9289.744847\n\n\n2150\n18949\nbmw\ni8\n2015\nAutomatic\n43102\nHybrid\n0\n69.0922\n1.5\n42890\n9300.576839\n\n\n2151\n18977\nbmw\ni8\n2016\nAutomatic\n10087\nHybrid\n0\n68.9279\n1.5\n48998\n12607.867130\n\n\n2744\n18866\nmerc\nM Class\n2004\nAutomatic\n121000\nDiesel\n325\n29.3713\n2.7\n19950\n4068.883513\n\n\n3548\n13149\naudi\nS4\n2019\nAutomatic\n4900\nDiesel\n145\n40.7030\n0.0\n45000\n10679.644966\n\n\n4116\n16420\naudi\nSQ5\n2020\nAutomatic\n1500\nDiesel\n145\n34.7968\n0.0\n56450\n13166.374034\n\n\n4117\n17611\naudi\nSQ5\n2019\nAutomatic\n1500\nDiesel\n145\n34.5016\n0.0\n48800\n11426.005642\n\n\n4851\n16577\nbmw\nZ3\n2002\nAutomatic\n16500\nPetrol\n325\n29.7614\n2.2\n14995\n3426.196256\nDo you notice some unique characteristics of these observations due to which they may be outliers?\nWhat methods you can propose to estimate the price of these outliers more accurately, which will also result in the overall reduction in RMSE?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#high-leverage-points",
    "href": "Lec5_Potential_issues.html#high-leverage-points",
    "title": "6  Beyond Fit (statistical theory)",
    "section": "6.2 High leverage points",
    "text": "6.2 High leverage points\nHigh leverage points are those with an unsual value of the predictor(s). They have the potential to have a relatively higher impact on the OLS line / plane / hyperplane, as compared to the outliers.\nLeverage statistic (page 99 of the book): In order to quantify an observation’s leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For simple linear regression, \\[\\begin{equation}\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i'=1}^{n}(x_{i'} - \\bar x)^2}.\n\\end{equation}\\]\nIt is clear from this equation that \\(h_i\\) increases with the distance of \\(x_i\\) from \\(\\bar x\\). A large value of \\(h_i\\) indicates that the \\(i^{th}\\) observation is distance from the center of all the other observations in terms of predictor values.\nThe leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\), and the average leverage for all the observations is always equal to \\((p+1)/n\\):\n\\[\\begin{equation}\n\\bar{h} = \\frac{p+1}{n}\n\\end{equation}\\]\nSo if a given observation has a leverage statistic that greatly exceeds \\((p+1)/n\\), then we may suspect that the corresponding point has high leverage.\nIf the \\(i^{th}\\) observation has a large leverage \\(h_i\\), it may exercise substantial leverage in determining the fitted value \\(\\hat{Y}_i\\), because:\n\nThe fitted value \\(\\hat{Y}_i\\) is a linear combination of the observed \\(Y\\) values, and \\(h_i\\) is the weight of observation \\(Y_i\\) in determining this fitted value.\nThe larger the \\(h_i\\), the smaller is the variance of the residual \\(e_i\\), and the closer the fitted value \\(\\hat{Y}_i\\) will tend to be the observed value \\(Y_i\\).\n\nThumb rules:\n\nA leverage \\(h_i\\) is usually considered large if it is more than twice as large as the mean value \\(\\bar{h}\\).\nAnother suggested guideline is that \\(h_i\\) values exceeding 0.5 indicate very high leverage, whereas those between 0.2 and 0.5 indicate moderate leverage.\n\nInfluential points: Note that if a high leverage point falls in line with the regression line, then it will not affect the regression line. However, it may inflate \\(R\\)-squared and increase the significance of predictors. If a high leverage point falls away from the regression line, then it is also an outlier, and will affect the regression line. The points whose presence significantly affects the regression line are called influential points. A point that is both a high leverage point and an outlier is likely to be an influential point. However, a high leverage point is not necessarily an influential point.\nSource for influential points: https://online.stat.psu.edu/stat501/book/export/html/973\nLet us see if there are any high leverage points in our regression model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.803\n\n\nModel:\nOLS\nAdj. R-squared:\n0.803\n\n\nMethod:\nLeast Squares\nF-statistic:\n1834.\n\n\nDate:\nSun, 10 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n16:53:39\nLog-Likelihood:\n-1173.8\n\n\nNo. Observations:\n4960\nAIC:\n2372.\n\n\nDf Residuals:\n4948\nBIC:\n2450.\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-238.2125\n25.790\n-9.237\n0.000\n-288.773\n-187.652\n\n\nyear\n0.1227\n0.013\n9.608\n0.000\n0.098\n0.148\n\n\nengineSize\n13.8349\n5.795\n2.387\n0.017\n2.475\n25.195\n\n\nmileage\n0.0005\n0.000\n3.837\n0.000\n0.000\n0.001\n\n\nmpg\n-1.2446\n0.345\n-3.610\n0.000\n-1.921\n-0.569\n\n\nyear:engineSize\n-0.0067\n0.003\n-2.324\n0.020\n-0.012\n-0.001\n\n\nyear:mileage\n-2.67e-07\n6.8e-08\n-3.923\n0.000\n-4e-07\n-1.34e-07\n\n\nyear:mpg\n0.0006\n0.000\n3.591\n0.000\n0.000\n0.001\n\n\nengineSize:mileage\n-2.668e-07\n4.08e-07\n-0.654\n0.513\n-1.07e-06\n5.33e-07\n\n\nengineSize:mpg\n0.0028\n0.000\n6.842\n0.000\n0.002\n0.004\n\n\nmileage:mpg\n7.235e-08\n1.79e-08\n4.036\n0.000\n3.72e-08\n1.08e-07\n\n\nI(mileage ** 2)\n1.828e-11\n5.64e-12\n3.240\n0.001\n7.22e-12\n2.93e-11\n\n\n\n\n\n\n\n\nOmnibus:\n711.514\nDurbin-Watson:\n0.498\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2545.807\n\n\nSkew:\n0.699\nProb(JB):\n0.00\n\n\nKurtosis:\n6.220\nCond. No.\n1.73e+13\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Computing the leverage statistic for each observation\ninfluence = model_log.get_influence()\nleverage = influence.hat_matrix_diag\n\n\n#Visualizng leverage against studentized residuals\nsns.set(rc={'figure.figsize':(15,8)})\nsm.graphics.influence_plot(model_log);\n\n\n\n\n\n\n\n\nLet us identify the high leverage points in the data, as they may be affecting the regression line if they are outliers as well, i.e., if they are influential points. Note that there is no defined threshold for a point to be classified as a high leverage point. Some statisticians consider points having twice the average leverage as high leverage points, some consider points having thrice the average leverage as high leverage points, and so on.\n\nout = model_log.outlier_test()\n\n\n#Average leverage of points\naverage_leverage = (model_log.df_model+1)/model_log.nobs\naverage_leverage\n\n0.0024193548387096775\n\n\nLet us consider points having four times the average leverage as high leverage points.\n\n#We will remove all observations that have leverage higher than the threshold value.\nhigh_leverage_threshold = 3*average_leverage\n\n\n#Number of high leverage points in the dataset\nnp.sum(leverage&gt;high_leverage_threshold)\n\n269\n\n\n\n6.2.1 Identifying extrapolation using leverage\nLeverage can be used to check if prediction on a particular point will lead to extrapolation.\nBelow is the function that can be used to find the leverage at for a particular observation xnew. Note that xnew has to be a single-dimensional array, and X has to be the predictor matrix (also called the design matrix).\n\ndef leverage_compute(xnew, X):\n    return(xnew.reshape(-1, 1).T.dot(np.linalg.inv(X.T.dot(X))).dot(xnew.reshape(-1, 1))[0][0])\n\nAs expected, the function will return the same leverage as provided by the hat_matrix_diag attribute of the objected returned by the get_influence() method of model_log as shown below:\n\nleverage[0]\n\n0.0026426981240353694\n\n\nAs the observation for prediction is required we need to create the predictor matrix X to create all the observations with the interactions specified in the model.\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\n\n\nleverage_compute(X[0,:], X)\n\n0.0026426973869101977\n\n\nIf the leverage for a new observation is higher than the maximum leverage among all the observations in the training dataset, then prediction at the new observation will be extrapolation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#influential-points",
    "href": "Lec5_Potential_issues.html#influential-points",
    "title": "6  Beyond Fit (statistical theory)",
    "section": "6.3 Influential points",
    "text": "6.3 Influential points\nObservations that are both high leverage points and outliers are influential points that may affect the regression line. Let’s remove these influential points from the data and see if it improves the model prediction accuracy on test data.\n\n#Dropping influential points from data\ntrain_filtered = train.drop(np.intersect1d(np.where(np.abs(out.student_resid) &gt; critical_value)[0],\n                                           (np.where(leverage&gt;high_leverage_threshold)[0])))\n\nNote that as the Bonferroni’s critical value is very conservative estimate, we have rounded off the critical value to 4, instead of 4.42.\n\ntrain_filtered.shape\n\n(4948, 11)\n\n\n\n#Number of points removed as they were influential\ntrain.shape[0]-train_filtered.shape[0]\n\n12\n\n\nWe removed 12 influential data points from the training data.\n\n#Model after removing the influential observations\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.815\n\n\nModel:\nOLS\nAdj. R-squared:\n0.814\n\n\nMethod:\nLeast Squares\nF-statistic:\n1971.\n\n\nDate:\nSun, 10 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n16:54:08\nLog-Likelihood:\n-1027.9\n\n\nNo. Observations:\n4948\nAIC:\n2080.\n\n\nDf Residuals:\n4936\nBIC:\n2158.\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-256.2339\n25.421\n-10.080\n0.000\n-306.070\n-206.398\n\n\nyear\n0.1317\n0.013\n10.462\n0.000\n0.107\n0.156\n\n\nengineSize\n18.4650\n5.663\n3.261\n0.001\n7.364\n29.566\n\n\nmileage\n0.0006\n0.000\n4.288\n0.000\n0.000\n0.001\n\n\nmpg\n-1.1810\n0.338\n-3.489\n0.000\n-1.845\n-0.517\n\n\nyear:engineSize\n-0.0090\n0.003\n-3.208\n0.001\n-0.015\n-0.004\n\n\nyear:mileage\n-2.933e-07\n6.7e-08\n-4.374\n0.000\n-4.25e-07\n-1.62e-07\n\n\nyear:mpg\n0.0006\n0.000\n3.458\n0.001\n0.000\n0.001\n\n\nengineSize:mileage\n-4.316e-07\n4e-07\n-1.080\n0.280\n-1.21e-06\n3.52e-07\n\n\nengineSize:mpg\n0.0048\n0.000\n11.537\n0.000\n0.004\n0.006\n\n\nmileage:mpg\n7.254e-08\n1.75e-08\n4.140\n0.000\n3.82e-08\n1.07e-07\n\n\nI(mileage ** 2)\n1.668e-11\n5.53e-12\n3.017\n0.003\n5.84e-12\n2.75e-11\n\n\n\n\n\n\n\n\nOmnibus:\n718.619\nDurbin-Watson:\n0.521\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2512.509\n\n\nSkew:\n0.714\nProb(JB):\n0.00\n\n\nKurtosis:\n6.185\nCond. No.\n1.75e+13\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.75e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nLet us compare the square root of 5-fold cross-validated mean squared error of the model with and without the influential points.\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nnp.sqrt(mean_squared_error(np.exp(cross_val_predict(LinearRegression(), X, y)), np.exp(y)))\n\n9811.74078331643\n\n\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nnp.sqrt(mean_squared_error(np.exp(cross_val_predict(LinearRegression(), X, y)), np.exp(y)))\n\n9800.202063309154\n\n\nWhy can’t we use cross_val_score() instead of cross_val_predict() here?\nThere seems to be a slight improvement in prediction error after removing influential points. Note that none of the points had “very high leverage”, and thus the change is not substantial.\nNote that we obtain a higher R-squared value of 81.5% as compared to 80% with the complete data. Removing the influential points helped obtain a slightly better model fit. However, that may also happen just by reducing observations.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n8922.977452912108\n\n\nThe RMSE on test data has also reduced. This shows that some of the influential points were impacting the regression line. With those points removed, the model better captures the general trend in the data.\n\n6.3.1 Influence on single fitted value (DFFITS)\n\nA useful measure of the influence that the \\(i^{th}\\) observation has on the fitted value \\(\\hat{Y}_i\\) is:\n\n\\[\\begin{equation}\n(DFFITS)_i = \\frac{\\hat{Y}_i-\\hat{Y}_{i(i)}}{\\sqrt{MSE_{i}h_i}}\n\\end{equation}\\]\n\nNote that the denominator in the above fraction is the estimated standard deviation of \\(\\hat{Y}_i\\), but uses the error mean square when the \\(i^{th}\\) observation is omitted.\n\\(DFFITS\\) for the \\(i^{th}\\) observation represents the number of estimated standard deviations of \\(\\hat{Y}_i\\) that the fitted value \\(\\hat{Y}_i\\) increases or decreases with the inclusion of the \\(i^{th}\\) observation in fitting the regression model.\nIt can be shown that:\n\n\\[\\begin{equation}\n(DFFITS)_i = t_i\\sqrt{\\frac{h_i}{1-h_i}}\n\\end{equation}\\]\nwhere \\(t_i\\) is the studentized deleted residual for the \\(i^{th}\\) observation.\n\nWe can see that if an observation has high leverage and is an outlier, it is likely to be influential\nFor large datasets, an observation is considered influential if the magnitude of \\(DFFITS\\) for it exceeds \\(2\\sqrt{\\frac{p}{n}}\\)\n\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.dffits[0])\nplt.xlabel('Observation index')\nplt.ylabel('DFFITS (model)');\n\n\n\n\n\n\n\n\nLet us analyze the point with the highest \\(DFFITS\\).\n\nnp.where(influence.dffits[0]&gt;1)\n\n(array([ 813, 4851], dtype=int64),)\n\n\n\ntrain.loc[813,:]\n\ncarID                12454\nbrand                   vw\nmodel            Caravelle\nyear                  2012\ntransmission     Semi-Auto\nmileage             212000\nfuelType            Diesel\ntax                    325\nmpg                34.4424\nengineSize             2.0\nprice                11995\nName: 813, dtype: object\n\n\n\ntrain.loc[train.model == ' Caravelle','mileage'].describe()\n\ncount        65.000000\nmean      25638.692308\nstd       42954.135726\nmin          10.000000\n25%        3252.000000\n50%        6900.000000\n75%       30414.000000\nmax      212000.000000\nName: mileage, dtype: float64\n\n\n\n# Prediction with model developed based on all points\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', \n                     data = train)\nmodel_log = ols_object.fit();\nnp.exp(model_log.predict(train.loc[[813],:]))\n\n813    5502.647323\ndtype: float64\n\n\n\n# Prediction with model developed based on all points except the 813th point\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', \n                     data = train.drop(index = 813))\nmodel_log = ols_object.fit();\nnp.exp(model_log.predict(train.loc[[813],:]))\n\n813    4581.374593\ndtype: float64\n\n\nLet us see the leverage and studentized residual for this observation.\n\n# Leverage\nleverage[813]\n\n0.19038697461006687\n\n\n\n# Studentized residual\nout.student_resid[813]\n\n2.823478041409651\n\n\nDo you notice what may be contributing to the high influence of this point?\n\n\n6.3.2 Influence on all fitted values (Cook’s distance)\nIn contrast to \\(DFFITS\\), which considers the influence of the \\(i^{th}\\) observation on the fitted value \\(\\hat{Y}_i\\), Cook’s distance considers the influence of the \\(i^{th}\\) observation on all \\(n\\) the fitted values:\n\\[\\begin{equation}\nD_i = \\frac{\\sum_{j=1}^{n} (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{pMSE}\n\\end{equation}\\]\nIt can be shown that:\n\\[\\begin{equation}\nD_i = \\frac{e_i^2}{pMSE}\\bigg[\\frac{h_i}{(1-h_i)^2}\\bigg]\n\\end{equation}\\]\nThe larger \\(h_i\\) or \\(e_i\\), the larger is \\(D_i\\). \\(D_i\\) can be related to the \\(F(p, n- p)\\) distribution. If the percentile value is 50% or more, the observation is considered as highly influential.\nCook’s distance is considered high if it is greater than 0.5 and extreme if it is greater than 1.\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.cooks_distance[0])\nplt.xlabel('Observation index')\nplt.ylabel(\"Cook's Distance (model)\");\n\n\n\n\n\n\n\n\n\n# Point with the highest Cook's distance\nnp.where(influence.cooks_distance[0]&gt;0.15)\n\n(array([813], dtype=int64),)\n\n\nThe critical Cook’s distance value for a point to be highly influential in this dataset is:\n\nstats.f.ppf(0.5, 11, 4949)\n\n0.9402181103263811\n\n\nThus, we don’t have any highly influential points in the dataset.\n\n\n6.3.3 Influence on regression coefficients (DFBETAS)\n\n\\(DFBETAS\\) measures the influence of the \\(i^{th}\\) observation on the regression coefficient.\n\\(DFBETAS\\) of the \\(i^{th}\\) observation on the \\(k^{th}\\) regression coefficient is:\n\n\\[\\begin{equation}\n(DFBETAS)_{k(i)} = \\frac{\\hat{\\beta}_k-\\hat{\\beta}_{k(i)}}{\\sqrt{MSE_ic_{k}}}\n\\end{equation}\\]\nwhere \\(c_k\\) is the \\(k^{th}\\) diagonal element of \\((X^TX)^{-1}\\).\nFor large datasets, an observation is considered influential if \\(DFBETAS\\) exceeds \\(\\frac{2}{\\sqrt{n}}\\).\nBelow is the plot of \\(DFBETAS\\) for the year predictor against the observation index.\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.dfbetas[:,1])\nplt.xlabel('Observation index')\nplt.ylabel(\"DFBETAS (year)\");\n\n\n\n\n\n\n\n\nLet us analyze the point with the highest magnitude of \\(DFBETAS\\).\n\nnp.where(influence.dfbetas[:,1]&lt;-0.8)\n\n(array([4851], dtype=int64),)\n\n\n\ntrain.year.describe()\n\ncount    4960.000000\nmean     2016.737903\nstd         2.884035\nmin      1997.000000\n25%      2016.000000\n50%      2017.000000\n75%      2019.000000\nmax      2020.000000\nName: year, dtype: float64\n\n\n\ntrain.loc[train.year&lt;=2002,:]\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n330\n13200\naudi\nA8\n1997\nAutomatic\n122000\nPetrol\n265\n19.3511\n4.2\n4650\n\n\n732\n13988\nvw\nBeetle\n2001\nManual\n47729\nPetrol\n330\n32.5910\n2.0\n2490\n\n\n3157\n18794\nford\nPuma\n2002\nManual\n108000\nPetrol\n230\n38.5757\n1.7\n2195\n\n\n3525\n19395\nmerc\nS Class\n2001\nAutomatic\n108800\nDiesel\n325\n31.5473\n3.2\n1695\n\n\n3532\n17531\nmerc\nS Class\n1999\nAutomatic\n34000\nPetrol\n145\n24.8735\n3.2\n5995\n\n\n3533\n18761\nmerc\nS Class\n2001\nAutomatic\n66000\nPetrol\n570\n24.7744\n3.2\n4495\n\n\n3535\n18813\nmerc\nS Class\n1998\nAutomatic\n43534\nPetrol\n265\n23.2962\n6.0\n19990\n\n\n3536\n17891\nmerc\nS Class\n2002\nAutomatic\n24000\nPetrol\n570\n20.7968\n5.0\n6995\n\n\n3707\n18746\nhyundi\nSanta Fe\n2002\nManual\n94000\nPetrol\n325\n30.2671\n2.4\n1200\n\n\n4091\n12995\nmerc\nSLK\n1998\nAutomatic\n113557\nPetrol\n265\n31.8368\n2.3\n1990\n\n\n4094\n19585\nmerc\nSLK\n2001\nAutomatic\n69234\nPetrol\n325\n30.8839\n2.0\n3990\n\n\n4096\n14265\nmerc\nSLK\n2001\nAutomatic\n48172\nPetrol\n325\n29.7058\n2.3\n3990\n\n\n4097\n15821\nmerc\nSLK\n2002\nAutomatic\n61400\nPetrol\n325\n29.6568\n2.3\n3990\n\n\n4098\n13021\nmerc\nSLK\n2001\nAutomatic\n91000\nPetrol\n325\n30.3248\n2.3\n3950\n\n\n4099\n12660\nmerc\nSLK\n2001\nAutomatic\n42087\nPetrol\n325\n29.9404\n2.3\n4490\n\n\n4101\n17521\nmerc\nSLK\n2002\nAutomatic\n75034\nPetrol\n325\n30.1380\n2.3\n4990\n\n\n4107\n13977\nmerc\nSLK\n2000\nAutomatic\n87000\nPetrol\n265\n27.2998\n3.2\n1490\n\n\n4108\n18679\nmerc\nSLK\n2000\nAutomatic\n113237\nPetrol\n270\n26.8765\n3.2\n3990\n\n\n4109\n14598\nmerc\nSLK\n2001\nAutomatic\n64476\nPetrol\n325\n27.4628\n3.2\n4990\n\n\n4847\n17268\nbmw\nZ3\n1997\nManual\n49000\nPetrol\n270\n34.9548\n1.9\n3950\n\n\n4848\n12137\nbmw\nZ3\n1999\nManual\n58000\nPetrol\n270\n35.3077\n1.9\n3950\n\n\n4849\n13288\nbmw\nZ3\n1999\nManual\n74282\nPetrol\n245\n35.4143\n1.9\n3995\n\n\n4850\n19172\nbmw\nZ3\n2001\nManual\n60000\nPetrol\n325\n30.7305\n2.2\n5950\n\n\n4851\n16577\nbmw\nZ3\n2002\nAutomatic\n16500\nPetrol\n325\n29.7614\n2.2\n14995\n\n\n\n\n\n\n\nLet us see the leverage and studentized residual for this observation.\n\n# Leverage\nleverage[4851]\n\n0.047120455781282225\n\n\n\n# Studentized residual\nout.student_resid[4851]\n\n4.938606329343604\n\n\nDo you see what makes this point influential?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#collinearity",
    "href": "Lec5_Potential_issues.html#collinearity",
    "title": "6  Beyond Fit (statistical theory)",
    "section": "6.4 Collinearity",
    "text": "6.4 Collinearity\nCollinearity refers to the situation when two or more predictor variables have a high linear association. Linear association between a pair of variables can be measured by the correlation coefficient. Thus the correlation matrix can indicate some potential collinearity problems.\n\n6.4.1 Why and how is collinearity a problem\n(Source: page 100-101 of book)\nThe presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for \\(\\hat \\beta_j\\) to grow. Recall that the t-statistic for each predictor is calculated by dividing \\(\\hat \\beta_j\\) by its standard error. Consequently, collinearity results in a decline in the \\(t\\)-statistic. As a result, in the presence of collinearity, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test—the probability of correctly detecting a non-zero coefficient—is reduced by collinearity.\n\n\n6.4.2 How to measure collinearity/multicollinearity\n(Source: page 102 of book)\nUnfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor the ratio of the variance of \\(\\hat \\beta_j\\) when fitting the full model divided by the variance of \\(\\hat \\beta_j\\) if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\nThe estimated variance of the coefficient \\(\\beta_j\\), of the \\(j^{th}\\) predictor \\(X_j\\), can be expressed as:\n\\[\\hat{var}(\\hat{\\beta_j}) = \\frac{(\\hat{\\sigma})^2}{(n-1)\\hat{var}({X_j})}.\\frac{1}{1-R^2_{X_j|X_{-j}}},\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the \\(R\\)-squared for the regression of \\(X_j\\) on the other covariates (a regression that does not involve the response variable \\(Y\\)).\nIn case of simple linear regression, the variance expression in the equation above does not contain the term \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\), as there is only one predictor. However, in case of multiple linear regression, the variance of the estimate of the \\(j^{th}\\) coefficient (\\(\\hat{\\beta_j}\\)) gets inflated by a factor of \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\) (Note that in the complete absence of collinearity, \\(R^2_{X_j|X_{-j}}=0\\), and the value of this factor will be 1).\nThus, the Variance inflation factor, or the VIF for the estimated coefficient of the \\(j^{th}\\) predictor \\(X_j\\) is:\n\\[\\begin{equation}\nVIF(\\hat \\beta_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\n\\end{equation}\\]\n\n#Correlation matrix\ntrain.corr()\n\n\n\n\n\n\n\n\ncarID\nyear\nmileage\ntax\nmpg\nengineSize\nprice\n\n\n\n\ncarID\n1.000000\n0.006251\n-0.001320\n0.023806\n-0.010774\n0.011365\n0.012129\n\n\nyear\n0.006251\n1.000000\n-0.768058\n-0.205902\n-0.057093\n0.014623\n0.501296\n\n\nmileage\n-0.001320\n-0.768058\n1.000000\n0.133744\n0.125376\n-0.006459\n-0.478705\n\n\ntax\n0.023806\n-0.205902\n0.133744\n1.000000\n-0.488002\n0.465282\n0.144652\n\n\nmpg\n-0.010774\n-0.057093\n0.125376\n-0.488002\n1.000000\n-0.419417\n-0.369919\n\n\nengineSize\n0.011365\n0.014623\n-0.006459\n0.465282\n-0.419417\n1.000000\n0.624899\n\n\nprice\n0.012129\n0.501296\n-0.478705\n0.144652\n-0.369919\n0.624899\n1.000000\n\n\n\n\n\n\n\nLet us compute the Variance Inflation Factor (VIF) for the four predictors.\n\nX = train[['mpg','year','mileage','engineSize']]\n\n\nX.columns[1:]\n\nIndex(['year', 'mileage', 'engineSize'], dtype='object')\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(X)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\nfor i in range(len(X.columns)):\n    vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n\nprint(vif_data)\n\n      feature           VIF\n0       const  1.201579e+06\n1         mpg  1.243040e+00\n2        year  2.452891e+00\n3     mileage  2.490210e+00\n4  engineSize  1.219170e+00\n\n\nAs all the values of VIF are close to one, we do not have the problem of multicollinearity in the model. Note that the VIF of year and mileage is relatively high as they are the most correlated.\nQ1: Why is the VIF of the constant so high?\nQ2: Why do we need to include the constant while finding the VIF?\n\n\n6.4.3 Manual computation of VIF\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'price~mpg', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.137\n\n\nModel:\nOLS\nAdj. R-squared:\n0.137\n\n\nMethod:\nLeast Squares\nF-statistic:\n786.0\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n1.14e-160\n\n\nTime:\n17:04:39\nLog-Likelihood:\n-54812.\n\n\nNo. Observations:\n4960\nAIC:\n1.096e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.096e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.144e+04\n676.445\n61.258\n0.000\n4.01e+04\n4.28e+04\n\n\nmpg\n-374.2975\n13.351\n-28.036\n0.000\n-400.471\n-348.124\n\n\n\n\n\n\n\n\nOmnibus:\n2132.208\nDurbin-Watson:\n0.320\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n13751.995\n\n\nSkew:\n1.942\nProb(JB):\n0.00\n\n\nKurtosis:\n10.174\nCond. No.\n158.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n(13.351/9.338)**2\n\n2.044183378279958\n\n\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'price~year+mpg+engineSize+mileage', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n17:01:18\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.661e+06\n1.49e+05\n-24.593\n0.000\n-3.95e+06\n-3.37e+06\n\n\nyear\n1817.7366\n73.751\n24.647\n0.000\n1673.151\n1962.322\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'year~mpg+engineSize+mileage', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nyear\nR-squared:\n0.592\n\n\nModel:\nOLS\nAdj. R-squared:\n0.592\n\n\nMethod:\nLeast Squares\nF-statistic:\n2400.\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n17:00:13\nLog-Likelihood:\n-10066.\n\n\nNo. Observations:\n4960\nAIC:\n2.014e+04\n\n\nDf Residuals:\n4956\nBIC:\n2.017e+04\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2018.3135\n0.140\n1.44e+04\n0.000\n2018.039\n2018.588\n\n\nmpg\n0.0095\n0.002\n5.301\n0.000\n0.006\n0.013\n\n\nengineSize\n0.1171\n0.037\n3.203\n0.001\n0.045\n0.189\n\n\nmileage\n-9.139e-05\n1.08e-06\n-84.615\n0.000\n-9.35e-05\n-8.93e-05\n\n\n\n\n\n\n\n\nOmnibus:\n2949.664\nDurbin-Watson:\n1.161\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n63773.271\n\n\nSkew:\n-2.426\nProb(JB):\n0.00\n\n\nKurtosis:\n19.883\nCond. No.\n1.91e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.91e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#VIF for year\n1/(1-0.592)\n\n2.4509803921568625\n\n\nNote that year and mileage have a high linear correlation. Removing one of them should decrease the standard error of the coefficient of the other, without significanty decrease R-squared.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+mileage+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nTue, 07 Feb 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n21:39:45\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.661e+06\n1.49e+05\n-24.593\n0.000\n-3.95e+06\n-3.37e+06\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\nyear\n1817.7366\n73.751\n24.647\n0.000\n1673.151\n1962.322\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nRemoving mileage from the above regression.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.641\n\n\nModel:\nOLS\nAdj. R-squared:\n0.641\n\n\nMethod:\nLeast Squares\nF-statistic:\n2951.\n\n\nDate:\nTue, 07 Feb 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n21:40:00\nLog-Likelihood:\n-52635.\n\n\nNo. Observations:\n4960\nAIC:\n1.053e+05\n\n\nDf Residuals:\n4956\nBIC:\n1.053e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-5.586e+06\n9.78e+04\n-57.098\n0.000\n-5.78e+06\n-5.39e+06\n\n\nmpg\n-101.9120\n9.500\n-10.727\n0.000\n-120.536\n-83.288\n\n\nengineSize\n1.196e+04\n194.848\n61.392\n0.000\n1.16e+04\n1.23e+04\n\n\nyear\n2771.1844\n48.492\n57.147\n0.000\n2676.118\n2866.251\n\n\n\n\n\n\n\n\nOmnibus:\n2389.075\nDurbin-Watson:\n0.528\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n26920.051\n\n\nSkew:\n2.018\nProb(JB):\n0.00\n\n\nKurtosis:\n13.675\nCond. No.\n1.41e+06\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the standard error of the coefficient of year has reduced from 73 to 48, without any large reduction in R-squared.\n\n\n6.4.4 When can we overlook multicollinearity?\n\nThe severity of the problems increases with the degree of the multicollinearity. Therefore, if there is only moderate multicollinearity (5 &lt; VIF &lt; 10), we may overlook it.\nMulticollinearity affects only the standard errors of the coefficients of collinear predictors. Therefore, if multicollinearity is not present for the predictors that we are particularly interested in, we may not need to resolve it.\nMulticollinearity affects the standard error of the coefficients and thereby their \\(p\\)-values, but in general, it does not influence the prediction accuracy, except in the case that the coefficients are so unstable that the predictions are outside of the domain space of the response. If our sole aim is prediction, and we don’t wish to infer the statistical significance of predictors, then we may avoid addressing multicollinearity. “The fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations, provided these inferences are made within the region of observations” - Neter, John, Michael H. Kutner, Christopher J. Nachtsheim, and William Wasserman. “Applied linear statistical models.” (1996): 318.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html",
    "href": "Logistic Regression.html",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "",
    "text": "7.1 Theory Behind Logistic Regression\nRead sections 4.1 - 4.3 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLogistic regression is the go-to linear classification algorithm for two-class problems. It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#theory-behind-logistic-regression",
    "href": "Logistic Regression.html#theory-behind-logistic-regression",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "",
    "text": "7.1.1 Description\nLogistic regression is named for the function used at the core of the method, the logistic function.\nThe logistic function, also called the Sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\\[\\frac{1}{1 + e^{-x}}\\]\n\\(e\\) is the base of the natural logarithms and \\(x\\) is value that you want to transform via the logistic function.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\nx = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(10, 6))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Sigmoid Function\")\n\nText(0.5, 1.0, 'Sigmoid Function')\n\n\n\n\n\n\n\n\n\nThe logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n\\[\\hat{p}=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}\\]\nor\n\\[\\hat{p}=\\frac{1.0}{1.0+e^{-(\\hat{\\beta_0}+\\hat{\\beta_1}x_1)}}\\]\n\\(\\hat{\\beta_0}\\) is the estimated intercept term\n\\(\\hat{\\beta_1}\\) is the estimated coefficient for \\(x_1\\)\n\\(\\hat{p}\\) is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n\n\n7.1.2 Learning the Logistic Regression Model\nThe coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using maximum-likelihood estimation.\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\nThe best coefficients should result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that maximize the likelihood of the observed data. In other words, in MLE, we estimate the parameter values (Beta values) which are the most likely to produce that data at hand.\nHere is an analogy to understand the idea behind Maximum Likelihood Estimation (MLE). Let us say, you are listening to a song (data). You are not aware of the singer (parameter) of the song. With just the musical piece at hand, you try to guess the singer (parameter) who you feel is the most likely (MLE) to have sung that song. Your are making a maximum likelihood estimate! Out of all the singers (parameter space) you have chosen them as the one who is the most likely to have sung that song (data).\nWe are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\nWhen you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm.\n\n\n7.1.3 Preparing Data for Logistic Regression\nThe assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\nMuch study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\nUltimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n\nBinary Output Variable: This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\nRemove Noise: Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\nGaussian Distribution: Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\nRemove Correlated Inputs: Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\nFail to Converge: It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "href": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "7.2 Logistic Regression: Scikit-learn vs Statsmodels",
    "text": "7.2 Logistic Regression: Scikit-learn vs Statsmodels\nPython gives us two ways to do logistic regression. Statsmodels offers modeling from the perspective of statistics. Scikit-learn offers some of the same models from the perspective of machine learning.\nSo we need to understand the difference between statistics and machine learning! Statistics makes mathematically valid inferences about a population based on sample data. Statistics answers the question, “What is the evidence that X is related to Y?” Machine learning has the goal of optimizing predictive accuracy rather than inference. Machine learning answers the question, “Given X, what prediction should we make for Y?”",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#training-a-logistic-regression-model",
    "href": "Logistic Regression.html#training-a-logistic-regression-model",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "7.3 Training a logistic regression model",
    "text": "7.3 Training a logistic regression model\nRead the data on social network ads. The data shows if the person purchased a product when targeted with an ad on social media. Fit a logistic regression model to predict if a user will purchase the product based on their characteristics such as age, gender and estimated salary.\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\nx = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(6, 4))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Sigmoid Function\");",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels-1",
    "href": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels-1",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "7.4 Logistic Regression: Scikit-learn vs Statsmodels",
    "text": "7.4 Logistic Regression: Scikit-learn vs Statsmodels\nPython gives us two ways to do logistic regression. Statsmodels offers modeling from the perspective of statistics. Scikit-learn offers some of the same models from the perspective of machine learning.\nSo we need to understand the difference between statistics and machine learning! Statistics makes mathematically valid inferences about a population based on sample data. Statistics answers the question, “What is the evidence that X is related to Y?” Machine learning has the goal of optimizing predictive accuracy rather than inference. Machine learning answers the question, “Given X, what prediction should we make for Y?”\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nRead the data on social network ads. The data shows if the person purchased a product when targeted with an ad on social media. Fit a logistic regression model to predict if a user will purchase the product based on their characteristics such as age, gender and estimated salary.\n\ntrain = pd.read_csv('./Datasets/Social_Network_Ads_train.csv') #Develop the model on train data\ntest = pd.read_csv('./Datasets/Social_Network_Ads_test.csv') #Test the model on test data\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n\n\n\n\n\n\n7.4.1 Examining the Distribution of the Target Column, make sure our target is not severely imbalanced\n\ntrain.Purchased.value_counts()\n\nPurchased\n0    194\n1    106\nName: count, dtype: int64\n\n\n\nsns.countplot(x = 'Purchased',data = train);\n\n\n\n\n\n\n\n\n\n\n7.4.2 Fitting a linear regression\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlm = sm.ols(formula = 'Purchased~Age', data = train).fit() #Developing linear regression model\nsns.lineplot(x = 'Age', y= lm.predict(train), data = train, color = 'blue') #Visualizing model\n\n\n\n\n\n\n\n\n\n\n7.4.3 Logistic Regression with Statsmodel\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlogit_model = sm.logit(formula = 'Purchased~Age', data = train).fit() #Developing logistic regression model\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') #Visualizing model\n\nOptimization terminated successfully.\n         Current function value: 0.430107\n         Iterations 7\n\n\n\n\n\n\n\n\n\n\nlogit_model.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\nPurchased\nNo. Observations:\n300\n\n\nModel:\nLogit\nDf Residuals:\n298\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 09 Feb 2025\nPseudo R-squ.:\n0.3378\n\n\nTime:\n18:28:20\nLog-Likelihood:\n-129.03\n\n\nconverged:\nTrue\nLL-Null:\n-194.85\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.805e-30\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-7.8102\n0.885\n-8.825\n0.000\n-9.545\n-6.076\n\n\nAge\n0.1842\n0.022\n8.449\n0.000\n0.141\n0.227\n\n\n\n\n\n\nlogit_model_gender = sm.logit(formula = 'Purchased~Gender', data = train).fit()\nlogit_model_gender.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.648804\n         Iterations 4\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nPurchased\nNo. Observations:\n300\n\n\nModel:\nLogit\nDf Residuals:\n298\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 09 Feb 2025\nPseudo R-squ.:\n0.001049\n\n\nTime:\n18:28:20\nLog-Likelihood:\n-194.64\n\n\nconverged:\nTrue\nLL-Null:\n-194.85\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.5225\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5285\n0.168\n-3.137\n0.002\n-0.859\n-0.198\n\n\nGender[T.Male]\n-0.1546\n0.242\n-0.639\n0.523\n-0.629\n0.319\n\n\n\n\n\n\n# Predicted probabilities\npredicted_probabilities = logit_model.predict(train)\npredicted_probabilities\n\n0      0.235159\n1      0.348227\n2      0.235159\n3      0.348227\n4      0.046473\n         ...   \n295    0.737081\n296    0.481439\n297    0.065810\n298    0.829688\n299    0.150336\nLength: 300, dtype: float64\n\n\n\n# Predicted classes (binary outcome, 0 or 1)\npredicted_classes = (predicted_probabilities &gt; 0.5).astype(int)\npredicted_classes\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n295    1\n296    0\n297    0\n298    1\n299    0\nLength: 300, dtype: int32\n\n\n\n#Function to compute confusion matrix and prediction accuracy on training data\ndef confusion_matrix_train(model,cutoff=0.5):\n    # Confusion matrix\n    cm_df = pd.DataFrame(model.pred_table(threshold = cutoff))\n    #Formatting the confusion matrix\n    cm_df.columns = ['Predicted 0', 'Predicted 1'] \n    cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n    cm = np.array(cm_df)\n    # Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n\n\ncm = confusion_matrix_train(logit_model)\n\nClassification accuracy = 83.3%\n\n\n\n\n\n\n\n\n\n\n# change the cutoff to 0.3\ncm = confusion_matrix_train(logit_model, 0.3)\n\nClassification accuracy = 73.7%\n\n\n\n\n\n\n\n\n\n\n# increase the cutoff to 0.7\ncm = confusion_matrix_train(logit_model, 0.8)\n\nClassification accuracy = 74.7%\n\n\n\n\n\n\n\n\n\nMaking prediction on test set and output the model’s performance\n\n# Predicted probabilities\npredicted_probabilities = logit_model.predict(test)\n\n\n# Predicted classes (binary outcome, 0 or 1)\npredicted_classes = (predicted_probabilities &gt; 0.5).astype(int)\npredicted_classes\n\n0     0\n1     0\n2     0\n3     0\n4     0\n     ..\n95    1\n96    1\n97    1\n98    0\n99    1\nLength: 100, dtype: int32\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\nconfusion_mat = confusion_matrix(test.Purchased, predicted_classes)\n# Define labels for the confusion matrix\nlabels = ['Actual Negative', 'Actual Positive']\n# Create a formatted confusion matrix\nformatted_confusion_mat = pd.DataFrame(confusion_mat, index=labels, columns=[f'Predicted {label}' for label in labels])\n\nprint(\"Confusion Matrix:\")\nprint(formatted_confusion_mat)\n\nConfusion Matrix:\n                 Predicted Actual Negative  Predicted Actual Positive\nActual Negative                         58                          5\nActual Positive                          9                         28\n\n\n\n\n7.4.4 Logistic Regression with Sklearn\n\nX_train = train[['Age']]\ny_train = train['Purchased']\n\nX_test = test[['Age']]\ny_test = test['Purchased']\n\n\n# turn off regularization\nskn_model = LogisticRegression(penalty=None)\n\n\nskn_model.fit(X_train, y_train)\n\nLogisticRegression(penalty=None)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(penalty=None) \n\n\n\n# Note that in sklearn, .predict returns the classes directly, with 0.5 threshold\ny_pred_test = skn_model.predict(X_test)\ny_pred_test\n\narray([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64)\n\n\n\n# To return the prediction probabilities, we need .predict_proba\n# # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) vs 1 (2nd column in array)\ny_pred_probs = skn_model.predict_proba(X_test)\ny_pred_probs[:5]\n\narray([[0.79634123, 0.20365877],\n       [0.95352574, 0.04647426],\n       [0.944647  , 0.055353  ],\n       [0.8717078 , 0.1282922 ],\n       [0.92191865, 0.07808135]])\n\n\n\ncm=confusion_matrix(y_test,y_pred_test)\n#plt.figure(figsize=(4,4))\nplt.title(\"Confusion Matrix on test data\")\nsns.heatmap(cm, annot=True,fmt='d', cmap='Blues')\nplt.ylabel(\"Actual Values\")\nplt.xlabel(\"Predicted Values\")\n\nText(0.5, 5.183333333333314, 'Predicted Values')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\nfrom sklearn.metrics import precision_score\nprint(\"Precision:\", precision_score(y_test, y_pred_test))\nfrom sklearn.metrics import recall_score\nprint(\"Recall:\", recall_score(y_test, y_pred_test))\nfrom sklearn.metrics import f1_score\nprint(\"F1 score:\",  f1_score(y_test, y_pred_test))\n\nAccuracy: 0.86\nPrecision: 0.8484848484848485\nRecall: 0.7567567567567568\nF1 score: 0.8\n\n\n\n\n7.4.5 Changing the default threshold\n\nnew_threshold = 0.3\n\n\npredicted_classes_new_threshold = (y_pred_probs &gt; new_threshold).astype(int)\npredicted_classes_new_threshold[:5]\n\narray([[1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0]])\n\n\n\nconfusion_mat_new_threshold = confusion_matrix(y_test, predicted_classes_new_threshold[:, 1])\nprint(\"Confusion Matrix (Threshold =\", new_threshold, \"):\")\nprint(confusion_mat_new_threshold)\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import precision_score\nprint(\"Precision:\", precision_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import recall_score\nprint(\"Recall:\", recall_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import f1_score\nprint(\"F1 score:\",  f1_score(y_test, predicted_classes_new_threshold[:, 1]))\n\nConfusion Matrix (Threshold = 0.3 ):\n[[44 19]\n [ 7 30]]\nAccuracy: 0.74\nPrecision: 0.6122448979591837\nRecall: 0.8108108108108109\nF1 score: 0.6976744186046512",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#performance-measurement",
    "href": "Logistic Regression.html#performance-measurement",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "7.5 Performance Measurement",
    "text": "7.5 Performance Measurement\nWe have already seen the confusion matrix, and classification accuracy. Now, let us see some other useful performance metrics that can be computed from the confusion matrix. The metrics below are computed for the confusion matrix immediately above this section (or the confusion matrix on test data corresponding to the model logit_model_diabetes).\n\n7.5.1 Precision-recall\nPrecision measures the accuracy of positive predictions. Also called the precision of the classifier\n\\[\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}\\]\n==&gt; 70.13%\nPrecision is typically used with recall (Sensitivity or True Positive Rate). The ratio of positive instances that are correctly detected by the classifier.\n\\(\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}\\) ==&gt; 88.52%\nPrecision / Recall Tradeoff: Increasing precision reduces recall and vice versa.\nVisualize the precision-recall curve for the model logit_model_diabetes.\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\n\n\n296\n15701537\nMale\n42\n149000\n1\n\n\n297\n15807481\nMale\n28\n79000\n0\n\n\n298\n15603942\nFemale\n51\n134000\n0\n\n\n299\n15690188\nFemale\n33\n28000\n0\n\n\n\n\n300 rows × 5 columns\n\n\n\n\ny=train.Purchased\nypred = lm.predict(train)\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\nAs the decision threshold probability increases, the precision increases, while the recall decreases.\nQ: How are the values of the thresholds chosen to make the precision-recall curve?\nHint: Look at the documentation for precision_recall_curve.\n\n\n7.5.2 The Receiver Operating Characteristics (ROC) Curve\nA ROC(Receiver Operator Characteristic Curve) is a plot of sensitivity (True Positive Rate) on the y axis against (1−specificity) (False Positive Rate) on the x axis for varying values of the threshold t. The 45° diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to random chance. The ROC curve for the gold standard is the line connecting (0,0) to (0,1) and (0,1) to (1,1).\n\n\n\n\n\nHigh Threshold:\n\nHigh specificity\nLow sensitivity\n\nLow Threshold\n\nLow specificity\nHigh sensitivity\n\nThe area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a good blog link.\nHere is good post by google developers on interpreting ROC-AUC, and its advantages / disadvantages.\nVisualize the ROC curve and compute the ROC-AUC for the model logit_model_diabetes.\n\ny=train.Purchased\nypred = lm.predict(train)\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n0.8593901964598327\n\n\n\n\n\n\n\n\n\nQ: How are the values of the auc_thresholds chosen to make the ROC curve? Why does it look like a step function?\nBelow is a function that prints the confusion matrix along with all the performance metrics we discussed above for a given decision threshold probability, on train / test data. Note that ROC-AUC does not depend on a decision threshold probability.\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict(data)\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    fnr = (cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = (cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = (cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = (cm[1,1])/(cm[1,0]+cm[1,1])\n    fpr_roc, tpr_roc, auc_thresholds = roc_curve(actual_values, pred_values)\n    auc_value = (auc(fpr_roc, tpr_roc))# AUC of ROC\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n    print(\"Precision = {:.1%}\".format(precision))\n    print(\"TPR or Recall = {:.1%}\".format(tpr))\n    print(\"FNR = {:.1%}\".format(fnr))\n    print(\"FPR = {:.1%}\".format(fpr))\n    print(\"ROC-AUC = {:.1%}\".format(auc_value))\n\n\nconfusion_matrix_data(test,test.Purchased,lm,0.3)\n\nClassification accuracy = 68.2%\nPrecision = 58.3%\nTPR or Recall = 94.6%\nFNR = 5.4%\nFPR = 52.1%\nROC-AUC = 89.4%",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#the-receiver-operating-characteristics-roc-curve-1",
    "href": "Logistic Regression.html#the-receiver-operating-characteristics-roc-curve-1",
    "title": "7  Logistic regression: Introduction and Metrics",
    "section": "8.1 The Receiver Operating Characteristics (ROC) Curve",
    "text": "8.1 The Receiver Operating Characteristics (ROC) Curve\nA ROC(Receiver Operator Characteristic Curve) is a plot of sensitivity (True Positive Rate) on the y axis against (1−specificity) (False Positive Rate) on the x axis for varying values of the threshold t. The 45° diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to random chance. The ROC curve for the gold standard is the line connecting (0,0) to (0,1) and (0,1) to (1,1).\n\nHigh Threshold: * High specificity * Low sensitivity\nLow Threshold * Low specificity * High sensitivity\nThe area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a link\n\nfrom sklearn.metrics import roc_curve\n\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\nplt.figure(figsize=(9,6)); \nplot_roc_curve(fpr, tpr)\nplt.show();\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, y_pred_test)\n\n0.8386958386958387",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html",
    "title": "8  Logistic regression: Others",
    "section": "",
    "text": "8.1 Decision Boundary in Classification\n# Importing the dataset\ndataset = pd.read_csv('datasets/apples_and_oranges.csv')\ndataset.head()\n\n\n\n\n\n\n\n\nWeight\nSize\nClass\n\n\n\n\n0\n69\n4.39\norange\n\n\n1\n69\n4.21\norange\n\n\n2\n65\n4.09\norange\n\n\n3\n72\n5.85\napple\n\n\n4\n67\n4.70\norange\n# No. of apples and oranges\ndataset['Class'].value_counts()\n\nClass\norange    20\napple     20\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#decision-boundary-in-classification",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#decision-boundary-in-classification",
    "title": "8  Logistic regression: Others",
    "section": "",
    "text": "8.1.1 Encoding Target\n\nle = LabelEncoder()\ndataset['Class'] = le.fit_transform(dataset['Class'])\nle.classes_\n\narray(['apple', 'orange'], dtype=object)\n\n\nThis implies that, * 0 represents Apple * 1 represents Orange\n\ndataset.head()\n\n\n\n\n\n\n\n\nWeight\nSize\nClass\n\n\n\n\n0\n69\n4.39\n1\n\n\n1\n69\n4.21\n1\n\n\n2\n65\n4.09\n1\n\n\n3\n72\n5.85\n0\n\n\n4\n67\n4.70\n1\n\n\n\n\n\n\n\n\n\n8.1.2 Plotting the dataset\n\nplt.figure(figsize=(9,6))\nplt.title('Apples and Oranges', fontweight='bold', fontsize=16)\nplt.xlabel('Weight')\nplt.ylabel('Size')\nscatter = plt.scatter(dataset['Weight'], dataset['Size'], c=dataset['Class'], cmap='viridis')\nplt.legend(*scatter.legend_elements(),\n           loc = 'upper left',\n           title = 'Class');\n\n\n\n\n\n\n\n\nWe can observe that oranges have lower weight and size compared to apples. Further by drawing a straight line between these two groups of data points, we can clearly distinguish between apples and oranges.\n\n\n8.1.3 Building a Logistic Regression model to distinguish apples and oranges\nAs we can clearly distinguish between apples and oranges using a straight line decision boundary, we can choose the hypothesis y = a0 + a1  x1 + a2 * x2* for Logistic Regression where, a0, a1, a2 are the fitting parameters x1 is Weight x2 is Size\n\n# Defining target and features\ny = dataset['Class']\nx = dataset.drop(columns=['Class'])\n\n\n# Creating object of LogisticRegression class\nlog_reg = LogisticRegression()\n\n\n# Fitting parameters\nlog_reg.fit(x,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Intercept - a0\nlog_reg.intercept_\n\narray([106.60287324])\n\n\n\n# Coefficients - a1, a2 respectively\nlog_reg.coef_\n\narray([[-1.42833694, -1.31285258]])\n\n\n\n# Predicting labels for the given dataset\nlabel_predictions = log_reg.predict(x)\nlabel_predictions\n\narray([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0])\n\n\n\n\n8.1.4 Linear Decision Boundary with naive features\n\n# Parameter values\na0 = log_reg.intercept_[0]\na1 = log_reg.coef_[0][0]\na2 = log_reg.coef_[0][1]\n\n\n# Defining x1 and x2 values for decision boundary\nx1 = np.array([69, 71])\nx2 = (0.5 -a0 - (a1 * x1)) / a2\n\n\n# Plotting the decision boundary\nplt.figure(figsize=(9,6))\nplt.title('Apples and Oranges', fontweight='bold', fontsize=16)\nplt.xlabel('Weight')\nplt.ylabel('Size')\nscatter = plt.scatter(dataset['Weight'], dataset['Size'], c=dataset['Class'], cmap='viridis')\nplt.legend(*scatter.legend_elements(),\n           loc = 'upper left',\n           title = 'Class')\nplt.plot(x1, x2, color='red', label='Decision Boundary')\nplt.show()\n\n\n\n\n\n\n\n\nIn this problem, we have just two features x1 and x2, if we use just those as they are we will end up with a straight line which divides our 2D plane into two half-planes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#non-linear-decision-boundary",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#non-linear-decision-boundary",
    "title": "8  Logistic regression: Others",
    "section": "8.2 Non-linear Decision Boundary",
    "text": "8.2 Non-linear Decision Boundary\nIn some occasions, we want to have a more complex boundary, and we can achieve this by transforming our features. For instance, when confronted with a training data distribution as illustrated below,  it becomes imperative to generate polynomial features \\[(x_1^2, x_2^2)\\] in order to enhance the delineation between the two classes. \nLet’s delve into a concrete example below to illustrate this concept.For the purpose of illustrating the decision boundary, I chose not to split the data into training and test sets.\n\n#Load our Dataset for Logistic Regression\ncomponents = pd.read_csv('datasets/ex2data2.txt', header=None, names = ['feature 1', 'feature 2', 'faulty'])\ncomponents.head()\n\n\n\n\n\n\n\n\nfeature 1\nfeature 2\nfaulty\n\n\n\n\n0\n0.051267\n0.69956\n1\n\n\n1\n-0.092742\n0.68494\n1\n\n\n2\n-0.213710\n0.69225\n1\n\n\n3\n-0.375000\n0.50219\n1\n\n\n4\n-0.513250\n0.46564\n1\n\n\n\n\n\n\n\n\n# check the balance of the dataset\ncomponents['faulty'].value_counts()\n\nfaulty\n0    60\n1    58\nName: count, dtype: int64\n\n\n\n# get positive and negative samples for plotting\npos = components['faulty'] == 1\nneg = components['faulty'] == 0\n\n\n# Visualize Data\nfig, axes = plt.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Non Faculty')\naxes.legend(title='Legend', loc = 'best' )\naxes.set_xlim(-1,1.5)\naxes.set_xlim(-1,1.5)\n\n\n\n\n\n\n\n\nAs we can see that the positive and negative examples are not linearly seperable. So we have to add additional higher order polynomial features.\n\n# define function to map higher order polynomial features\ndef mapFeature(X1, X2, degree):\n    res = np.ones(X1.shape[0])\n    for i in range(1,degree + 1):\n        for j in range(0,i + 1):\n            res = np.column_stack((res, (X1 ** (i-j)) * (X2 ** j)))\n    \n    return res\n\n\n# Get the features \nX = components.iloc[:, :2]\n\n\ndegree = 2\n\n\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n\n\n# Get the target variable\ny = components.iloc[:, 2]\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef costFunc(theta, X, y):\n    m = y.shape[0]\n    z = X.dot(theta)\n    h = sigmoid(z)\n    term1 = y * np.log(h)\n    term2 = (1- y) * np.log(1 - h)\n    J = -np.sum(term1 + term2, axis = 0) / m\n    return J \n\n\n# Set initial values for our parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)\n\n\n# Now call the optimization routine\n#NOTE: This automatically picks the learning rate\nfrom scipy.optimize import minimize\nres = minimize(costFunc, initial_theta.flatten(), args=(X_poly, y))\n\n\n# our optimizated coefficients\ntheta = res.x\n\n\n# define a function to plot the decision boundary\ndef plotDecisionBoundary(theta,degree, axes):\n    u = np.linspace(-1, 1.5, 50)\n    v = np.linspace(-1, 1.5, 50)\n    U,V = np.meshgrid(u,v)\n    # convert U, V to vectors for calculating additional features\n    # using vectorized implementation\n    U = np.ravel(U)\n    V = np.ravel(V)\n    Z = np.zeros((len(u) * len(v)))\n    \n    X_poly = mapFeature(U, V, degree)\n    Z = X_poly.dot(theta)\n    \n    # reshape U, V, Z back to matrix\n    U = U.reshape((len(u), len(v)))\n    V = V.reshape((len(u), len(v)))\n    Z = Z.reshape((len(u), len(v)))\n    \n    cs = axes.contour(U,V,Z,levels=[0],cmap= \"Greys_r\")\n    axes.legend(labels=['Non Faculty', 'faulty', 'Decision Boundary'])\n    return cs\n\n\n# Plot Decision boundary\nfig, axes = plt.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes);\n\n\n\n\n\n\n\n\nof course, you can increase the degree of the polynomial you want to fit, but the overfitting could become a problem\n\n# set degree = 6\ndegree = 6\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)\n\n\n# Run the optimization function\nres = minimize(costFunc, initial_theta.flatten(), args=(X_poly, y))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = plt.subplots()\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color='r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color='g', marker='o', label='Good')\n#axes.legend(title='Legend', loc='best')\n\nplotDecisionBoundary(theta, degree, axes)\n\n\n\n\n\n\n\n\nAs we can see the model tries pretty hard to capture every single example perfectly and overfits the data. This kind of model has overfitting issue. i.e The model has not pre-conceived notion about the seperation of the positive and negative examples and pretty much can fit any kind of data. Such model will fail in predicting the correct classification when it sees new examples.\nOne of techiques is to use regularization, which we will cover later. The idea is to penalize the algorithm when it tries to overfit by adding a regularization term to the cost function.\nThe New Cost function with the regularization is specified as\n\\(J(\\theta ) = \\frac{1}{m} \\sum_{i=1}^m[-y_i log(h_\\theta (z_i) – (1 – y_i) log(1-h_\\theta (z_i))] +\n                    \\frac{\\lambda}{2m} \\sum_{j=1}^n[\\theta_j^2]\\)\nwhere \\(\\lambda\\) = regularization factor  n = number of features.  (NOTE: The regularization term does include the intercept term \\(\\theta_0\\)\n\n8.2.1 By adding Polynomial Features\n\ncomponents\n\n\n\n\n\n\n\n\nfeature 1\nfeature 2\nfaulty\n\n\n\n\n0\n0.051267\n0.699560\n1\n\n\n1\n-0.092742\n0.684940\n1\n\n\n2\n-0.213710\n0.692250\n1\n\n\n3\n-0.375000\n0.502190\n1\n\n\n4\n-0.513250\n0.465640\n1\n\n\n...\n...\n...\n...\n\n\n113\n-0.720620\n0.538740\n0\n\n\n114\n-0.593890\n0.494880\n0\n\n\n115\n-0.484450\n0.999270\n0\n\n\n116\n-0.006336\n0.999270\n0\n\n\n117\n0.632650\n-0.030612\n0\n\n\n\n\n118 rows × 3 columns\n\n\n\n\nX = components[['feature 1', 'feature 2']]\ny = components['faulty']\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n\nfeature_names = poly.get_feature_names_out()\nprint(feature_names)\n\n['1' 'feature 1' 'feature 2' 'feature 1^2' 'feature 1 feature 2'\n 'feature 2^2']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_poly, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nlabel_predictions = model.predict(X_poly)\n\n\n8.2.1.1 Accuracy Score\n\naccuracy_score(y, label_predictions)\n\n0.8135593220338984\n\n\n\n\n8.2.1.2 Confusion Matrix\n\ncm = confusion_matrix(y, label_predictions)\ncm\n\narray([[47, 13],\n       [ 9, 49]], dtype=int64)\n\n\n\n\n\n8.2.2 By Transforming Continuous Variable\nVariable transformation is an important technique to create robust models using logistic regression. Because the predictors are linear in the log of the odds, it is often helpful to transform the continuous variables to create a more linear relationship. To determine the best transformation of a continuous variable, a univariate plot is very helpful. Remember the nice univariate plot of Y variable against X variable in linear regression? This is not easily attained, because Y is dichotomous in logistic regression.\nThere are different recommended solutions. Among them, * One is to create several variations (in forms of squared, cubed, or logged transformations etc.). * Another solution is to break some continuous variables into segments and treat them as categorical variables. This may work well to pick up nonlinear trends. The biggest drawback is that it loses the benefit of the linear trend relationship in the curve1. It also may lead to over fitting.\n\ntrain = pd.read_csv('./Datasets/Social_Network_Ads_train.csv') #Develop the model on train data\ntest = pd.read_csv('./Datasets/Social_Network_Ads_test.csv') #Test the model on test data\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n\n\n\n\n\n\ntrain.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 300 entries, 0 to 299\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   User ID          300 non-null    int64 \n 1   Gender           300 non-null    object\n 2   Age              300 non-null    int64 \n 3   EstimatedSalary  300 non-null    int64 \n 4   Purchased        300 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 11.8+ KB\n\n\n\ntrain.Gender.value_counts()\n\nGender\nFemale    151\nMale      149\nName: count, dtype: int64\n\n\n\ntmp_1 = pd.get_dummies(train['Gender'], drop_first=True)\ntrain = pd.concat([train, tmp_1], axis=1)\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n\n\n\n\n\n\n\n\ntmp_1 = pd.get_dummies(test['Gender'], drop_first=True)\ntest = pd.concat([test, tmp_1], axis=1)\ntest.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\n\n\n\n\n0\n15810944\nMale\n35\n20000\n0\nTrue\n\n\n1\n15668575\nFemale\n26\n43000\n0\nFalse\n\n\n2\n15603246\nFemale\n27\n57000\n0\nFalse\n\n\n3\n15694829\nFemale\n32\n150000\n1\nFalse\n\n\n4\n15697686\nMale\n29\n80000\n0\nTrue\n\n\n\n\n\n\n\n\n# Separating features and target on training set\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\"], axis = 1)\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nMale\n\n\n\n\n0\n36\n33000\nTrue\n\n\n1\n39\n61000\nFalse\n\n\n2\n36\n118000\nTrue\n\n\n3\n39\n122000\nTrue\n\n\n4\n26\n118000\nFalse\n\n\n...\n...\n...\n...\n\n\n295\n48\n96000\nFalse\n\n\n296\n42\n149000\nTrue\n\n\n297\n28\n79000\nTrue\n\n\n298\n51\n134000\nFalse\n\n\n299\n33\n28000\nFalse\n\n\n\n\n300 rows × 3 columns\n\n\n\n\n# Separating features and target on test set\ny_test = test.Purchased\nX_test = test.drop([\"Purchased\", \"Gender\", \"User ID\"], axis = 1)\nX_test\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nMale\n\n\n\n\n0\n35\n20000\nTrue\n\n\n1\n26\n43000\nFalse\n\n\n2\n27\n57000\nFalse\n\n\n3\n32\n150000\nFalse\n\n\n4\n29\n80000\nTrue\n\n\n...\n...\n...\n...\n\n\n95\n49\n39000\nFalse\n\n\n96\n47\n34000\nTrue\n\n\n97\n60\n42000\nTrue\n\n\n98\n39\n59000\nFalse\n\n\n99\n51\n23000\nTrue\n\n\n\n\n100 rows × 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model = LogisticRegression()\nsklearn_model.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\ny_pred_test = sklearn_model.predict(X_test)\nprint('Accuracy of logistic regression on test set : {:.4f}'.format(accuracy_score(y_test, y_pred_test )))\n\nAccuracy of logistic regression on test set : 0.8800\n\n\n\n8.2.2.1 Log transformation of salary\n\n\nsns.histplot(train.EstimatedSalary)\n\n\n\n\n\n\n\n\n\ntrain[\"log_salary\"] = np.log(train[\"EstimatedSalary\"])\nsns.histplot(train.log_salary)\n\n\n\n\n\n\n\n\nThe reason for such transformations have nothing to do with their distribution. Instead, the reason has to do with the functional form of the effect. Say we want to know the effect of the number of publications on the probability of getting tenure. It is reasonable to believe that getting an extra publication when one has only 1 publication has more impact compared with getting an extra publication when one has already published 50 articles. The log transformation is one way to capture such a (testable) assumption of diminishing returns.\n\ntest[\"log_salary\"] = np.log(test[\"EstimatedSalary\"])\n\n\n# Separating features and target\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\"], axis = 1)\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nMale\nlog_salary\n\n\n\n\n0\n36\nTrue\n10.404263\n\n\n1\n39\nFalse\n11.018629\n\n\n2\n36\nTrue\n11.678440\n\n\n3\n39\nTrue\n11.711776\n\n\n4\n26\nFalse\n11.678440\n\n\n...\n...\n...\n...\n\n\n295\n48\nFalse\n11.472103\n\n\n296\n42\nTrue\n11.911702\n\n\n297\n28\nTrue\n11.277203\n\n\n298\n51\nFalse\n11.805595\n\n\n299\n33\nFalse\n10.239960\n\n\n\n\n300 rows × 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model_log = LogisticRegression()\nsklearn_model_log.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Separating features and target for the test dataset\ny_test_log = test.Purchased\nX_test_log = test.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\"], axis = 1)\n\n\ny_log_pred_test = sklearn_model_log.predict(X_test_log)\nprint('Accuracy of logistic regression after log transformation of salary on test set : {:.4f}'.format(accuracy_score( y_test_log, y_log_pred_test)))\n\nAccuracy of logistic regression after log transformation of salary on test set : 0.8300\n\n\n\n\n\n8.2.3 By Binning Continous Variables\n\nsns.histplot(data=train.Age)\n\n\n\n\n\n\n\n\n\nbins = [train.Age.min()-1, 25, 35, 48, train.Age.max()]\nlabels = ['Big Kid', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Purchased\", data=train)\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n10.404263\nAdult\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n11.018629\nAdult\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n11.678440\nAdult\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n11.711776\nAdult\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n11.678440\nYoung Adult\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\nFalse\n11.472103\nAdult\n\n\n296\n15701537\nMale\n42\n149000\n1\nTrue\n11.911702\nAdult\n\n\n297\n15807481\nMale\n28\n79000\n0\nTrue\n11.277203\nYoung Adult\n\n\n298\n15603942\nFemale\n51\n134000\n0\nFalse\n11.805595\nSenior\n\n\n299\n15690188\nFemale\n33\n28000\n0\nFalse\n10.239960\nYoung Adult\n\n\n\n\n300 rows × 8 columns\n\n\n\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \ntrain['AgeGroup']= label_encoder.fit_transform(train['AgeGroup']) \ntrain['AgeGroup'].unique() \n\narray([0, 3, 1, 2])\n\n\n\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\ntest['AgeGroup']= label_encoder.fit_transform(test['AgeGroup']) \n\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n10.404263\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n11.018629\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n11.678440\n0\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n11.711776\n0\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n11.678440\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\nFalse\n11.472103\n0\n\n\n296\n15701537\nMale\n42\n149000\n1\nTrue\n11.911702\n0\n\n\n297\n15807481\nMale\n28\n79000\n0\nTrue\n11.277203\n3\n\n\n298\n15603942\nFemale\n51\n134000\n0\nFalse\n11.805595\n2\n\n\n299\n15690188\nFemale\n33\n28000\n0\nFalse\n10.239960\n3\n\n\n\n\n300 rows × 8 columns\n\n\n\n\n# Separating features and target on train set\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\", \"Age\"], axis = 1)\nX_train\n\n\n\n\n\n\n\n\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\nTrue\n10.404263\n0\n\n\n1\nFalse\n11.018629\n0\n\n\n2\nTrue\n11.678440\n0\n\n\n3\nTrue\n11.711776\n0\n\n\n4\nFalse\n11.678440\n3\n\n\n...\n...\n...\n...\n\n\n295\nFalse\n11.472103\n0\n\n\n296\nTrue\n11.911702\n0\n\n\n297\nTrue\n11.277203\n3\n\n\n298\nFalse\n11.805595\n2\n\n\n299\nFalse\n10.239960\n3\n\n\n\n\n300 rows × 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model_bin = LogisticRegression()\nsklearn_model_bin.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Separating features and target on test set\ny_test = test.Purchased\nX_test_bin = test.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\", \"Age\"], axis = 1)\nX_test_bin\n\n\n\n\n\n\n\n\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\nTrue\n9.903488\n3\n\n\n1\nFalse\n10.668955\n3\n\n\n2\nFalse\n10.950807\n3\n\n\n3\nFalse\n11.918391\n3\n\n\n4\nTrue\n11.289782\n3\n\n\n...\n...\n...\n...\n\n\n95\nFalse\n10.571317\n2\n\n\n96\nTrue\n10.434116\n0\n\n\n97\nTrue\n10.645425\n2\n\n\n98\nFalse\n10.985293\n0\n\n\n99\nTrue\n10.043249\n2\n\n\n\n\n100 rows × 3 columns\n\n\n\n\ny_bin_pred_test = sklearn_model_bin.predict(X_test_bin)\nprint('Accuracy of logistic regression after age binning on test set : {:.4f}'.format(accuracy_score( y_test, y_bin_pred_test)))\n\nAccuracy of logistic regression after age binning on test set : 0.7100",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#reference",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#reference",
    "title": "8  Logistic regression: Others",
    "section": "8.3 Reference",
    "text": "8.3 Reference\n\nhttps://www.linkedin.com/pulse/generating-non-linear-decision-boundaries-using-logistic-d-urso/\nhttps://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_4_Twoclass.html\nhttps://www.kaggle.com/code/lzs0047/logistic-regression-non-linear-decision-boundary/edit\nhttps://www.kaggle.com/code/ashishrane7/logistic-regression-non-linear-decision-boundary/notebook",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "9  Cross-Validation",
    "section": "",
    "text": "9.1 Review: Train-Test Split and Its Limitations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#review-train-test-split-and-its-limitations",
    "href": "cross_validation.html#review-train-test-split-and-its-limitations",
    "title": "9  Cross-Validation",
    "section": "",
    "text": "9.1.1 Train-Test Split Recap\nThroughout this course, we have used the train-test split approach to evaluate models:\n\nWe split the dataset into training and testing sets.\nThe training set is used to fit the model.\nThe testing set is used to evaluate performance on unseen data.\n\nThis approach provides a simple yet effective way to estimate out-of-sample performance. However, it has limitations:\n\n\n9.1.2 Limitations of Train-Test Split\n\nHigh Variance: The model’s performance depends on which specific observations end up in the training and testing sets. A different split might lead to different results.\nData Efficiency: A portion of the dataset is reserved for testing, meaning the model is not trained on all available data, which can be problematic for small datasets.\nInstability: A single split does not always provide a robust estimate of performance, especially when data is imbalanced or noisy.\n\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n\nfrom sklearn.datasets import make_classification\n\n%matplotlib inline\nplt.style.use('ggplot')\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 5, 4\n\n\n# Generate synthetic dataset for binary classification\nX, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n\nPlease execute the cell below multiple times. It will be evident that the accuracy score varies with each run due to different observations in both the training and test sets.\n\n# use train/test split \nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y)\n\n# Define the model\nlogit_model = LogisticRegression()\nlogit_model.fit(X_train_class, y_train_class)\ny_pred = logit_model.predict(X_test_class)\nprint(accuracy_score(y_test_class, y_pred))\n\n0.856",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-key-concepts",
    "href": "cross_validation.html#cross-validation-key-concepts",
    "title": "9  Cross-Validation",
    "section": "9.2 Cross-Validation: Key Concepts",
    "text": "9.2 Cross-Validation: Key Concepts\nTo address the limitations of train-test split, we introduce cross-validation, which provides a more reliable estimate of model performance by using multiple train-test splits.\n\n9.2.1 Steps for K-fold cross-validation\n\nSplit the dataset into K equal partitions (or “folds”).\nUse fold 1 as the testing set and the union of the other folds as the training set.\nCalculate testing accuracy.\nRepeat steps 2 and 3 K times, using a different fold as the testing set each time.\nUse the average testing accuracy as the estimate of out-of-sample accuracy.\n\nDiagram of 5-fold cross-validation:\n\n\n\n5-fold cross-validation\n\n\n\n# simulate splitting a dataset of 25 observations into 5 folds\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=False).split(range(25))\n\n# print the contents of each training and testing set\nprint('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))\nfor iteration, data in enumerate(kf, start=1):\n    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))\n\nIteration                   Training set observations                   Testing set observations\n    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       \n    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       \n    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     \n    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     \n    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]     \n\n\nKey takeaways\n\nThe dataset consists of 25 observations (indexed from 0 to 24).\n\nWe use 5-fold cross-validation, meaning the process runs for 5 iterations (one per fold).\n\nIn each iteration:\n\nThe dataset is split into a training set and a testing set.\n\nEach observation is included in either the training set or the testing set, but never both simultaneously.\n\n\nOver the entire cross-validation process:\n\nEach observation appears in the testing set exactly once.\n\nEach observation is included in the training set for (K-1) = 4 iterations.\n\n\nThis ensures that every data point contributes to both model training and evaluation, improving the robustness of performance estimates.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-in-scikit-learn",
    "href": "cross_validation.html#cross-validation-in-scikit-learn",
    "title": "9  Cross-Validation",
    "section": "9.3 Cross-Validation in Scikit-Learn",
    "text": "9.3 Cross-Validation in Scikit-Learn\n\n9.3.1 cross_val_score\ncross_val_score is a function in Scikit-Learn that simplifies k-fold cross-validation for model evaluation. It automates the process of splitting the dataset, training the model, and computing performance metrics across multiple folds.\n\nBy default, it uses 5-fold cross-validation (cv=5).\nFor classification models, it evaluates performance using accuracy as the default scoring metric.\nFor regression models, it uses R² (coefficient of determination) by default.\nThe function returns an array of scores, one for each fold, providing a more reliable estimate of model performance than a single train-test split.\n\nUsing cross_val_score ensures a more robust evaluation by reducing variance and making better use of available data.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(logit_model, X, y)\n\nprint(scores)\n\n[0.87  0.855 0.85  0.83  0.875]\n\n\nFinally, we compute the mean performance score across all folds to obtain a robust evaluation.\n\n# get the mean score\nscores.mean()\n\n0.8560000000000001\n\n\nBy default, cross_val_score in Scikit-Learn performs k-fold cross-validation using the default cross-validator for the given estimator type.\n\n9.3.1.1 Default k-fold cross-validation Settings\ncv=5 unless specified\n\n\n\n\n\n\n\n\n\nModel Type\nDefault Cross-Validator\nShuffling\nStratification\n\n\n\n\nClassification\nStratifiedKFold(n_splits=5)\n❌ No\n✅ Yes\n\n\nRegression\nKFold(n_splits=5)\n❌ No\n❌ No\n\n\n\n\n\n9.3.1.2 How to Modify the Behavior\nIf you need shuffling or a different cross-validation strategy, specify a custom cross-validator.\n\n9.3.1.2.1 Enable Shuffling\n\n# Turn on the shuffle\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(logit_model, X, y, cv=kf, scoring='accuracy')\nscores\n\narray([0.83 , 0.875, 0.87 , 0.85 , 0.865])\n\n\n\n\n9.3.1.2.2 Stratified Splitting for Classification\nFor classification problems, stratified sampling is recommended for creating the folds\n\nEach response class should be represented with equal proportions in each of the K folds\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(logit_model, X, y, cv=skf, scoring='accuracy')\nscores\n\narray([0.83 , 0.84 , 0.875, 0.89 , 0.845])\n\n\n\n\n9.3.1.2.3 Use Leave-One-Out (LOO) Cross-Validation\nIf you don’t have much data, so any split from the full set to the training and validation set is going to result in really very few observations on which you can train. Leave-on-out cross validation (LOOCV) that might work better for cross-validation. Say you have 16 observations. Train on 15 and validate on the other one. Repeat this until you have trained on every set of 15 with the 16th sample left out.\n\nfrom sklearn.model_selection import LeaveOneOut\n\nloo = LeaveOneOut()\nscores = cross_val_score(logit_model, X, y, cv=loo, scoring='accuracy')\nprint(len(scores))\nscores[:10]\n\n1000\n\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\nTypes of Cross-Validation\n\n\n\n\n\n\n\n\nMethod\nDescription\nUse Case\n\n\n\n\nk-Fold CV\nSplits data into k folds, each used for training/testing\nMost common approach (e.g., k=5 or k=10)\n\n\nStratified k-Fold\nLike k-Fold, but preserves class proportions\nUseful for imbalanced classification\n\n\nLeave-One-Out (LOO-CV)\nUses each sample as a test set once\nFor very small datasets\n\n\nLeave-P-Out (LPO-CV)\nLeaves out p samples for testing in each iteration\nComputationally expensive\n\n\nTime Series CV\nEnsures training data precedes test data (rolling windows)\nTime-series forecasting problems\n\n\n\n\n\n\n9.3.1.3 Changing the Scoring Metric in cross_val_score\nwe can specify a different evaluation metric using the scoring parameter. Scikit-Learn provides various built-in metrics, including F1-score, precision, recall, and more.\nYou can specify different metrics based on the type of model:\n\n\n\n\n\n\n\n\nTask Type\nMetric Name (for scoring)\nDescription\n\n\n\n\nClassification\n'accuracy'\nDefault, ratio of correct predictions\n\n\nClassification\n'precision', 'recall'\nMeasure of correctness for positive class\n\n\nClassification\n'f1', 'f1_macro'\nHarmonic mean of precision & recall\n\n\nRegression\n'neg_mean_squared_error'\nMSE (lower is better)\n\n\nRegression\n'r2'\nDefault, measures variance explained\n\n\n\nYou can refer to the scikit-learn documentation. If a built-in metric doesn’t fit your needs, you can define a custom scoring function\nFor classification problems, especially imbalanced datasets, F1-score is a better metric than accuracy as it considers both precision and recall. Let’s use it as our metric next\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\nscores = cross_val_score(logit_model, X, y, cv=folds, scoring= \"f1\")\nscores \n\narray([0.875     , 0.81632653, 0.88151659, 0.87958115, 0.85869565])\n\n\n\n\n\n9.3.2 cross_validate in Scikit-Learn\nThe cross_validate function provides a more comprehensive evaluation of a model compared to cross_val_score.\nIts Key Feature includes:\n\nAllows multiple evaluation metrics to be specified at once.\nReturns a detailed dictionary containing:\n\nTraining scores and test scores across different folds.\nFit times (time taken to train the model for each fold).\nScore times (time taken to evaluate the model for each fold).\n\nUseful for:\n\nAnalyzing training/testing time variability across folds.\nComparing multiple performance metrics simultaneously to get a more complete picture of model performance.\n\n\nThis function is ideal when you need deeper insights into how your model behaves across different folds, beyond just performance scores.\n\nfrom sklearn.model_selection import cross_validate\n\nscores = cross_validate(logit_model, X, y, scoring=\"accuracy\", return_train_score=True)\n \ndf_scores = pd.DataFrame(scores)\ndf_scores\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_score\ntrain_score\n\n\n\n\n0\n0.007137\n0.001002\n0.870\n0.86125\n\n\n1\n0.006532\n0.000997\n0.855\n0.86750\n\n\n2\n0.004417\n0.000998\n0.850\n0.87375\n\n\n3\n0.001992\n0.001005\n0.830\n0.87000\n\n\n4\n0.001035\n0.000000\n0.875\n0.85750\n\n\n\n\n\n\n\nLet’s use multiple metrics\n\n# Define scoring metrics explicitly for multiclass\nscoring = ['accuracy', 'recall', 'precision', 'f1', 'roc_auc']\n\n# Perform cross-validation\nscores = cross_validate(logit_model, X, y, scoring=scoring, return_train_score=True)\n\n# Convert to DataFrame for better readability\ndf_scores = pd.DataFrame(scores)\ndf_scores\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_accuracy\ntrain_accuracy\ntest_recall\ntrain_recall\ntest_precision\ntrain_precision\ntest_f1\ntrain_f1\ntest_roc_auc\ntrain_roc_auc\n\n\n\n\n0\n0.003802\n0.000000\n0.870\n0.86125\n0.868687\n0.852500\n0.868687\n0.867684\n0.868687\n0.860025\n0.941394\n0.931588\n\n\n1\n0.004556\n0.004991\n0.855\n0.86750\n0.810000\n0.862155\n0.890110\n0.870886\n0.848168\n0.866499\n0.944400\n0.931368\n\n\n2\n0.003989\n0.004989\n0.850\n0.87375\n0.840000\n0.867168\n0.857143\n0.878173\n0.848485\n0.872636\n0.926300\n0.936212\n\n\n3\n0.003987\n0.004063\n0.830\n0.87000\n0.840000\n0.862155\n0.823529\n0.875318\n0.831683\n0.868687\n0.905200\n0.940987\n\n\n4\n0.000000\n0.011978\n0.875\n0.85750\n0.890000\n0.847118\n0.864078\n0.864450\n0.876847\n0.855696\n0.940200\n0.932781\n\n\n\n\n\n\n\n\n\n9.3.3 cross_val_predict in sklearn\ncross_val_predict is a function in Scikit-Learn that performs cross-validation but instead of returning evaluation scores, it returns predicted values for each instance in the dataset as if they were unseen.\n\n9.3.3.1 Process:\n\nThe dataset is split into k folds.\nThe model is trained on k-1 folds.\nPredictions are made on the remaining fold.\nThis process repeats until every instance has been predicted once, ensuring that each prediction is made on unseen data.\n\nThis approach mimics real-world predictions, making it useful for evaluating model performance with classification reports, confusion matrices, and ROC curves.\n\nfrom sklearn.model_selection import cross_val_predict\n\n# output the predicted probabilities\ny_pred_prob = cross_val_predict(logit_model, X, y, cv=5, method='predict_proba')[:,1]\nprint(y_pred_prob.shape)\nprint(y_pred_prob[:10])\n\n(1000,)\n[0.04211674 0.97024937 0.96688462 0.01087727 0.65634726 0.02306063\n 0.14247751 0.92610262 0.8783967  0.2408313 ]\n\n\n\n\n\n9.3.4 Key Differences Between cross_val_score, cross_validate, and cross_val_predict\n\n\n\n\n\n\n\n\n\n\n\nFunction\nReturns\nRequires scoring?\nSupports Multiple Metrics?\nCan Return Train Scores?\nPurpose\n\n\n\n\ncross_val_score\nArray of scores\n❌ No (defaults to accuracy for classification)\n❌ No\n❌ No\nEvaluates model performance using cross-validation\n\n\ncross_validate\nDictionary\n✅ Yes (must specify)\n✅ Yes (scoring={'accuracy', 'precision'})\n✅ Yes (return_train_score=True)\nProvides detailed evaluation metrics, including training times\n\n\ncross_val_predict\nArray of predictions\n❌ No (uses predict or predict_proba)\n❌ No\n❌ No\nGenerates out-of-sample predictions for each instance\n\n\n\n\n\n9.3.5 Advantages and Disadvantages of Cross-Validation\n\n9.3.5.1 Advantages:\n\nReduces Variance: By averaging results over multiple folds, cross-validation provides a more stable estimate of model performance.\n\nBetter Use of Data: Every observation gets a chance to be in both the training and testing sets, improving data efficiency.\n\nMore Reliable Performance Metrics: The results are less dependent on a single random split, making the evaluation more robust.\n\n\n\n9.3.5.2 Disadvantages Compared to Train-Test Split:\n\nHigher Computational Cost: Instead of training the model once (as in a train-test split), cross-validation requires training the model k times, making it computationally expensive for large datasets.\n\nPotential Overfitting to Small Data: If k is too large (e.g., Leave-One-Out Cross-Validation), it can lead to high variance and make the model too sensitive to small changes in data.\n\nChoosing k: Typically, k=5 or k=10 is recommended as a balance between bias and variance.\n\nCross-validation is preferable for small-to-moderate datasets where stable performance estimates are important, while a simple train-test split is often sufficient for large datasets when computational efficiency is a priority.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-for-hyperparameter-tuning",
    "href": "cross_validation.html#cross-validation-for-hyperparameter-tuning",
    "title": "9  Cross-Validation",
    "section": "9.4 Cross-Validation for Hyperparameter Tuning",
    "text": "9.4 Cross-Validation for Hyperparameter Tuning\nCross-validation is a powerful technique that can be used for:\n\nModel Performance Evaluation: Provides a more reliable estimate of how well a model generalizes to unseen data.\nHyperparameter Tuning: Helps find the optimal model parameters by evaluating different configurations.\n\nModel Selection: Compares multiple models to choose the one that performs best across different folds.\n\nFeature Selection: Assesses the impact of different feature subsets on model performance.\n\nIn this notebook, we focus exclusively on Hyperparameter Tuning with Cross-Validation.\nWe demonstrate how cross-validation can be used to systematically search for the best hyperparameters, ensuring better generalization and optimized model performance.\n\n9.4.1 Finding the optimal Degree in Polynomial Regression\n\n9.4.1.1 Background: Polynomial Regression\nYou already know simple linear regression:\n\\(y = \\beta_0 + \\beta_1 x_1\\)\nIn polynomial regression of degree \\(n\\), we fit a curve of the form:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_1^2 + \\beta_3x_1^3 ... + \\beta_nx_1^n\\)\nIn the experiment below, we fit polynomial models of varying degrees on simulated data to analyze their performance.\nWe build a linear regression model and use cross-validation to tune the polynomial degree (p).\nBy selecting the optimal degree, we aim to balance the trade-off between underfitting and overfitting, ensuring the model generalizes well to unseen data.\nSpecifically, we Will Cover:\n\nUsing cross_val_score for hyperparameter tuning to evaluate model performance across folds.\n\nUsing cross_validate to obtain detailed metrics, including training scores and fit times.\n\nApplying GridSearchCV to systematically search for the optimal polynomial degree.\n\n\n\n9.4.1.2 Finding the Optimal Degree with cross_val_score\n\n9.4.1.2.1 Step 1: Let’s begin by importing the required libraries.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n\n\n\n9.4.1.2.2 Step 2: Let’s generate input data\n\n# Simulate input data\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(360)])\nnp.random.seed(10)  #Setting seed for reproducibility\ny = np.sin(x) + np.random.normal(0,0.15,len(x))\ndata = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\nplt.plot(data['x'],data['y'],'.');\n\n\n\n\n\n\n\n\n\n\n9.4.1.2.3 Step 3: Train-Test Split\n\n# Split the data\nX = data['x'].values.reshape(-1, 1)\ny = data['y'].values\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n\n9.4.1.2.4 Step 4: Using cross_val_score to tune the p in polynomialFeatures\nIn sklearn, polynomial features can be generated using the PolynomialFeatures class. Also, to perform LinearRegression and PolynomialFeatures in tandem, we will use the module sklearn_pipeline - it basically creates the features and feeds the output to the model (in that sequence).\n\ndef cross_validation_score(X_train, y_train, max_degree=10, scoring='r2'):\n    \"\"\"\n    Perform cross-validation for polynomial regression models with different degrees.\n\n    Parameters:\n        X_train (array-like): Training feature data.\n        y_train (array-like): Training target data.\n        max_degree (int): Maximum polynomial degree to evaluate.\n        scoring (str): Scoring metric ('r2' for R², 'rmse' for root mean squared error).\n\n    Returns:\n        degrees (list): List of polynomial degrees evaluated.\n        scores_df (DataFrame): DataFrame of cross-validation scores across different degrees.\n    \"\"\"\n    degrees = range(1, max_degree + 1)\n    cv_scores = []\n\n    for degree in degrees:\n        # Create polynomial regression model\n        model = make_pipeline(\n            PolynomialFeatures(degree),\n            LinearRegression()\n        )\n\n        # Compute cross-validation scores\n        if scoring == 'rmse':\n            raw_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n            cv_score = np.sqrt(-raw_scores)  # Convert negative MSE to RMSE\n            score_label = \"RMSE (lower is better)\"\n        else:  # Default to R²\n            cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n            score_label = \"R² (higher is better)\"\n\n        cv_scores.append(cv_score)\n\n    # Convert scores to a DataFrame\n    scores_df = pd.DataFrame(np.array(cv_scores), index=degrees, columns=[f'Fold {i+1}' for i in range(cv_scores[0].shape[0])])\n    \n    print(f\"Cross-validation scores ({score_label}):\")\n    return degrees, scores_df\n\n\n# Example Usage:\nscoring_metric = 'rmse'  # Change to 'r2' if needed\ndegrees, scores = cross_validation_score(X_train, y_train, scoring=\"rmse\")\n\n# Print the formatted score matrix\nprint(scores)\n\nCross-validation scores (RMSE (lower is better)):\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n1   0.500794  0.456810  0.472232  0.484231  0.461889\n2   0.500255  0.455897  0.475378  0.485450  0.461456\n3   0.164968  0.158555  0.156423  0.152829  0.161931\n4   0.165521  0.158619  0.156413  0.153359  0.162379\n5   0.145887  0.138576  0.143649  0.141445  0.152968\n6   0.145995  0.139056  0.143390  0.144567  0.153870\n7   0.151113  0.138186  0.144122  0.144211  0.153438\n8   0.150406  0.137350  0.143297  0.146944  0.154557\n9   0.151095  0.138084  0.143219  0.146828  0.154599\n10  0.152122  0.139420  0.145089  0.147192  0.154581\n\n\n\n# take mean of each row and add it as the last column of the scores dataframe\nscores['mean'] = scores.mean(axis=1)\nprint(scores)\n\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5      mean\n1   0.500794  0.456810  0.472232  0.484231  0.461889  0.475191\n2   0.500255  0.455897  0.475378  0.485450  0.461456  0.475687\n3   0.164968  0.158555  0.156423  0.152829  0.161931  0.158941\n4   0.165521  0.158619  0.156413  0.153359  0.162379  0.159258\n5   0.145887  0.138576  0.143649  0.141445  0.152968  0.144505\n6   0.145995  0.139056  0.143390  0.144567  0.153870  0.145376\n7   0.151113  0.138186  0.144122  0.144211  0.153438  0.146214\n8   0.150406  0.137350  0.143297  0.146944  0.154557  0.146511\n9   0.151095  0.138084  0.143219  0.146828  0.154599  0.146765\n10  0.152122  0.139420  0.145089  0.147192  0.154581  0.147681\n\n\n\n# plot the mean scores for each degree\nplt.figure(figsize=(12, 6))\nplt.plot(degrees, scores['mean'], 'o-', label='Cross-validation RMSE')\nplt.xlabel('Degree')\nplt.ylabel('Mean CV Score')\nplt.title('Polynomial Degree vs. Mean CV RMSE')\nplt.grid(True)\nplt.legend();\n\n\n\n\n\n\n\n\n\n# Find optimal degree based on the mean cross-validation scores\noptimal_degree = scores['mean'].idxmin()\nprint(f'Optimal polynomial degree: {optimal_degree}')\n\nOptimal polynomial degree: 5\n\n\n\n# use r2 as performance metric, note that r2 is default metric in the function defination\ndegrees, scores = cross_validation_score(X_train, y_train)\n\n# Print the formatted score matrix\nprint(scores)\n\nCross-validation scores (R² (higher is better)):\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n1   0.474906  0.587650  0.567349  0.584471  0.590389\n2   0.476036  0.589297  0.561564  0.582378  0.591156\n3   0.943020  0.950323  0.952529  0.958609  0.949655\n4   0.942638  0.950283  0.952535  0.958321  0.949376\n5   0.955439  0.962054  0.959965  0.964546  0.955074\n6   0.955373  0.961790  0.960110  0.962963  0.954543\n7   0.952190  0.962267  0.959701  0.963145  0.954797\n8   0.952636  0.962722  0.960162  0.961735  0.954136\n9   0.952201  0.962322  0.960205  0.961795  0.954111\n10  0.951549  0.961590  0.959159  0.961606  0.954121\n\n\n\n# take mean of each row and add it as the last column of the scores dataframe\nscores['mean'] = scores.mean(axis=1)\nprint(scores)\n\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5      mean\n1   0.474906  0.587650  0.567349  0.584471  0.590389  0.560953\n2   0.476036  0.589297  0.561564  0.582378  0.591156  0.560086\n3   0.943020  0.950323  0.952529  0.958609  0.949655  0.950827\n4   0.942638  0.950283  0.952535  0.958321  0.949376  0.950631\n5   0.955439  0.962054  0.959965  0.964546  0.955074  0.959416\n6   0.955373  0.961790  0.960110  0.962963  0.954543  0.958956\n7   0.952190  0.962267  0.959701  0.963145  0.954797  0.958420\n8   0.952636  0.962722  0.960162  0.961735  0.954136  0.958278\n9   0.952201  0.962322  0.960205  0.961795  0.954111  0.958127\n10  0.951549  0.961590  0.959159  0.961606  0.954121  0.957605\n\n\n\n# plot the mean scores for each degree\nplt.figure(figsize=(12, 6))\nplt.plot(degrees, scores['mean'], 'o-', label='Cross-validation R²')\nplt.xlabel('Degree')\nplt.ylabel('Mean CV Score')\nplt.title('Polynomial Degree vs. Mean CV Score')\nplt.grid(True)\nplt.legend();\n\n\n\n\n\n\n\n\n\n# Find optimal degree based on the mean cross-validation scores\noptimal_degree = scores['mean'].idxmax()\nprint(f'Optimal polynomial degree: {optimal_degree}')\n\nOptimal polynomial degree: 5\n\n\n\n\n9.4.1.2.5 Step 5: Fitting the Final Model with the Optimal Polynomial Degree\nAfter determining the optimal polynomial degree (p=5) using cross-validation, we now refit the model on the entire training dataset.\nThis ensures that the model fully utilizes all available training data for the best possible fit.\nSteps: 1. Build the final model using a polynomial transformation with the optimal degree (p=5). 2. Train the model on the full training dataset. 3. Generate predictions and visualize how well the polynomial regression fits the data.\nThe plot below shows the final polynomial regression fit, highlighting how the model captures patterns in the dataset.\n\n# Fit final model with optimal degree\nfinal_model = make_pipeline(\n    PolynomialFeatures(optimal_degree),\n    LinearRegression()\n)\nfinal_model.fit(X_train, y_train)\n\n# Plot final model predictions\nplt.figure(figsize=(12, 6))\nX_sorted = np.sort(X)\ny_pred = final_model.predict(X_sorted)\n\nplt.scatter(X, y, color='blue', alpha=0.5, label='Data points')\nplt.plot(X_sorted, y_pred, color='red', label=f'Polynomial degree {optimal_degree}')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Polynomial Regression Fit')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.4.1.2.6 Step 6: Output the final model performance\n\n# Print final model performance metrics\nprint(\"\\nFinal Model Performance:\")\nprint(f\"Training R²: {final_model.score(X_train, y_train):.4f}\")\nprint(f\"Test R²: {final_model.score(X_test, y_test):.4f}\")\n\n\nFinal Model Performance:\nTraining R²: 0.9613\nTest R²: 0.9521\n\n\n\n\n\n9.4.1.3 Hyperparameter Tuning with cross_validate\nThe cross_validate function differs from cross_val_score in two ways:\n\nIt allows specifying multiple metrics for evaluation.\nIt returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score.\n\n\nfrom sklearn.model_selection import cross_validate\n\ndef cross_validation_full_results(X_train, y_train, max_degree=10):\n    \"\"\"\n    Perform cross-validation for polynomial regression models with different degrees.\n    Captures all cross-validation results including R², RMSE, fit time, score time, etc.\n\n    Parameters:\n        X_train (array-like): Training feature data.\n        y_train (array-like): Training target data.\n        max_degree (int): Maximum polynomial degree to evaluate.\n\n    Returns:\n        full_results_df (DataFrame): DataFrame containing all cross-validation metrics for each degree.\n    \"\"\"\n    degrees = []\n    results_dict = {\n        \"Fold\": [],\n        \"Fit Time\": [],\n        \"Score Time\": [],\n        \"R²\": [],\n        \"RMSE\": []\n    }\n\n    for degree in range(1, max_degree + 1):\n        # Create polynomial regression model\n        model = make_pipeline(\n            PolynomialFeatures(degree),\n            LinearRegression()\n        )\n\n        # Perform cross-validation for both R² and RMSE, capturing additional metrics\n        cv_results = cross_validate(model, X_train, y_train, cv=5, \n                                    scoring=['r2', 'neg_root_mean_squared_error'], return_train_score=True)\n        \n        # Store each fold's results separately\n        for fold in range(5):  # 5 folds\n            degrees.append(degree)\n            results_dict[\"Fold\"].append(fold + 1)\n            results_dict[\"Fit Time\"].append(cv_results[\"fit_time\"][fold])\n            results_dict[\"Score Time\"].append(cv_results[\"score_time\"][fold])\n            results_dict[\"R²\"].append(cv_results[\"test_r2\"][fold])\n            results_dict[\"RMSE\"].append(-cv_results[\"test_neg_root_mean_squared_error\"][fold])  # Convert negative RMSE to positive\n\n    # Convert to DataFrame\n    full_results_df = pd.DataFrame(results_dict)\n    full_results_df.insert(0, \"Degree\", degrees)  # Add Degree column at the front\n\n    print(\"Complete Cross-Validation Results (Higher R² is better, Lower RMSE is better, Fit Time & Score Time included):\")\n    return full_results_df\n\nscores_df = cross_validation_full_results(X_train, y_train, max_degree=10)\n\nscores_df\n\nComplete Cross-Validation Results (Higher R² is better, Lower RMSE is better, Fit Time & Score Time included):\n\n\n\n\n\n\n\n\n\nDegree\nFold\nFit Time\nScore Time\nR²\nRMSE\n\n\n\n\n0\n1\n1\n0.001993\n0.002523\n0.474906\n0.500794\n\n\n1\n1\n2\n0.001994\n0.002033\n0.587650\n0.456810\n\n\n2\n1\n3\n0.000000\n0.000000\n0.567349\n0.472232\n\n\n3\n1\n4\n0.000996\n0.000996\n0.584471\n0.484231\n\n\n4\n1\n5\n0.000997\n0.001993\n0.590389\n0.461889\n\n\n5\n2\n1\n0.001482\n0.000000\n0.476036\n0.500255\n\n\n6\n2\n2\n0.006236\n0.002004\n0.589297\n0.455897\n\n\n7\n2\n3\n0.000996\n0.000997\n0.561564\n0.475378\n\n\n8\n2\n4\n0.000997\n0.000997\n0.582378\n0.485450\n\n\n9\n2\n5\n0.000997\n0.000997\n0.591156\n0.461456\n\n\n10\n3\n1\n0.000996\n0.000997\n0.943020\n0.164968\n\n\n11\n3\n2\n0.000997\n0.001250\n0.950323\n0.158555\n\n\n12\n3\n3\n0.001089\n0.000000\n0.952529\n0.156423\n\n\n13\n3\n4\n0.000000\n0.000000\n0.958609\n0.152829\n\n\n14\n3\n5\n0.000000\n0.000000\n0.949655\n0.161931\n\n\n15\n4\n1\n0.000000\n0.000000\n0.942638\n0.165521\n\n\n16\n4\n2\n0.000000\n0.000000\n0.950283\n0.158619\n\n\n17\n4\n3\n0.000996\n0.000996\n0.952535\n0.156413\n\n\n18\n4\n4\n0.000997\n0.000996\n0.958321\n0.153359\n\n\n19\n4\n5\n0.000997\n0.000997\n0.949376\n0.162379\n\n\n20\n5\n1\n0.000996\n0.000992\n0.955439\n0.145887\n\n\n21\n5\n2\n0.000997\n0.001001\n0.962054\n0.138576\n\n\n22\n5\n3\n0.000997\n0.000993\n0.959965\n0.143649\n\n\n23\n5\n4\n0.000996\n0.000997\n0.964546\n0.141445\n\n\n24\n5\n5\n0.000000\n0.000000\n0.955074\n0.152968\n\n\n25\n6\n1\n0.000000\n0.000000\n0.955373\n0.145995\n\n\n26\n6\n2\n0.000000\n0.000000\n0.961790\n0.139056\n\n\n27\n6\n3\n0.000000\n0.000000\n0.960110\n0.143390\n\n\n28\n6\n4\n0.000000\n0.000996\n0.962963\n0.144567\n\n\n29\n6\n5\n0.000000\n0.000997\n0.954543\n0.153870\n\n\n30\n7\n1\n0.000998\n0.001304\n0.952190\n0.151113\n\n\n31\n7\n2\n0.000997\n0.001003\n0.962267\n0.138186\n\n\n32\n7\n3\n0.000000\n0.000997\n0.959701\n0.144122\n\n\n33\n7\n4\n0.000000\n0.000993\n0.963145\n0.144211\n\n\n34\n7\n5\n0.000997\n0.000997\n0.954797\n0.153438\n\n\n35\n8\n1\n0.000997\n0.000842\n0.952636\n0.150406\n\n\n36\n8\n2\n0.000000\n0.000000\n0.962722\n0.137350\n\n\n37\n8\n3\n0.000000\n0.000000\n0.960162\n0.143297\n\n\n38\n8\n4\n0.010667\n0.000995\n0.961735\n0.146944\n\n\n39\n8\n5\n0.000000\n0.000000\n0.954136\n0.154557\n\n\n40\n9\n1\n0.000000\n0.000000\n0.952201\n0.151095\n\n\n41\n9\n2\n0.000000\n0.000000\n0.962322\n0.138084\n\n\n42\n9\n3\n0.000000\n0.000000\n0.960205\n0.143219\n\n\n43\n9\n4\n0.000996\n0.001993\n0.961795\n0.146828\n\n\n44\n9\n5\n0.000170\n0.000000\n0.954111\n0.154599\n\n\n45\n10\n1\n0.000000\n0.000000\n0.951549\n0.152122\n\n\n46\n10\n2\n0.000000\n0.000000\n0.961590\n0.139420\n\n\n47\n10\n3\n0.000000\n0.012748\n0.959159\n0.145089\n\n\n48\n10\n4\n0.000996\n0.001614\n0.961606\n0.147192\n\n\n49\n10\n5\n0.000000\n0.000000\n0.954121\n0.154581\n\n\n\n\n\n\n\n\nmean_results_df = scores_df.groupby(\"Degree\")[[\"R²\", \"RMSE\"]].mean().reset_index()\nmean_results_df\n\n\n\n\n\n\n\n\nDegree\nR²\nRMSE\n\n\n\n\n0\n1\n0.560953\n0.475191\n\n\n1\n2\n0.560086\n0.475687\n\n\n2\n3\n0.950827\n0.158941\n\n\n3\n4\n0.950631\n0.159258\n\n\n4\n5\n0.959416\n0.144505\n\n\n5\n6\n0.958956\n0.145376\n\n\n6\n7\n0.958420\n0.146214\n\n\n7\n8\n0.958278\n0.146511\n\n\n8\n9\n0.958127\n0.146765\n\n\n9\n10\n0.957605\n0.147681\n\n\n\n\n\n\n\n\n\n9.4.1.4 Grid Search Cross-Validation for Hyperparameter Tuning\nA common use of cross-validation is tuning the hyperparameters of a model. One of the most widely used techniques for this is grid search cross-validation.\nHow Grid Search Cross-Validation Works\n\nDefine a Grid of Hyperparameters:\nWe specify a set of hyperparameters and the possible values we want to evaluate for each.\nEvaluate All Combinations:\nEvery possible combination of hyperparameter values in the grid is systematically tested.\nCross-Validation for Model Evaluation:\nFor each combination, the model is trained and evaluated using cross-validation to assess performance.\nSelect the Best Hyperparameter Setting:\nThe combination that yields the best validation performance is chosen as the optimal set of hyperparameters.\n\nGrid search ensures that we explore multiple hyperparameter settings in a structured way, improving the model’s performance without manually adjusting parameters.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the pipeline\nmodel = make_pipeline(PolynomialFeatures(), LinearRegression())\n\n# Define the hyperparameter grid\nparam_grid = {'polynomialfeatures__degree': np.arange(1, 11)}\n\n# define a KFold cross-validation with shuffling\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(model, param_grid, cv=cv, scoring= 'r2', return_train_score=True, verbose=1, n_jobs=-1)\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('linearregression',\n                                        LinearRegression())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             return_train_score=True, scoring='r2', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('linearregression',\n                                        LinearRegression())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             return_train_score=True, scoring='r2', verbose=1) best_estimator_: PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=5) LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_polynomialfeatures__degree\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\n...\nmean_test_score\nstd_test_score\nrank_test_score\nsplit0_train_score\nsplit1_train_score\nsplit2_train_score\nsplit3_train_score\nsplit4_train_score\nmean_train_score\nstd_train_score\n\n\n\n\n0\n0.014676\n0.001534\n0.002452\n0.000697\n1\n{'polynomialfeatures__degree': 1}\n0.514869\n0.594851\n0.583781\n0.548702\n...\n0.543991\n0.043422\n9\n0.580311\n0.549217\n0.565220\n0.567494\n0.584039\n0.569256\n0.012343\n\n\n1\n0.014464\n0.001952\n0.002246\n0.000386\n2\n{'polynomialfeatures__degree': 2}\n0.516412\n0.596158\n0.585003\n0.539017\n...\n0.543109\n0.043391\n10\n0.580415\n0.549575\n0.565611\n0.569567\n0.584617\n0.569957\n0.012318\n\n\n2\n0.010744\n0.001704\n0.001196\n0.000399\n3\n{'polynomialfeatures__degree': 3}\n0.948330\n0.954229\n0.939908\n0.950587\n...\n0.948949\n0.004903\n7\n0.952702\n0.950599\n0.955068\n0.951851\n0.951932\n0.952430\n0.001481\n\n\n3\n0.003352\n0.001858\n0.001900\n0.000197\n4\n{'polynomialfeatures__degree': 4}\n0.948137\n0.954200\n0.938938\n0.950252\n...\n0.948634\n0.005233\n8\n0.952718\n0.950600\n0.955190\n0.951892\n0.951934\n0.952467\n0.001522\n\n\n4\n0.003586\n0.002212\n0.001844\n0.000487\n5\n{'polynomialfeatures__degree': 5}\n0.957618\n0.965540\n0.945282\n0.960822\n...\n0.958620\n0.007195\n1\n0.961771\n0.959014\n0.965001\n0.960735\n0.960368\n0.961378\n0.002015\n\n\n5\n0.002876\n0.001588\n0.001994\n0.000631\n6\n{'polynomialfeatures__degree': 6}\n0.957569\n0.965655\n0.943992\n0.960471\n...\n0.958278\n0.007659\n2\n0.961771\n0.959019\n0.965278\n0.960858\n0.960459\n0.961477\n0.002097\n\n\n6\n0.003966\n0.003833\n0.001792\n0.000374\n7\n{'polynomialfeatures__degree': 7}\n0.957219\n0.965979\n0.944374\n0.960850\n...\n0.958136\n0.007431\n3\n0.962026\n0.959143\n0.965312\n0.960954\n0.960862\n0.961659\n0.002046\n\n\n7\n0.002766\n0.000773\n0.001784\n0.000394\n8\n{'polynomialfeatures__degree': 8}\n0.957222\n0.966218\n0.944548\n0.960278\n...\n0.957822\n0.007242\n4\n0.962026\n0.959278\n0.965478\n0.961240\n0.961259\n0.961856\n0.002026\n\n\n8\n0.002394\n0.000487\n0.001599\n0.000487\n9\n{'polynomialfeatures__degree': 9}\n0.955364\n0.965595\n0.944424\n0.960261\n...\n0.957306\n0.007211\n5\n0.962294\n0.959391\n0.965482\n0.961240\n0.961259\n0.961933\n0.002006\n\n\n9\n0.002936\n0.000066\n0.001993\n0.000036\n10\n{'polynomialfeatures__degree': 10}\n0.955130\n0.965594\n0.944371\n0.959727\n...\n0.957090\n0.007175\n6\n0.962307\n0.959391\n0.965501\n0.961302\n0.961271\n0.961955\n0.002008\n\n\n\n\n10 rows × 21 columns\n\n\n\n\npd.set_option('display.float_format', '{:.6f}'.format) \ncv_results[[\"mean_test_score\", \"mean_train_score\"]]\n\n\n\n\n\n\n\n\nmean_test_score\nmean_train_score\n\n\n\n\n0\n0.543991\n0.569256\n\n\n1\n0.543109\n0.569957\n\n\n2\n0.948949\n0.952430\n\n\n3\n0.948634\n0.952467\n\n\n4\n0.958620\n0.961378\n\n\n5\n0.958278\n0.961477\n\n\n6\n0.958136\n0.961659\n\n\n7\n0.957822\n0.961856\n\n\n8\n0.957306\n0.961933\n\n\n9\n0.957090\n0.961955\n\n\n\n\n\n\n\n\n# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_polynomialfeatures__degree\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_polynomialfeatures__degree\"], cv_results[\"mean_train_score\"])\nplt.xlabel('degree')\nplt.ylabel('r-squared')\nplt.title(\"Optimal polynomial degree\")\nplt.legend(['test score', 'train score'], loc='upper left');\n\n\n\n\n\n\n\n\n\n# print out the best hyperparameters and the best score\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)\n\n{'polynomialfeatures__degree': 5}\n0.9586200484884486\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])\n\n\nBenefits of GridSearchCV\n\nNo need for manually writing nested for loops for hyperparameter tuning.\n\nAllows parallel processing using multiple CPU cores (n_jobs=-1), speeding up the search.\n\nGridSearchCV guarantees finding the best combination.\n\nHowever, GridSearchCV performs an exhaustive search over all possible hyperparameter combinations, ensuring that the best parameters are found. This can be slow, especially when dealing with multiple hyperparameters.\n\n\n9.4.1.5 GridSearchCV vs. Faster Alternatives for Hyperparameter Tuning\nTwo Ways to Speed Up Hyperparameter Tuning\n\nRandomizedSearchCV\n\nInstead of checking every combination, it randomly samples a set number of hyperparameter values, significantly reducing computation time.\n\nSuitable when a rough estimate of the best hyperparameters is sufficient.\n\nShuffleSplit\n\nUnlike KFold, which ensures each sample is used exactly once as a test set, ShuffleSplit randomly selects train-test splits in each iteration.\n\nReduces redundant computations, making the process faster while maintaining good model performance.\n\n\nBoth approaches can be combined with n_jobs=-1 to leverage parallel processing for even faster results.\n\n\n\n9.4.2 Tuning the classification threshold\nBy default, classifiers use 0.5 as the threshold for classification. However, adjusting this threshold can improve precision, recall, or F1-score depending on the application. Let’s use cross_val_predict to tune the Classification Threshold\n\n9.4.2.1 Using TunedThresholdClassifierCV from sklearn ( &gt;= Version 1.5)\n\nfrom sklearn.model_selection import TunedThresholdClassifierCV\n\n# Define cross-validation strategy\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# Define TunedThresholdClassifierCV\ntuned_clf = TunedThresholdClassifierCV(\n    estimator=logit_model,\n    scoring=\"f1\",  # Optimize for F1-score\n    cv=cv\n)\n\n# Fit the model\ntuned_clf.fit(X_train_class, y_train_class)\n\n\n# Print the best threshold and the corresponding score\nprint(f\"Best threshold: {tuned_clf.best_threshold_:.2f}\")\nprint(f\"Best F1-score: {tuned_clf.best_score_:.4f}\")\n\nBest threshold: 0.39\nBest F1-score: 0.8757\n\n\n\n\n9.4.2.2 Using cross_val_predict\n\n# use the knn to tune the threshold for classification, the threshold is among np.arange(0.1, 0.9, 0.1), using 5 fold cross validation for tuning the threshold\nthresholds = np.arange(0.1, 0.9, 0.1)\nscores = []\nfor threshold in thresholds:\n    y_pred_class_prob = cross_val_predict(logit_model, X_train_class, y_train_class, cv=5, method='predict_proba')[:,1]\n    y_pred_class = (y_pred_class_prob &gt; threshold).astype(int)\n    scores.append(accuracy_score(y_train_class, y_pred_class))\nscores = pd.Series(scores, index=thresholds)\n\ndf_scores = pd.DataFrame({'threshold': thresholds, 'accuracy': scores}, )\nprint(df_scores.to_string(index=False))\n\n threshold  accuracy\n  0.100000  0.716000\n  0.200000  0.832000\n  0.300000  0.864000\n  0.400000  0.868000\n  0.500000  0.865333\n  0.600000  0.861333\n  0.700000  0.841333\n  0.800000  0.810667\n\n\n\n# print out the best threshold  and the cv score for the best threshold\nbest_threshold = scores.idxmax()\nprint(best_threshold)\nprint(scores.loc[best_threshold])\n\n0.4\n0.868",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html",
    "href": "Regularization in Python.html",
    "title": "10  Regularization",
    "section": "",
    "text": "10.1 Why do we need regularization?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#why-do-we-need-regularization",
    "href": "Regularization in Python.html#why-do-we-need-regularization",
    "title": "10  Regularization",
    "section": "",
    "text": "10.1.1 The Challenge of Overfitting and Underfitting\nWhen building machine learning models, we aim to find patterns in data that generalize well to unseen samples. However, models can suffer from two key issues:\n\nUnderfitting: The model is too simple to capture the underlying pattern in the data.\nOverfitting: The model is too complex and captures noise rather than generalizable patterns.\n\nRegularization is a technique used to address overfitting by penalizing overly complex models.\n\n\n10.1.2 Understanding the Bias-Variance Tradeoff\nA well-performing model balances two competing sources of error:\n\nBias: Error due to overly simplistic assumptions (e.g., underfitting).\nVariance: Error due to excessive sensitivity to training data (e.g., overfitting).\n\nA high-bias model (e.g., a simple linear regression) may not capture the underlying trend, while a high-variance model (e.g., a deep neural network with many parameters) may memorize noise instead of learning meaningful patterns.\nRegularization helps reduce variance while maintaining an appropriate level of model complexity.\n\n\n10.1.3 Visualizing Overfitting vs. Underfitting\nTo better understand this concept, consider three different models:\n\nUnderfitting (High Bias): The model is too simple and fails to capture important trends.\nGood Fit (Balanced Bias & Variance): The model generalizes well to unseen data.\nOverfitting (High Variance): The model is too complex and captures noise, leading to poor generalization.\n\n\\[\n\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n\\]\nRegularization helps control variance by penalizing large coefficients, leading to a model that generalizes better.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#simulating-data-for-an-overfitting-linear-model",
    "href": "Regularization in Python.html#simulating-data-for-an-overfitting-linear-model",
    "title": "10  Regularization",
    "section": "10.2 Simulating Data for an Overfitting Linear Model",
    "text": "10.2 Simulating Data for an Overfitting Linear Model\n\n10.2.1 Generating the data\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(360)])\nnp.random.seed(10)  #Setting seed for reproducibility\ny = np.sin(x) + np.random.normal(0,0.15,len(x))\ndata = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\nplt.plot(data['x'],data['y'],'.');\n\n\n\n\n\n\n\n\n\n# check how the data looks like\ndata.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n0.2\n\n\n1\n0.017\n0.12\n\n\n2\n0.035\n-0.2\n\n\n3\n0.052\n0.051\n\n\n4\n0.07\n0.16\n\n\n\n\n\n\n\nPolynomial features allow linear regression to model non-linear relationships. Higher-degree terms capture more complex patterns in the data. Let’s manually expands features, similar to PolynomialFeatures in sklearn.preprocessing. Using polynomial regression, we can evaluate different polynomial degrees and analyze the balance between underfitting and overfitting.\n\nfor i in range(2,16):  #power of 1 is already there\n    colname = 'x_%d'%i      #new var will be x_power\n    data[colname] = data['x']**i\ndata.head()\n\n\n\n\n\n\n\n\nx\ny\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\n\n\n\n\n0\n0\n0.2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0.017\n0.12\n0.0003\n5.3e-06\n9.3e-08\n1.6e-09\n2.8e-11\n4.9e-13\n8.6e-15\n1.5e-16\n2.6e-18\n4.6e-20\n8e-22\n1.4e-23\n2.4e-25\n4.2e-27\n\n\n2\n0.035\n-0.2\n0.0012\n4.3e-05\n1.5e-06\n5.2e-08\n1.8e-09\n6.3e-11\n2.2e-12\n7.7e-14\n2.7e-15\n9.4e-17\n3.3e-18\n1.1e-19\n4e-21\n1.4e-22\n\n\n3\n0.052\n0.051\n0.0027\n0.00014\n7.5e-06\n3.9e-07\n2.1e-08\n1.1e-09\n5.6e-11\n3e-12\n1.5e-13\n8.1e-15\n4.2e-16\n2.2e-17\n1.2e-18\n6.1e-20\n\n\n4\n0.07\n0.16\n0.0049\n0.00034\n2.4e-05\n1.7e-06\n1.2e-07\n8.1e-09\n5.6e-10\n3.9e-11\n2.8e-12\n1.9e-13\n1.3e-14\n9.4e-16\n6.5e-17\n4.6e-18\n\n\n\n\n\n\n\nWhat This Code Does\n\nGenerates Higher-Degree Polynomial Features: Iterates over the range 2 to 15, computing polynomial terms (x², x³, ..., x¹⁵).\nDynamically Creates Column Names: New feature names are automatically generated in the format x_2, x_3, ..., x_15.\nExpands the Dataset: Each polynomial-transformed feature is stored\n\n\n\n10.2.2 Splitting the Data\nNext, we will split the data into training and testing sets. As we’ve learned, models tend to overfit when trained on a small dataset.\nTo intentionally create an overfitting scenario, we will: - Use only 10% of the data for training. - Reserve 90% of the data for testing.\nThis is not a typical train-test split but is deliberately done to demonstrate overfitting, where the model performs well on the training data but generalizes poorly to unseen data.\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.9)\n\n\nprint('Number of observations in the training data:', len(train))\nprint('Number of observations in the test data:',len(test))\n\nNumber of observations in the training data: 36\nNumber of observations in the test data: 324\n\n\n\n\n10.2.3 Splitting the target and features\n\nX_train = train.drop('y', axis=1).values\ny_train = train.y.values\nX_test = test.drop('y', axis=1).values\ny_test = test.y.values\n\n\n\n10.2.4 Building Models\n\n10.2.4.1 Building a linear model with only 1 predictor x\n\n# Linear regression with one feature\nindependent_variable_train = X_train[:, 0:1]\n\nlinreg = LinearRegression()\nlinreg.fit(independent_variable_train, y_train)\ny_train_pred = linreg.predict(independent_variable_train)\nrss_train = sum((y_train_pred-y_train)**2)/X_train.shape[0]\n\nindependent_variable_test = X_test[:, 0:1]\ny_test_pred = linreg.predict(independent_variable_test)\nrss_test = sum((y_test_pred-y_test)**2)/X_test.shape[0]\n\nprint(\"Training Error\", rss_train)\nprint(\"Testing Error\", rss_test)\n\nplt.plot(X_train[:, 0:1], y_train_pred)\nplt.plot(X_train[:, 0:1], y_train, '.');\n\nTraining Error 0.22398220582126424\nTesting Error 0.22151086120574928\n\n\n\n\n\n\n\n\n\n\n\n10.2.4.2 Building a linear regression model with three features x, x_2, x_3\n\nindependent_variable_train = X_train[:, 0:3]\nindependent_variable_train[:3]\n\narray([[ 1.36135682,  1.85329238,  2.52299222],\n       [ 2.30383461,  5.30765392, 12.22795682],\n       [ 1.51843645,  2.30564925,  3.50098186]])\n\n\n\ndef sort_xy(x,y):\n    idx = np.argsort(x)\n    x2,y2= x[idx] ,y[idx] \n    return x2,y2\n\n\n# Linear regression with 3 features\n\nlinreg = LinearRegression()\n\nlinreg.fit(independent_variable_train, y_train)\ny_train_pred = linreg.predict(independent_variable_train)\nrss_train = sum((y_train_pred-y_train)**2)/X_train_std.shape[0]\n\nindependent_variable_test = X_test[:, 0:3]\ny_test_pred = linreg.predict(independent_variable_test)\nrss_test = sum((y_test_pred-y_test)**2)/X_test_std.shape[0]\n\nprint(\"Training Error\", rss_train)\nprint(\"Testing Error\", rss_test)\n\nplt.plot(*sort_xy(X_train[:, 0], y_train_pred))\nplt.plot(X_train[:, 0], y_train, '.');\n\nTraining Error 0.02167114498970705\nTesting Error 0.028159311299747036\n\n\n\n\n\n\n\n\n\nLet’s define a helper function that dynamically builds and trains a linear regression model based on a specified number of features. It allows for flexibility in selecting features and automates the process for multiple models.\n\n# Define a function which will fit linear vregression model, plot the results, and return the coefficient\ndef linear_regression(train_x, train_y, test_x, test_y, features, models_to_plot):\n    \n    #fit the model\n    linreg = LinearRegression()\n    linreg.fit(train_x, train_y)\n    train_y_pred = linreg.predict(train_x)\n    test_y_pred = linreg.predict(test_x)\n    \n    #check if a plot is to be made for the entered features\n    if features in models_to_plot:\n        plt.subplot(models_to_plot[features])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x[:, 0], train_y_pred))\n        plt.plot(train_x[:, 0], train_y, '.')\n        \n        plt.title('Number of Predictors: %d'%features)\n        \n    #return the result in pre-defined format\n    rss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [rss_train]\n    \n    rss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([rss_test])\n    \n    ret.extend([linreg.intercept_])\n    ret.extend(linreg.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the results:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['Number_of_variable_%d'%i for i in range(1, 16)]\ncoef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1:231, 3:232, 6:233, 9:234, 12:235, 15:236}\n\n\nimport matplotlib.pyplot as plt\n# Iterate through all powers and store the results in a matrix form\nplt.figure(figsize = (12, 8))\nfor i in range(1, 16):\n    train_x = X_train[:, 0:i]\n    train_y = y_train\n    test_x = X_test[:, 0:i]\n    test_y = y_test\n    \n    coef_matrix_simple.iloc[i-1, 0:i+3] = linear_regression(train_x, train_y, test_x, test_y, features=i, models_to_plot=models_to_plot)\n\n\n\n\n\n\n\n\nKey Takeaways:\nAs we increase the number of features (higher-degree polynomial terms), we observe the following:\n- The model becomes more flexible, capturing intricate patterns in the training data.\n- The curve becomes increasingly wavy and complex, adapting too closely to the data points.\n- This results in overfitting, where the model performs well on the training set but fails to generalize to unseen data.\nOverfitting occurs because the model learns noise instead of the true underlying pattern, leading to poor performance on new data.\nTo better understand this phenomenon, let’s:\n- Evaluate model performance on both the training and test sets.\n- Output the model coefficients to analyze how feature coefficients changes with increasing complexity.\n\n# Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_simple\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nNumber_of_variable_1\n0.22\n0.22\n0.88\n-0.29\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_2\n0.22\n0.22\n0.84\n-0.25\n-0.0057\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_3\n0.022\n0.028\n-0.032\n1.7\n-0.83\n0.089\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_4\n0.021\n0.03\n-0.09\n2\n-1\n0.14\n-0.0037\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_5\n0.02\n0.025\n-0.019\n1.5\n-0.48\n-0.092\n0.037\n-0.0026\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_6\n0.019\n0.029\n-0.13\n2.4\n-1.9\n0.77\n-0.21\n0.031\n-0.0017\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_7\n0.017\n0.034\n-0.37\n4.7\n-6.5\n4.7\n-1.9\n0.4\n-0.044\n0.0019\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_8\n0.017\n0.035\n-0.42\n5.3\n-8\n6.4\n-2.9\n0.73\n-0.1\n0.0076\n-0.00022\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_9\n0.016\n0.036\n-0.57\n7.1\n-14\n16\n-9.9\n3.8\n-0.91\n0.13\n-0.011\n0.00037\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_10\n0.016\n0.036\n-0.51\n6.2\n-11\n9.7\n-4.5\n0.83\n0.11\n-0.087\n0.018\n-0.0017\n6.6e-05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_11\n0.014\n0.044\n0.17\n-4\n36\n-86\n1e+02\n-71\n31\n-8.8\n1.6\n-0.18\n0.012\n-0.00034\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_12\n0.013\n0.049\n0.54\n-10\n67\n-1.6e+02\n2e+02\n-1.5e+02\n74\n-24\n5.4\n-0.8\n0.076\n-0.0041\n0.0001\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_13\n0.0086\n0.065\n-0.56\n9.9\n-56\n2e+02\n-3.9e+02\n4.5e+02\n-3.2e+02\n1.6e+02\n-51\n11\n-1.7\n0.17\n-0.0093\n0.00023\nNaN\nNaN\n\n\nNumber_of_variable_14\n0.009\n0.062\n-0.075\n0.62\n7\n-5.1\n-12\n11\n9.4\n-21\n15\n-6.2\n1.6\n-0.27\n0.028\n-0.0016\n4.2e-05\nNaN\n\n\nNumber_of_variable_15\n0.0097\n0.061\n-0.3\n3.4\n-0.93\n-2\n-0.61\n1.2\n1.2\n-0.79\n-1.2\n1.6\n-0.81\n0.24\n-0.043\n0.0047\n-0.00029\n7.8e-06\n\n\n\n\n\n\n\nLet’s plot the training error versus the test error below and identify the overfitting\n\ncoef_matrix_simple[['mrss_train', 'mrss_test']].plot()\nax = plt.gca()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\n\n\n\n10.2.5 Overfitting Indicated by Training and Test MRSS Trends\nAs observed in the plot:\n- The Training Mean Residual Sum of Squares (MRSS) consistently decreases as the number of features increases.\n- However, after a certain point, the Test MRSS starts to rise, indicating that the model is no longer generalizing well to unseen data.\nThis trend suggests that while adding more features helps the model fit the training data better, it also causes the model to memorize noise, leading to poor performance on the test set.\nThis is a classic sign of overfitting, where the model captures excessive complexity in the data rather than the true underlying pattern.\nNext, let’s mitigate the overfitting issue using regularization",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#regularization-combating-overfitting",
    "href": "Regularization in Python.html#regularization-combating-overfitting",
    "title": "10  Regularization",
    "section": "10.3 Regularization: Combating Overfitting",
    "text": "10.3 Regularization: Combating Overfitting\nRegularization is a technique that modifies the loss function by adding a penalty term to control model complexity.\nThis helps prevent overfitting by discouraging large coefficients in the model.\n\n10.3.1 Regularized Loss Function\nThe regularized loss function is given by:\n\\(L_{reg}(\\beta) = L(\\beta) + \\alpha R(\\beta)\\)\nwhere: - \\(L(\\beta)\\) is the original loss function (e.g., Mean Squared Error in linear regression).\n- \\(R(\\beta)\\) is the regularization term, which penalizes large coefficients.\n- \\(\\alpha\\) is a hyperparameter that controls the strength of regularization.\n\n\n10.3.2 Regularization Does Not Penalize the Intercept\n\nThe intercept (bias term) captures the baseline mean of the target variable.\n\nPenalizing the intercept would shift predictions incorrectly instead of controlling complexity.\n\nThus, regularization only applies to feature coefficients, not the intercept.\n\n\n\n10.3.3 Types of Regularization\n\nL1 Regularization (Lasso Regression): Encourages sparsity by driving some coefficients to zero.\n\nL2 Regularization (Ridge Regression): Shrinks coefficients but keeps all of them nonzero.\n\nElastic Net: A combination of both L1 and L2 regularization.\n\nBy applying regularization, we obtain models that balance bias-variance tradeoff, leading to better generalization.\n\n\n10.3.4 Why Is Feature Scaling Required in Regularization?\n\n10.3.4.1 The Effect of Feature Magnitudes on Regularization\nRegularization techniques such as Lasso (L1), Ridge (L2), and Elastic Net apply penalties to the model’s coefficients. However, when features have vastly different scales, regularization disproportionately affects certain features, leading to:\n\nUneven shrinkage of coefficients, causing instability in the model.\nIncorrect feature importance interpretation, as some features dominate due to their larger numerical scale.\nSuboptimal performance, since regularization penalizes large coefficients more, even if they belong to more informative features.\n\n\n\n10.3.4.2 Example: The Need for Feature Scaling\nImagine a dataset with two features:\n\nFeature 1: Number of bedrooms (range: 1-5).\n\nFeature 2: House area in square feet (range: 500-5000).\n\nSince house area has much larger values, the model assigns smaller coefficients to compensate, making regularization unfairly biased toward smaller-scale features.\n\n\n10.3.4.3 How to Scale Features for Regularization\nTo ensure fair treatment of all features, apply feature scaling before training a regularized model:\n\n10.3.4.3.1 Standardization (Recommended)\n\\[\nx_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}\n\\] - Centers the data around zero with unit variance. - Used in Lasso, Ridge, and Elastic Net.\n\n\n10.3.4.3.2 Min-Max Scaling\n\\[\nx_{\\text{scaled}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n\\] - Scales features to a fixed [0, 1] range. - Common for neural networks but less effective for regularization.\nLet’s use StandardScaler to scale the features\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Feature scaling on X_train\nX_train_std = scaler.fit_transform(X_train)\ncolumns = data.drop('y', axis=1).columns\nX_train_std = pd.DataFrame(X_train_std, columns = columns)\nX_train_std.head()\n# Feature scaling on X_test\nX_test_std = scaler.transform(X_test)\nX_test_std = pd.DataFrame(X_test_std, columns = columns)\nX_test_std.head()\n\n\n\n\n\n\n\n\nx\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\n\n\n\n\n0\n-0.96\n-1\n-0.89\n-0.78\n-0.69\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n1\n-0.061\n-0.34\n-0.49\n-0.55\n-0.57\n-0.56\n-0.53\n-0.5\n-0.47\n-0.45\n-0.42\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n2\n-1.8\n-1.2\n-0.95\n-0.8\n-0.7\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n3\n0.21\n-0.051\n-0.24\n-0.36\n-0.43\n-0.46\n-0.47\n-0.46\n-0.45\n-0.43\n-0.41\n-0.4\n-0.38\n-0.36\n-0.35\n\n\n4\n-1.7\n-1.2\n-0.95\n-0.8\n-0.7\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n\n\n\n\n\nIn the next section, we will explore different types of regularization techniques and see how they help in preventing overfitting.\n\n\n\n\n10.3.5 Ridge Regression: L2 Regularization\nRidge regression is a type of linear regression that incorporates L2 regularization to prevent overfitting by penalizing large coefficients.\n\n10.3.5.1 Ridge Regression Loss Function\nThe regularized loss function for Ridge regression is given by:\n\\[\nL_{\\text{Ridge}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\beta^\\top x_i|^2 + \\alpha \\sum_{j=1}^{J} \\beta_j^2\n\\]\nwhere:\n\n\\(y_i \\text{ is the true target value for observation } i.\\)\n\\(x_i \\text{ is the feature vector for observation } i.\\)\n\\(\\beta \\text{ is the vector of model coefficients.}\\)\n\\(\\alpha \\text{ is the regularization parameter, which controls the penalty strength.}\\)\n\\(\\sum_{j=1}^{J} \\beta_j^2 \\text{ is the L2 norm (sum of squared coefficients).}\\)\n\nNote that \\(j\\) starts from 1, excluding the intercept from regularization.\nThe penalty term in Ridge regression,\n\\[\n\\sum_{j=1}^{J} \\beta_j^2 = ||\\beta||_2^2\n\\]\nis the squared L2 norm of the coefficient vector \\(\\beta\\).\nMinimizing this norm ensures that the model coefficients remain small and stable, reducing sensitivity to variations in the data.\nLet’s build a Ridge Regression model using scikit-learn, The alpha parameter controls the strength of the regularization:\n\nfrom sklearn.linear_model import Ridge\n# defining a function which will fit ridge regression model, plot the results, and return the coefficients\ndef ridge_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    ridgereg = Ridge(alpha=alpha)\n    ridgereg.fit(train_x, train_y)\n    train_y_pred = ridgereg.predict(train_x)\n    test_y_pred = ridgereg.predict(test_x)\n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([ridgereg.intercept_])\n    ret.extend(ridgereg.coef_)\n    \n    return ret\n\nLet’s experiment with different values of alpha in Ridge Regression and observe how it affects the model’s coefficients and performance.\n\n#initialize a dataframe to store the coefficient:\nalpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0, 10)]\ncoef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_ridge.iloc[i,] = ridge_regression(X_train_std, train_y, X_test_std, test_y, alpha_ridge[i], models_to_plot)\n\n\n\n\n\n\n\n\nAs we can observe, when increasing alpha, the model becomes simpler, with coefficients shrinking more aggressively due to stronger regularization. This reduces the risk of overfitting but may lead to underfitting if alpha is set too high.\nNext, let’s output the training error versus the test error and examine how the feature coefficients change with different alpha values.\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_ridge\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.0092\n0.059\n-0.072\n-2.8\n1.9e+02\n-1.3e+03\n-6e+03\n1.1e+05\n-5.6e+05\n1.5e+06\n-2e+06\n7.5e+05\n1.4e+06\n-1.4e+06\n-7.5e+05\n2e+06\n-1.3e+06\n2.8e+05\n\n\nalpha_1e-10\n0.016\n0.036\n-0.072\n12\n-1.3e+02\n7.1e+02\n-1.8e+03\n1.8e+03\n1.1e+03\n-2.8e+03\n-7.9e+02\n2.4e+03\n1.5e+03\n-1.4e+03\n-2.1e+03\n3.5e+02\n2.2e+03\n-1e+03\n\n\nalpha_1e-08\n0.017\n0.034\n-0.072\n8.1\n-70\n2.8e+02\n-5.6e+02\n4.2e+02\n1.7e+02\n-2.6e+02\n-1.8e+02\n1e+02\n1.8e+02\n34\n-1.1e+02\n-79\n64\n3.8\n\n\nalpha_0.0001\n0.019\n0.024\n-0.072\n2.9\n-8.2\n4.5\n-0.022\n-1.3\n0.31\n1.8\n2.1\n1.1\n-0.44\n-1.8\n-2.4\n-1.9\n-0.06\n3.1\n\n\nalpha_0.001\n0.019\n0.023\n-0.072\n2.5\n-5.6\n-0.5\n1.3\n1.6\n1.4\n0.84\n0.21\n-0.39\n-0.82\n-1\n-0.91\n-0.48\n0.28\n1.3\n\n\nalpha_0.01\n0.022\n0.024\n-0.072\n1.9\n-4\n-1.3\n0.73\n1.5\n1.3\n0.82\n0.23\n-0.26\n-0.58\n-0.69\n-0.6\n-0.31\n0.13\n0.72\n\n\nalpha_1\n0.089\n0.093\n-0.072\n0.08\n-0.61\n-0.48\n-0.22\n-0.011\n0.13\n0.2\n0.22\n0.21\n0.17\n0.12\n0.058\n-0.0066\n-0.072\n-0.14\n\n\nalpha_5\n0.12\n0.13\n-0.072\n-0.17\n-0.3\n-0.24\n-0.14\n-0.055\n0.0059\n0.046\n0.069\n0.08\n0.081\n0.077\n0.069\n0.057\n0.045\n0.031\n\n\nalpha_10\n0.14\n0.14\n-0.072\n-0.19\n-0.24\n-0.18\n-0.12\n-0.058\n-0.014\n0.018\n0.039\n0.053\n0.06\n0.063\n0.064\n0.062\n0.059\n0.054\n\n\nalpha_20\n0.16\n0.17\n-0.072\n-0.18\n-0.19\n-0.15\n-0.097\n-0.055\n-0.022\n0.0025\n0.021\n0.034\n0.043\n0.049\n0.053\n0.056\n0.057\n0.057\n\n\n\n\n\n\n\nTo better observe the pattern, let’s visualize how the coefficients change as we increase \\(\\alpha\\)\n\ndef plot_ridge_reg_coeff(train_x):\n    alphas = np.logspace(3,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        ridge = Ridge(alpha = a)\n        ridge.fit(train_x, train_y)\n        coefs.append(ridge.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Feature coefficient')\n    plt.legend(train_x.columns );    \nplot_ridge_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test.png\")\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\alpha\\) increases, the coefficients become smaller and approach zero. Now, let’s examine the number of zero coefficients.\n\ncoef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15     0\nalpha_1e-10     0\nalpha_1e-08     0\nalpha_0.0001    0\nalpha_0.001     0\nalpha_0.01      0\nalpha_1         0\nalpha_5         0\nalpha_10        0\nalpha_20        0\ndtype: int32\n\n\nLet’s plot how the test error and training error change as we increase \\(\\alpha\\)\n\ncoef_matrix_ridge[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\nAs we can observe, as \\(𝜆\\) increases beyond a certain value, both the training MRSS and test MRSS begin to rise, indicating that the model starts underfitting.\n\n\n\n10.3.6 Lasso Regression: L1 Regularization\nLASSO stands for Least Absolute Shrinkage and Selection Operator. There are two key aspects in this name:\n- “Absolute” refers to the use of the absolute values of the coefficients in the penalty term.\n- “Selection” highlights LASSO’s ability to shrink some coefficients to exactly zero, effectively performing feature selection.\nLasso regression performs L1 regularization, which helps prevent overfitting by penalizing large coefficients and enforcing sparsity in the model.\n\n10.3.6.1 Lasso Regression Loss Function\nIn standard linear regression, the loss function is the Mean Squared Error (MSE):\n\\[L_{\\text{MSE}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2\\]\nLASSO modifies this by adding an L1 regularization penalty, leading to the following regularized loss function:\n\\[L_{\\text{Lasso}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2 + \\alpha  \\sum_{j=1}^{J} |\\beta_j|\\]\nwhere:\n\n\\(y_i \\text{ is the true target value for observation } i.\\)\n\\(x_i \\text{ is the feature vector for observation } i.\\)\n\\(\\beta \\text{ is the vector of model coefficients.}\\)\n\\(\\alpha \\text{ is the regularization parameter, which controls the penalty strength.}\\)\n\\(\\sum_{j=1}^{J} |\\beta_j| \\text{ is the } \\mathbf{L_1} \\text{ norm (sum of absolute values of coefficients).}\\)\n\nThe penalty term in Lasso regression,\n\\[\n\\sum_{j=1}^{J} |\\beta_j| = ||\\beta||_1\n\\]\nis the L1 norm of the coefficient vector ( \\(\\beta\\) ).\nMinimizing this norm encourages sparsity, meaning some coefficients become exactly zero, leading to an automatically selected subset of features.\nNext, let’s build a Lasso Regression model. Similar to Ridge regression, we will explore a range of values for the regularization parameter alpha.\n\nfrom sklearn.linear_model import Lasso\nalpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 0.1, 1, 5]\n\n\n# Defining a function which will fit lasso regression model, plot the results, and return the coefficient\ndef lasso_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    if alpha == 0:\n        lassoreg = LinearRegression()    \n    else:\n        lassoreg = Lasso(alpha=alpha, max_iter=20000)\n    lassoreg.fit(train_x, train_y)\n    train_y_pred = lassoreg.predict(train_x)\n    test_y_pred = lassoreg.predict(test_x)\n        \n        \n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0:1], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([lassoreg.intercept_])\n    ret.extend(lassoreg.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the coefficient:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0, 10)]\ncoef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 0.1:236}\n\n\nmodels_to_plot\n\n{1e-10: 231, 1e-05: 232, 0.0001: 233, 0.001: 234, 0.01: 235, 0.1: 236}\n\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_lasso.iloc[i,] = lasso_regression(X_train_std, train_y, X_test_std, test_y, alpha_lasso[i], models_to_plot)\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.487e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.495e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.675e-03, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_lasso\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-10\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-08\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-05\n0.019\n0.024\n-0.072\n2.8\n-6.8\n0.73\n1.7\n1.6\n0.93\n0.21\n0\n-0.6\n-0.68\n-0.55\n-0.3\n-0\n0.023\n0.71\n\n\nalpha_0.0001\n0.02\n0.024\n-0.072\n2.7\n-6.5\n0\n2.8\n1.4\n0\n0\n-0\n-0\n-0.68\n-0.43\n-0\n-0\n0\n0.35\n\n\nalpha_0.001\n0.024\n0.026\n-0.072\n2\n-4.5\n-0\n0\n2.6\n0\n0\n0\n-0\n-0\n-0\n-0\n-0.37\n-0\n-0\n\n\nalpha_0.01\n0.084\n0.088\n-0.072\n0.27\n-1.3\n-0\n-0\n0\n0\n0\n0.68\n0\n0\n0\n0\n0\n0\n-0\n\n\nalpha_0.1\n0.19\n0.2\n-0.072\n-0.22\n-0.27\n-0\n-0\n-0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.12\n\n\nalpha_1\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\nalpha_5\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\n\n\n\n\n\n\ndef plot_lasso_reg_coeff(train_x):\n    alphas = np.logspace(1,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        laso = Lasso(alpha=a, max_iter = 100000)\n        laso.fit(train_x, train_y)\n        coefs.append(laso.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Standardized coefficient')\n    plt.legend(train_x.columns );    \nplot_lasso_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test1.png\")\n\n\n\n\n\n\n\n\n\ncoef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15      0\nalpha_1e-10      0\nalpha_1e-08      0\nalpha_1e-05      2\nalpha_0.0001     8\nalpha_0.001     11\nalpha_0.01      12\nalpha_0.1       12\nalpha_1         15\nalpha_5         15\ndtype: int32\n\n\n\ncoef_matrix_lasso[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test'])\n\n\n\n\n\n\n\n\nEffect of alpha in Lasso Regression - Small alpha (close to 0): The penalty is minimal, and Lasso behaves like ordinary linear regression. - Moderate alpha: Some coefficients shrink, and some become exactly zero, performing feature selection. - Large alpha: Many coefficients become zero, leading to a very simple model (potentially underfitting).\n\n\n\n10.3.7 Elastic Net Regression: Combining L1 and L2 Regularization\n\n10.3.7.1 Mathematical Formulation*\nElastic Net regression combines both L1 (Lasso) and L2 (Ridge) penalties, balancing feature selection and coefficient shrinkage. The regularized loss function for Elastic Net is given by:\n\\[\nL_{\\text{ElasticNet}}(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2 + \\alpha \\left( (1 - \\rho) \\frac{1}{2} \\sum_{j=1}^{p} \\beta_j^2 + \\rho \\sum_{j=1}^{p} |\\beta_j| \\right)\n\\]\nwhere:\n\n\\(y_i\\) is the true target value for observation \\(i\\).\n\\(x_i\\) is the feature vector for observation \\(i\\).\n\\(\\beta\\) is the vector of model coefficients.\n\\(\\alpha\\) is the regularization strength parameter in scikit-learn.\n\\(\\rho\\) is the l1_ratio parameter in scikit-learn, controlling the mix of L1 and L2 penalties.\n\\(\\sum_{j=1}^{p} |\\beta_j|\\) is the L1 norm, enforcing sparsity.\n\\(\\sum_{j=1}^{p} \\beta_j^2\\) is the L2 norm, ensuring coefficient stability.\n\n\n\n10.3.7.2 Elastic Net Special Cases\n\nWhen l1_ratio = 0, Elastic Net behaves like Ridge Regression (L2 regularization).\nWhen l1_ratio = 1, Elastic Net behaves like Lasso Regression (L1 regularization).\nWhen 0 &lt; l1_ratio &lt; 1, Elastic Net balances feature selection (L1) and coefficient shrinkage (L2).\n\nNow, let’s implement an Elastic Net Regression model using scikit-learn and explore how different values of alpha and l1_ratio affect the model.\n\nfrom sklearn.linear_model import ElasticNet\nalpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 0.1, 1, 5]\nl1_ratio=0.5\n\n\n# Defining a function which will fit lasso regression model, plot the results, and return the coefficient\ndef elasticnet_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    if alpha == 0:\n        model = LinearRegression()    \n    else:\n        model = ElasticNet(alpha=alpha, max_iter=20000, l1_ratio=l1_ratio)\n    model.fit(train_x, train_y)\n    train_y_pred = model.predict(train_x)\n    test_y_pred = model.predict(test_x)\n        \n        \n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0:1], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([model.intercept_])\n    ret.extend(model.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the coefficient:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0, 10)]\ncoef_matrix_elasticnet = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 0.1:236}\n\n\nmodels_to_plot\n\n{1e-10: 231, 1e-05: 232, 0.0001: 233, 0.001: 234, 0.01: 235, 0.1: 236}\n\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_elasticnet.iloc[i,] = elasticnet_regression(X_train_std, train_y, X_test_std, test_y, alpha_lasso[i], models_to_plot)\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.279e-02, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_elasticnet\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-10\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-08\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-05\n0.019\n0.024\n-0.072\n2.7\n-6.6\n0.37\n1.7\n1.7\n1\n0.27\n-0.13\n-0.61\n-0.69\n-0.57\n-0.33\n-0.019\n0.15\n0.66\n\n\nalpha_0.0001\n0.02\n0.023\n-0.072\n2.4\n-5.5\n-0.6\n1.3\n2.2\n1.2\n0.056\n-0\n-0.36\n-0.83\n-0.68\n-0.26\n-0\n0\n0.71\n\n\nalpha_0.001\n0.028\n0.03\n-0.072\n1.6\n-3.3\n-0.6\n0\n0.99\n1.1\n0.54\n0\n0\n-0\n-0\n-0.21\n-0.25\n-0.11\n-0\n\n\nalpha_0.01\n0.076\n0.081\n-0.072\n0.31\n-1.1\n-0.48\n-0\n0\n0.14\n0.35\n0.33\n0.14\n0\n0\n0\n-0\n-0\n-0.067\n\n\nalpha_0.1\n0.15\n0.16\n-0.072\n-0.19\n-0.41\n-0.0068\n-0\n-0\n0\n0\n0\n0\n0\n0\n0.04\n0.062\n0.073\n0.075\n\n\nalpha_1\n0.48\n0.52\n-0.072\n-0.013\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\nalpha_5\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\n\n\n\n\n\n\ndef plot_elasticnet_reg_coeff(train_x):\n    alphas = np.logspace(1,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        model = ElasticNet(alpha=a, max_iter=20000, l1_ratio=l1_ratio)\n        model.fit(train_x, train_y)\n        coefs.append(model.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Standardized coefficient')\n    plt.legend(train_x.columns );    \nplot_elasticnet_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test2.png\")\n\n\n\n\n\n\n\n\n\ncoef_matrix_elasticnet.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15      0\nalpha_1e-10      0\nalpha_1e-08      0\nalpha_1e-05      0\nalpha_0.0001     3\nalpha_0.001      6\nalpha_0.01       7\nalpha_0.1        8\nalpha_1         14\nalpha_5         15\ndtype: int32\n\n\n\ncoef_matrix_elasticnet[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\nElasticNet is controlled by these key parameters:\n\nalpha (float, default=1.0):\nThe regularization strength multiplier. Higher values increase regularization.\nl1_ratio (float, default=0.5):\nThe mixing parameter between L1 and L2 penalties:\n\nl1_ratio = 0: Pure Ridge regression\n\nl1_ratio = 1: Pure Lasso regression\n\n0 &lt; l1_ratio &lt; 1: ElasticNet with mixed penalties",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#reference",
    "href": "Regularization in Python.html#reference",
    "title": "10  Regularization",
    "section": "10.4 Reference",
    "text": "10.4 Reference\nhttps://www.linkedin.com/pulse/tutorial-ridge-lasso-regression-subhajit-mondal/?trk=read_related_article-card_title",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html",
    "href": "Assignment_1_wi25.html",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#instructions",
    "href": "Assignment_1_wi25.html#instructions",
    "title": "Appendix A — Assignment 1",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThere is a bonus question worth 15 points.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 100 + 15 (bonus question) + 5 (proper formatting) = 120 out of 100. There is no partial credit for some parts of the bonus question.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#case-studies-regression-vs-classification-and-prediction-vs-inference-16-points",
    "href": "Assignment_1_wi25.html#case-studies-regression-vs-classification-and-prediction-vs-inference-16-points",
    "title": "Appendix A — Assignment 1",
    "section": "1) Case Studies: Regression vs Classification and Prediction vs Inference (16 points)",
    "text": "1) Case Studies: Regression vs Classification and Prediction vs Inference (16 points)\nFor each case below, explain (1) whether it is a classification or a regression problem and (2) whether the main purpose is prediction or inference. You need justify your answers for credit.\n\n1a)\nYou work for a company that is interested in conducting a marketing campaign. The goal of your project is to identify individuals who are likely to respond positively to a marketing campaign, based on observations of demographic variables (such as age, gender, income etc.) measured on each individual. (2+2 points)\n\n\n1b)\nFor the same company, now you are working on a different project. This one is focused on understanding the impact of advertisements in different media types on the company sales. For example, you are interested in the following question: ‘How large of an increase in sales is associated with a given increase in radio and TV advertising?’ (2+2 points)\n\n\n1c)\nA company is selling furniture and they are interested in the finding the association between demographic characteristics of customers (such as age, gender, income etc.) and if they would purchase a particular company product. (2+2 points)\n\n\n1d)\nWe are interested in forecasting the % change in the USD/Euro exchange rate using the weekly changes in the stock markets of a number of countries. We collect weekly data for all of 2023. For each week, we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. (2+2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#examples-for-different-regression-metrics-rmse-vs-mae-8-points",
    "href": "Assignment_1_wi25.html#examples-for-different-regression-metrics-rmse-vs-mae-8-points",
    "title": "Appendix A — Assignment 1",
    "section": "2) Examples for Different Regression Metrics: RMSE vs MAE (8 points)",
    "text": "2) Examples for Different Regression Metrics: RMSE vs MAE (8 points)\n\n2a)\nDescribe a regression problem, where it will be more proper to evaluate the model performance using the root mean squared error (RMSE) metric as compared to the mean absolute error (MAE) metric. You need to justify your answer for credit. (4 points)\nNote: You are not allowed to use the datasets and examples covered in the lectures.\n\n\n2b)\nDescribe a regression problem, where it will be more proper to evaluate the model performance using the mean absolute error (MAE) metric as compared to the root mean squared error (RMSE) metric. You need to justify your answer for credit. (4 points)\nNote: You are not allowed to use the datasets and examples covered in the lectures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#simple-linear-regression-formulation-3-points",
    "href": "Assignment_1_wi25.html#simple-linear-regression-formulation-3-points",
    "title": "Appendix A — Assignment 1",
    "section": "3) Simple Linear Regression: Formulation (3 points)",
    "text": "3) Simple Linear Regression: Formulation (3 points)\nWhen asked to state the simple linear regression model, a students wrote it as follows: \\(E(Y_i) = \\beta_0 + \\beta_1X_i + \\epsilon_i\\). Is this correct (1 point)? Justify your answer (2 points).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#modeling-the-petrol-consumption-in-u.s.-states-58-points",
    "href": "Assignment_1_wi25.html#modeling-the-petrol-consumption-in-u.s.-states-58-points",
    "title": "Appendix A — Assignment 1",
    "section": "4) Modeling the Petrol Consumption in U.S. States (58 points)",
    "text": "4) Modeling the Petrol Consumption in U.S. States (58 points)\nRead petrol_consumption_train.csv. Assume that each observation is a U.S. state. For each observation, the data has the following variables as its five columns:\nPetrol_tax: Petrol tax (cents per gallon)\nPer_capita_income: Average income (dollars)\nPaved_highways: Paved Highways (miles)\nProp_license: Proportion of population with driver’s licenses\nPetrol_consumption: Consumption of petrol (millions of gallons)\n\n4a)\nCreate a pairwise plot of all the variables in the dataset. (1 point) Print the correlation matrix of all the variables as well. (1 point) Which variable has the highest linear correlation with Petrol_consumption? (1 point)\nNote: Remember that a pairwise plot is a visualization tool that you can find in the seaborn library.\n\n\n4b)\nFit a simple linear regression model to predict Petrol_consumption using the column you found in part a as the only predictor. Print the model summary. (3 points)\n\n\n4c)\nWhen asked for a point estimate of the expected petrol consumption for a state where the proportion of population with driver’s license is 54.4%, a person gave the estimate 488 million gallons because that is the mean value of Petrol_consumption for the two observations of Prop_license = 0.544 pieces in the dataset. Is there an issue with this approach? Explain. (2 points) If there is an issue, then suggest a better approach and use it to estimate the expected petrol consumption for a state where the proportion of population with driver’s license is 54.4%. (2 points)\n\n\n4d)\nWhat is the increase in petrol consumption for an increase of 0.05 in the predictor? (3 points)\n\n\n4e)\nDoes petrol consumption have a statistically significant relationship with the predictor? You need to justify your answer for credit. (3 points)\n\n\n4f)\nHow much of the variation in petrol consumption can be explained by its linear relationship with the predictor? (2 points)\n\n\n4g)\nPredict the petrol consumption for a state in which 50% of the population has a driver’s license. (2 points) What are the confidence interval (2 points) and the prediction interval (2 points) for your prediction? Which interval is wider? (1 points) Why? (2 points)\n\n\n4h)\nPredict the petrol consumption for a state in which 10% of the population has a driver’s license. (3 points) Are you getting a reasonable outcome? (1 point) Why or why not? (2 points)\n\n\n4i)\nWhat is the residual standard error of the model? (3 points)\n\n\n4j)\nUsing the trained model, predict the petrol consumption of the observations in petrol_consumption_test.csv (2 points) and find the RMSE. (2 points) What is the unit of this RMSE value? (1 point)\n\n\n4k)\nBased on the answers to part i and part j, do you think the model is overfitting? You need to justify your answer for credit. (3 points)\n\n\n4l)\nMake a scatterplot of Petrol_consumption vs. the predictor using petrol_consumption_test.csv. (1 point) Over the scatterplot, plot the regression line (1 point), the prediction interval (2 points), and the confidence interval. (2 points)\nMake sure that regression line, prediction interval lines, and confidence interval lines have different colors. (1 point) Display a legend that correctly labels the lines as well. (1 point) Note that you need two lines of the same color to plot an interval.\n\n\n4m)\nThe dataset consists of 40 US States. If you combine this data with the data of the remaining 10 US States, are you likely to obtain narrower confidence and prediction intervals in the plot developed in the previous question, for the same level of confidence? Justify your answer. (2 points).\nIf yes, then can you gaurantee that the width of these intervals will reduce? Justify your answer. If no, then can you gaurantee that the width of these intervals will not reduce? Justify your answer. (2 points)\n\n\n4n)\nFind the correlation between Petrol_consumption and the rest of the variables in petrol_consumption_train.csv. Which column would have the lowest R-squared value when used as the predictor for a Simple Linear Regression model to predict Petrol_consumption? Note that you can directly answer this question from the correlation values and do not need to develop any more linear regression models. (2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#reproducing-the-results-with-scikit-learn-15-points",
    "href": "Assignment_1_wi25.html#reproducing-the-results-with-scikit-learn-15-points",
    "title": "Appendix A — Assignment 1",
    "section": "5) Reproducing the Results with Scikit-Learn (15 points)",
    "text": "5) Reproducing the Results with Scikit-Learn (15 points)\n\n5a)\nUsing the same datasets, same response and the same predictor as Question 4, reproduce the following outputs with scikit-learn:\n\nModel RMSE for test data (3 points)\nR-squared value of the model (3 points)\nResidual standard error of the model (3 points)\n\nNote that you are only allowed to use scikit-learn, pandas, and numpy tools for this question. Any other libraries will not receive any credit.\n\n\n5b)\nWhich of the model outputs from Question 4 cannot be reproduced using scikit-learn? Give two answers. (2+2 points) What does this tell about scikit-learn? (2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment_1_wi25.html#bonus-question-15-points",
    "href": "Assignment_1_wi25.html#bonus-question-15-points",
    "title": "Appendix A — Assignment 1",
    "section": "6) Bonus Question (15 points)",
    "text": "6) Bonus Question (15 points)\nPlease note that the bonus question requires you to look more into the usage of the tools we covered in class and it will be necessary to do your own research. We strongly suggest attempting it after you are done with the rest of the assignment.\n\n6a)\nFit a simple linear regression model to predict Petrol_consumption based on the predictor in Question 4, but without an intercept term. (5 points - no partial credit)\nWithout an intercept means that the equation becomes \\(Y = \\beta_1X\\). The intercept term, \\(\\beta_0\\), becomes 0.\nNote: You must answer this part correctly to qualify for the bonus points in the following parts.\n\n\n6b)\nPredict the petrol consumption for the observations in petrol_consumption_test.csv using the model without an intercept and find the RMSE. (1+2 points) Then, print the summary and find the R-squared. (2 points)\n\n\n6c)\nThe RMSE for the models with and without the intercept are similar, which indicates that both models are almost equally good. However, the R-squared for the model without intercept is much higher than the R-squared for the model with the intercept. Why? Justify your answer. (5 points - no partial credit)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Assignment 1</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html#instructions",
    "href": "Assignment 2.html#instructions",
    "title": "Appendix B — Assignment 2",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 100 + 5 (proper formatting) = 105 out of 100.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html#multiple-linear-regression-24-points",
    "href": "Assignment 2.html#multiple-linear-regression-24-points",
    "title": "Appendix B — Assignment 2",
    "section": "1) Multiple Linear Regression (24 points)",
    "text": "1) Multiple Linear Regression (24 points)\nA study was conducted on 97 male patients with prostate cancer who were due to receive a radical prostatectomy (complete removal of the prostate). The prostate.csv file contains data on 9 measurements taken from these 97 patients. Each row (observation) represents a patient and each column (variable) represents a measurement. The description of variables can be found here: https://rafalab.github.io/pages/649/prostate.html\n\n1a)\nFit a linear regression model with lpsa as the response and all the other variables as the predictors. Print its summary. (2 points) Write down the optimal equation that predicts lpsa using the predictors. (2 points)\n\n\n1b)\nIs the overall regression statistically significant? In other words, is there a statistically significant relationship between the response and at least one predictor? You need to justify your answer for credit. (2 points)\n\n\n1c)\nWhat does the optimal coefficient of svi tell us as a numeric output? Make sure you include the predictor, (svi) the response (lpsa) and the other predictors in your answer. (2 points)\n\n\n1d)\nCheck the \\(p\\)-values of gleason and age. Are these predictors statistically significant? You need to justify your answer for credit. (2 points)\n\n\n1e)\nCheck the 95% Confidence Interval of age. How can you relate it to its p-value and statistical significance, which you found in the previous part? (2 points)\n\n\n1f)\nThis question requires some thinking, and bringing your 303-1 and 303-2 knowledge together.\nFit a simple linear regression model on lpsa against gleason and check the \\(p\\)-value of gleason using the summary. (2 point) Did the statistical significance of gleason change in the absence of other predictors? (1 point) Why or why not? (3 points)\nHints:\n\nYou need to compare this model with the Multiple Linear Regression model you created above.\nPrinting a correlation matrix of all the predictors should be useful.\n\n\n\n1g)\nPredict the lpsa of a 65 year old man with lcavol = 1.35, lweight = 3.65, lbph = 0.1, svi = 0.22, lcp = -0.18, gleason = 6.75, and pgg45 = 25. Find the 95% confidence and prediction intervals as well. (2 points)\n\n\n1h)\nIn the Multiple Linear Regression model with all the predictors, you should see a total of five predictors that appear to be statistically insignificant. Why is it not a good idea to directly conclude that all of them are statistically insignificant? (2 points) Implement the additional test that concludes the statistical insignificance of all five predictors. (2 points)\nHint: f_test() method",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html#multiple-linear-regression-with-variable-transformations-22-points",
    "href": "Assignment 2.html#multiple-linear-regression-with-variable-transformations-22-points",
    "title": "Appendix B — Assignment 2",
    "section": "2) Multiple Linear Regression with Variable Transformations (22 points)",
    "text": "2) Multiple Linear Regression with Variable Transformations (22 points)\nThe infmort.csv file has the infant mortality data of different countries in the world. The mortality column represents the infant mortality rate with “deaths per 1000 infants” as the unit. The income column represents the per capita income in USD. The other columns should be self-explanatory. (This is an old dataset, as can be seen from some country names.)\n\n2a)\nStart your analysis by creating (i) a boxplot of log(mortality) for each region and (ii) a boxplot of income for each region. Note that the region column has the continent names. (3 points)\nNote: You need to use np.log, which is the natural log. This is to better distinguish the mortality values.\n\n\n2b)\nIn the previous part, you should see that Europe has the lowest infant mortality rate on average, but it also has the highest per capita income on average. Our goal is to see if Europe still has the lowest mortality rate if we remove the effect of income. We will try to find an answer for the rest of this question.\nCreate four scatter plots: (i) mortality against income, (ii) log(mortality) against income, (iii) mortality against log(income), and (iv) log(mortality) against log(income). (3 points) Based on the plots, create an appropriate model to predict the mortality rate as a function of per capita income. Print the model summary. (2 points)\n\n\n2c)\nUpdate the model you created in the previous part by adding region as a predictor. Print the model summary. (2 points)\n\n\n2d)\nUse the model developed in the previous part to compute a new adjusted_mortality variable for each observation in the data. (5 points) Adjusted mortality rate is the mortality rate after removing the estimated effect of income. You need to calculate it with the following steps:\n\nMultiply the (transformed) income column with its optimal coefficient. This is the estimated effect of income.\nSubtract the product from the (transformed) response column. This removes the estimated effect of income.\nYou may need to do a inverse transformation to calculate the actual adjusted mortality rate values.\n\nMake a boxplot of log(adjusted_mortality) for each region. (2 points)\n\n\n2e)\nUsing the plots in parts a and d, answer the following questions:\n\nDoes Europe still have the lowest mortality rate on average after removing the effect of income?\nHow did the distribution of values among different continents change after removing the effect of income? How did the comparison of different continents change? Does any non-European country have a lower mortality rate than all the European countries after removing the effect of income?\n\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html#variable-transformations-and-interactions-38-points",
    "href": "Assignment 2.html#variable-transformations-and-interactions-38-points",
    "title": "Appendix B — Assignment 2",
    "section": "3) Variable Transformations and Interactions (38 points)",
    "text": "3) Variable Transformations and Interactions (38 points)\nThe soc_ind.csv dataset contains many social indicators of a number of countries. Each row is a country and each column is a social indicator. The column names should be clear on what the variables represent. The GDP per capita will be the response variable throughout this question.\n\n3a)\nUsing correlations, find out the most useful predictor for a simple linear regression model with gdpPerCapita as the response. You can ignore categorical variables for now. Let that predictor be \\(P\\). (2 points)\n\n\n3b)\nCreate a scatterplot of gdpPerCapita vs \\(P\\). Does the relationship between gdpPerCapita and \\(P\\) seem linear or non-linear? (2 points)\n\n\n3c)\nIf the relationship in the previous part is non-linear, create three models:\n\nOnly with \\(P\\)\nWith \\(P\\) and its quadratic term\nWith \\(P\\), its quadratic term and its cubic term\n\n(2x3 = 6 points)\nCompare the \\(R\\)-squared values of the models. (2 points)\n\n\n3d)\nOn the same figure:\n\ncreate the scatterplot in part b.\nplot the linear regression line (only using \\(P\\))\nplot the polynomial regression curve that includes the quadratic and cubic terms.\nadd a legend to distinguish the linear and cubic fits.\n\n(6 points)\n\n\n3e)\nDevelop a model to predict gdpPerCapita using \\(P\\) and continent as predictors. (No higher-order terms.)\n\nWhich continent creates the baseline? (2 points) Write down its equation. (2 points)\nFor a given value of \\(P\\), are there any continents that do not have a statistically significant difference of predicted gdpPerCapita from the baseline continent? If yes, then which ones, and why? If no, then why not? You need to justify your answers for credit. (4 points)\n\n\n\n3f)\nThe model developed in the previous part has a limitation. It assumes that the increase in predicted gdpPerCapita with a unit increase in \\(P\\) does not depend on the continent.\nEliminate this limitation by including the interaction of continent with \\(P\\) in the model. Print the model summary of the model with interactions. (2 points) Which continent has the closest increase in predicted gdpPerCapita to the baseline continent with a unit increase in \\(P\\). Which continent has the furthest? You need to justify your answers for credit. (5 points)\n\n\n3g)\nUsing the model developed in the previous part, plot the regression lines of all the continents on the same figure. Put gdpPerCapita on the y-axis and \\(P\\) on the x-axis. (4 points) Use a legend to color-code the continents. (1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 2.html#prediction-with-sklearn-20-points",
    "href": "Assignment 2.html#prediction-with-sklearn-20-points",
    "title": "Appendix B — Assignment 2",
    "section": "4) Prediction with Sklearn (20 points)",
    "text": "4) Prediction with Sklearn (20 points)\n\nInstructions\n\nRead the soc_ind.csv dataset and use the Index column as the index of the dataframe.\nDrop the geographic_location and country columns since they are not useful for our prediction.\nWe will use only sklearn and pandas libraries in this task.\ngdpPerCapita is the response (target) variable.\n(2 points)\n\nAll other columns (except Index, geographic_location, and country) will serve as predictors.\n(2 points)\n\nThe continent column must be one-hot-encoded using OneHotEncoder.\n(5 points)\nOutput the encoded dummy variables (feature names) to confirm successful encoding.\n(2 points)\n\nTrain a LinearRegression model. Split the dataset into a training set (90%) and a test set (10%).\n\nSet random_state=3 for reproducibility.\n(3 points)\n\n\nCalculate the RMSE and R-squared for both the training set and the test set.\n(3 points)\n\nDiscuss whether your model shows signs of overfitting or underfitting.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Assignment 2</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#instructions",
    "href": "Assignment 3.html#instructions",
    "title": "Appendix C — Assignment 3",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answer in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to understand and grade.\nUse Quarto to print the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission. (1 point)\nThere aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#data-description",
    "href": "Assignment 3.html#data-description",
    "title": "Appendix C — Assignment 3",
    "section": "Data description",
    "text": "Data description\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls, where bank clients were called to subscribe for a term deposit.\nThere is one train data - train.csv, which you will use to develop a model. There are two test datasets - test1.csv and test2.csv, which you will use to test your model. Each observation is a phone call and each column is a variable about the client or the phone call. Each dataset has the following attributes about the clients called in the marketing campaign:\n\nage: Age of the client\neducation: Education level of the client\nday: Day of the month the call is made\nmonth: Month of the call\ny: did the client subscribe to a term deposit?\nduration: Call duration, in seconds. This attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for inference purposes and should be discarded if the intention is to have a realistic predictive model.\n\n(Source: UCI Data Archive. Please use the given datasets for the assignment, not the raw data from the source. It is just for reference.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#instructions-suggestions-for-answering-questions",
    "href": "Assignment 3.html#instructions-suggestions-for-answering-questions",
    "title": "Appendix C — Assignment 3",
    "section": "Instructions / suggestions for answering questions",
    "text": "Instructions / suggestions for answering questions\n\nInstruction: Use train.csv for all questions, unless otherwise stated.\nSuggestion 1: You may use the functions in the class notes for printing the confusion matrix and the overall classification accuracy based on test / train data.\nSuggestion 2:: If you make variable transformations, you will need to do it for all the three datasets. Your code will be a bit concise if you make a function containing all the transformations, and then call it for the training and the two test datasets. You can put this function in the beginning of the code and keep adding transformations to it as you proceed with the assignment. You may need transformations in questions (1) and (13).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#section",
    "href": "Assignment 3.html#section",
    "title": "Appendix C — Assignment 3",
    "section": "1)",
    "text": "1)\nRead the datasets. Make an appropriate visualization to visualize how the proportion of clients subscribing to a term deposit change with increasing call duration.\n(4 points)\nHints:\n\nBin duration to create duration_binned. Group the data to find the fraction of clients responding positively to the marketing campaign for each bin in duration_binned. Make a lineplot of percentage of clients subscribing to a term deposit vs duration_binned, where the bins in duration_binned are arranged in increasing order of duration.\nYou may choose an appropriate number of bins & type of binning that helps you visualize well.\nYou may also think of other ways of visualization. You don’t need to stick with this one.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#predictor-duration",
    "href": "Assignment 3.html#predictor-duration",
    "title": "Appendix C — Assignment 3",
    "section": "2) Predictor duration",
    "text": "2) Predictor duration\nBased on the plot in (1), comment whether duration seems to be a useful variable to predict if the client will subscribe to a term deposit.\n(1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#model-based-on-duration",
    "href": "Assignment 3.html#model-based-on-duration",
    "title": "Appendix C — Assignment 3",
    "section": "3) Model based on duration",
    "text": "3) Model based on duration\nDevelop a logistic regression model to predict if the client subscribed to a term deposit based on call duration. Use the model to make a lineplot showing the probability of the client subscribing to a term deposit based on call duration.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#note",
    "href": "Assignment 3.html#note",
    "title": "Appendix C — Assignment 3",
    "section": "Note",
    "text": "Note\nAnswer questions 4 to 11 based on the regression model developed in (3).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#model-significance",
    "href": "Assignment 3.html#model-significance",
    "title": "Appendix C — Assignment 3",
    "section": "4) Model significance",
    "text": "4) Model significance\nIs the regression model in statistically significant? Justify your answer.\n(1 point for code, 1 point for answer)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#subscription-probability-in-5-minutes",
    "href": "Assignment 3.html#subscription-probability-in-5-minutes",
    "title": "Appendix C — Assignment 3",
    "section": "5) Subscription probability in 5 minutes",
    "text": "5) Subscription probability in 5 minutes\nWhat is the probability that the client subscribes to a term deposit with a 5-minute marketing call? Note that the call duration in data is given in seconds.\n(2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#call-duration-for-subscription",
    "href": "Assignment 3.html#call-duration-for-subscription",
    "title": "Appendix C — Assignment 3",
    "section": "6) Call duration for subscription",
    "text": "6) Call duration for subscription\nWhat is the minimum call duration (in minutes) for which a client has a 95% or higher chance of subscribing to a term deposit?\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#maximum-call-duration",
    "href": "Assignment 3.html#maximum-call-duration",
    "title": "Appendix C — Assignment 3",
    "section": "7) Maximum call duration",
    "text": "7) Maximum call duration\nWhat is the maximum call duration (in minutes) in which a client refused to subscribe to a term deposit? What was the probability of the client subscribing to the term deposit in that call?\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#percent-increase-in-odds",
    "href": "Assignment 3.html#percent-increase-in-odds",
    "title": "Appendix C — Assignment 3",
    "section": "8) Percent increase in odds",
    "text": "8) Percent increase in odds\nWhat is the percentage increase in the odds of a client subscribing to a term deposit when the call duration increases by a minute?\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#doubling-the-subscription-odds",
    "href": "Assignment 3.html#doubling-the-subscription-odds",
    "title": "Appendix C — Assignment 3",
    "section": "9) Doubling the subscription odds",
    "text": "9) Doubling the subscription odds\nHow much must the call duration increase (in minutes) so that it doubles the odds of the client subscribing to a term deposit.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#classification-accuracy",
    "href": "Assignment 3.html#classification-accuracy",
    "title": "Appendix C — Assignment 3",
    "section": "10) Classification accuracy",
    "text": "10) Classification accuracy\nWhat is minimum overall classification accuracy of the model among the classification accuracies on train.csv, test1.csv and test2.csv? Consider a threshold of 30% when classifying observations.\n(2 + 1 + 1 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#recall",
    "href": "Assignment 3.html#recall",
    "title": "Appendix C — Assignment 3",
    "section": "11) Recall",
    "text": "11) Recall\nWhat is the minimum Recall of the model among the Recall performance on train.csv, test1.csv and test2.csv? Consider a decision threshold probability of 30% when classifying observations.\nHere, Recall is the proportion of clients predicted to subscribe to a term deposit among those who actually subscribed.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#subscription-probability-based-on-age-and-education",
    "href": "Assignment 3.html#subscription-probability-based-on-age-and-education",
    "title": "Appendix C — Assignment 3",
    "section": "12) Subscription probability based on age and education",
    "text": "12) Subscription probability based on age and education\nDevelop a logistic regression model to predict the probability of a client subscribing to a term deposit based on age, education and the two-factor interaction between age and education. Based on the model, answer:\n\nPeople with which type of education (primary / secondary / tertiary / unknown) have the highest percentage increase in odds of subscribing to a term deposit with a unit increase in age? Justify your answer.\nWhat is the percentage increase in odds of a person subscribing to a term deposit for a unit increase in age, if the person has tertiary education.\nWhat is the percentage increase in odds of a person subscribing to a term deposit for a unit increase in age, if the person has primary education.\n\n(1 point for developing the model, 3 points for (a), 3 points for (b), 3 points for (c))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#model-development",
    "href": "Assignment 3.html#model-development",
    "title": "Appendix C — Assignment 3",
    "section": "13) Model development",
    "text": "13) Model development\nDevelop a logistic regression model (using train.csv) to predict the probability of a client subscribing to a term deposit based on age, education, day and month. The model must have:\n\nMinimum overall classification accuracy of 75% among the classification accuracies on train.csv, test1.csv and test2.csv.\nMinimum recall of 50% among the recall performance on train.csv, test1.csv and test2.csv.\n\nFor all the three datasets - train.csv, test1.csv and test2.csv, print the:\n\nModel summary (only for train.csv),\nConfusion matrices,\nOverall classification accuracies, and\nRecall\n\nNote that:\n\nYou cannot use duration as a predictor because its value is determined after the marketing call ends. However, after the call ends, we already know whether the client responded positively or negatively. That is why we have used duration only for inference in the previous questions. It helped us understand the effect of the length of the call on marketing success.\nIt is possible to develop the model satisfying constrains (a) and (b) with just appropriate transformation(s) of the predictor(s). However, you may consider interactions if you wish. Justify the transformations, if any, with visualizations.\nYou are free to choose any value of the decision threshold probability for classifying observations. However, you must use the same threshold on all the three datasets.\n\n(10 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#roc-auc",
    "href": "Assignment 3.html#roc-auc",
    "title": "Appendix C — Assignment 3",
    "section": "14) ROC-AUC",
    "text": "14) ROC-AUC\nReport the probability that the model will predict a higher probability of response for a customer who signs up for the term deposit as compared to the customer who does not sign up, i.e., the ROC-AUC of the developed model in (13).\nHint: Use the functions roc_curve, and auc from the sklearn.metrics module\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#net-profit",
    "href": "Assignment 3.html#net-profit",
    "title": "Appendix C — Assignment 3",
    "section": "15) Net-profit",
    "text": "15) Net-profit\nSuppose that the model developed in (13) is used to predict the clients in test1.csv and test2.csv who will respond positively to the campaign. Only those clients who are predicted to respond positively are called during the marketing campaign. Assume that:\n\nA profit of \\$100 is associated with a client who responds positively to the campaign,\nA loss of \\$10 is associated with a client who responds negatively to the campaign\n\nWhat is the net profit from the campaign? Use the confusion matrices printed in (13).\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#decision-threshold-probability",
    "href": "Assignment 3.html#decision-threshold-probability",
    "title": "Appendix C — Assignment 3",
    "section": "16) Decision threshold probability",
    "text": "16) Decision threshold probability\nBased on the profit and loss associated with client responses specified in (15), and the model developed in (13), find the decision threshold probability of classification, such that the net profit is maximized. Use train.csv\nProceed as follows:\n\nYou would have obtained FPR and TPR for all potential decision threshold probabilities in (14).\nFormulate an expression quantifying the net profit per client, in terms of FPR, TPR, and the overall response rate, i.e., proportion of people actually subscribing to the term deposit.\nFind the decision threshold probability that maximizes the expression in (2).\n\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#net-profit-based-on-new-decision-threshold-probability",
    "href": "Assignment 3.html#net-profit-based-on-new-decision-threshold-probability",
    "title": "Appendix C — Assignment 3",
    "section": "17) Net profit based on new decision threshold probability",
    "text": "17) Net profit based on new decision threshold probability\nUsing the new decision threshold probability obtained in (16), answer (15), i.e., what is the net-profit associated with the clients in test1.csv and test2.csv if a marketing campaign is performed. Again, only those clients who are predicted to respond positively, based on the new decision threshold probability, are called during the marketing campaign\nAlso, print the confusion matrices for predictions on test1.csv and test2.csv with the new threshold probability.\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#model-preference",
    "href": "Assignment 3.html#model-preference",
    "title": "Appendix C — Assignment 3",
    "section": "18) Model preference",
    "text": "18) Model preference\nWas the classification accuracy of the model in (13) higher than that of the model in (17)? If yes, then should you prefer the model in (13) for the marketing campaign? Why or why not?\nNote: The model in (17) is the same as in (13), except with a different decision threshold probability\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#roc-curve",
    "href": "Assignment 3.html#roc-curve",
    "title": "Appendix C — Assignment 3",
    "section": "19) ROC curve",
    "text": "19) ROC curve\nPlot the ROC curve for the model developed in (13). Mark the point on the curve corresponding to the decision threshold probability identified in (16).\nNote that the ROC curve is independent of the decision threshold probability used by the model for prediction\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#profit-with-tpr-fpr",
    "href": "Assignment 3.html#profit-with-tpr-fpr",
    "title": "Appendix C — Assignment 3",
    "section": "20) Profit with TPR / FPR",
    "text": "20) Profit with TPR / FPR\nMake a scatterplot of TPR vs FPR, and color the points based on net profit per client.\nYou can use the following code to make the plot if you have the relevant metrics in tpr, fpr, and net_profit\n(1 point)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (9,6)\nplt.rcParams[\"figure.autolayout\"] = True\nf, ax = plt.subplots()\npoints = ax.scatter(fpr, tpr, c = net_profit, s=50, cmap=\"Blues\")\nf.colorbar(points, label = \"Net profit ($) \\n(per client)\")\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#precision-recall",
    "href": "Assignment 3.html#precision-recall",
    "title": "Appendix C — Assignment 3",
    "section": "21) Precision-recall",
    "text": "21) Precision-recall\nCompare the precision and recall of the models in (13) and (17) on train.csv.\nNote: The model in (17) is the same as in (13), except with a different decision threshold probability\n(4 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#precision-recall-important-metric",
    "href": "Assignment 3.html#precision-recall-important-metric",
    "title": "Appendix C — Assignment 3",
    "section": "22) Precision-recall: important metric",
    "text": "22) Precision-recall: important metric\nBased on the above comparison, which metric among precision and recall turns out to be more important for maximizing the net profit in the marketing campaign?\n(1 point)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#precision-recall-curve",
    "href": "Assignment 3.html#precision-recall-curve",
    "title": "Appendix C — Assignment 3",
    "section": "23) Precision-recall curve",
    "text": "23) Precision-recall curve\nPlot the precision-recall curve vs decision threshold probability for the model developed in (13). Mark the points on the curve corresponding to the decision threshold probability identified in (16).\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#precision-recall-vs-fpr-tpr",
    "href": "Assignment 3.html#precision-recall-vs-fpr-tpr",
    "title": "Appendix C — Assignment 3",
    "section": "24) Precision-recall vs FPR-TPR",
    "text": "24) Precision-recall vs FPR-TPR\nInstead of using the FPR and TPR metrics to find the optimum decision threshold probability in (16), use the precision-recall metrics to find the same.\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Assignment 3.html#sklearn",
    "href": "Assignment 3.html#sklearn",
    "title": "Appendix C — Assignment 3",
    "section": "25) Sklearn",
    "text": "25) Sklearn\nUsing train.csv and only sklearn, pandas, and numpy, train a Logistic Regression model. You need the following steps:\n\nThe response is still y.\nPredictors are education, month, day and age.\nNumerical predictors need to be transformed to all their second-order polynomial versions.\n\nLogistic regression benefits from feature scaling, especially when using polynomial features, as it helps the optimization algorithm converge faster. Use StandardScaler or MinMaxScaler to scale the features.\nCategorical predictors need to be one-hot-encoded. They should not interact with the numerical predictors.\n\nPrint the accuracy and recall for both training and test data using a threshold of 0.11. Use test1.csv as the test dataset. Remember that the test dataset needs to go through the exact same transformation as the training dataset.\nHints:\n\nDo not scale categorical features: One-hot encoded dummy variables are already on the same scale (0 or 1). Scaling them is unnecessary and may distort their meaning.\nScale numerical features: This includes both original numerical features and any polynomial features generated from them.\nUse a structured approach: A production-ready way to handle these transformations is by using ColumnTransformer within a Pipeline, ensuring that preprocessing steps are applied consistently. However, using a pipeline is optional.\n\n(8 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Assignment 3</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix D — Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]