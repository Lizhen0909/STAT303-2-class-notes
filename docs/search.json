[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science II with python (Class notes)",
    "section": "",
    "text": "Preface\nThis book serves as the course notes for STAT 303 (Sec 20), Winter 2026 at Northwestern University. It is the second course in the Python sequence and introduces linear models and other essentials in predictive modeling. Key topics include linear regression, logistic regression, cross-validation, regularization, and feature selection. To get the most out of these notes, please read the accompanying textbook before each class.\nThis edition builds on foundational materials developed by Professor Arvind Krishna, whose work has provided a strong framework for this resource. We are grateful for his contributions, which continue to shape the book‚Äôs development.\nThroughout the quarter, we will update and refine these notes to improve clarity, depth, and alignment with course objectives. Because this is a living document, feedback is always welcome. Contributions from students, instructors, and the broader academic community help make it stronger and more effective.\nIf you have suggestions, please share them using our dedicated Textbook Improvement Form.\nWe hope these notes become a helpful companion as you build skills and confidence in data science with Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>index.html</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html",
    "href": "Lec1_SimpleLinearRegression.html",
    "title": "2¬† Simple Linear Regression",
    "section": "",
    "text": "2.1 Learning objectives\nRead section 3.1 of the book before using these notes.\nBefore coding, create a dedicated Python environment for this course (use the same method as STAT303-1). If you see a ModuleNotFoundError, install the missing package, for example pip install scikit-learn or pip install statsmodels.\nNote: these notes focus on implementation. The book provides the conceptual explanations.\nBy the end of this notebook you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#learning-objectives",
    "href": "Lec1_SimpleLinearRegression.html#learning-objectives",
    "title": "2¬† Simple Linear Regression",
    "section": "",
    "text": "Fit a simple linear regression model (\\(Y=\\beta_0+\\beta_1X+\\varepsilon\\)) using statsmodels and sklearn.\nInterpret the slope and intercept in context.\nEvaluate model performance on a held-out test set (RMSE, MAE, \\(R^2\\)).\nCompute and visualize confidence intervals (mean response) vs prediction intervals (new observation).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#dataset",
    "href": "Lec1_SimpleLinearRegression.html#dataset",
    "title": "2¬† Simple Linear Regression",
    "section": "2.2 Dataset",
    "text": "2.2 Dataset\n\nTraining set: Datasets/Car_features_train.csv, Datasets/Car_prices_train.csv\nTest set: Datasets/Car_features_test.csv, Datasets/Car_prices_test.csv\n\n\n# Reading training set\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv') # Predictors\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv') # Response\ntrain = pd.merge(trainf,trainp)\nprint(train.shape)\ntrain.head()\n\n(4960, 11)\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\n\n\n\n\n\n\n\n\n# Read the test data\ntestf = pd.read_csv('./Datasets/Car_features_test.csv') # Predictors\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv') # Response\ntest = pd.merge(testf, testp)\nprint(test.shape)\ntest.head()\n\n(2672, 11)\n\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n0\n12000\nmerc\nGLS Class\n2017\nAutomatic\n12046\nDiesel\n150\n37.1965\n3.0\n38000\n\n\n1\n12001\nvw\nAmarok\n2017\nAutomatic\n37683\nDiesel\n260\n36.2442\n3.0\n23495\n\n\n2\n12004\nmerc\nGLS Class\n2019\nAutomatic\n10000\nDiesel\n145\n33.8061\n3.0\n59999\n\n\n3\n12013\nskoda\nScala\n2019\nManual\n3257\nPetrol\n145\n49.8767\n1.0\n16713\n\n\n4\n12017\naudi\nRS6\n2015\nSemi-Auto\n20982\nPetrol\n325\n29.4571\n4.0\n46000",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "href": "Lec1_SimpleLinearRegression.html#simple-linear-regression",
    "title": "2¬† Simple Linear Regression",
    "section": "2.3 Simple Linear Regression",
    "text": "2.3 Simple Linear Regression\nWe will build a simple linear regression model to predict car price based on engine size.\n\n2.3.1 Training with statsmodels (formula API)\nWe will use statsmodels.formula.api. The formula interface lets you specify a model with a compact string (similar to R), which is convenient for statistical modeling.\n\nimport statsmodels.formula.api as smf\n\n\n# Let's create the model_sm\n    \n# ols stands for Ordinary Least Squares - the name of the algorithm that optimizes Linear Regression models\n\n# data input needs the dataframe that has the predictor and the response\n# formula input needs to:\n    # be a string\n    # have the following syntax: \"response~predictor\"\n    \n# Using engineSize to predict price\nols_object = smf.ols(formula = 'price~engineSize', data = train)\n\n\n#Using the fit() function of the 'ols' class to fit the model_sm, i.e., train the model_sm\nmodel_sm = ols_object.fit()\n\n\n#Printing model_sm summary which contains among other things, the model_sm coefficients\nmodel_sm.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.390\n\n\nModel:\nOLS\nAdj. R-squared:\n0.390\n\n\nMethod:\nLeast Squares\nF-statistic:\n3177.\n\n\nDate:\nFri, 16 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n09:29:29\nLog-Likelihood:\n-53949.\n\n\nNo. Observations:\n4960\nAIC:\n1.079e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.079e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-4122.0357\n522.260\n-7.893\n0.000\n-5145.896\n-3098.176\n\n\nengineSize\n1.299e+04\n230.450\n56.361\n0.000\n1.25e+04\n1.34e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1271.986\nDurbin-Watson:\n0.517\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n6490.719\n\n\nSkew:\n1.137\nProb(JB):\n0.00\n\n\nKurtosis:\n8.122\nCond. No.\n7.64\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe fitted equation (from this dataset) is: \\(\\hat{price}\\) = -4122.0357 + 12990 * engineSize\n\n\\(R^2\\) is 39%. This is the proportion of variance in car price explained by engineSize.\nThe coefficient of engineSize (\\(\\hat{\\beta}_1\\)) is statistically significant (p-value near 0), so there is evidence of a linear relationship.\nThe 95% CI for \\(\\hat{\\beta}_1\\) is [1.25e+04, 1.34e+04].\nPrediction intervals are not shown in the summary table.\n\nThe coefficient of engineSize is 1.299e+04 (dollars per liter).\n\nA one-unit increase in engineSize increases the expected price by about $12,990.\nA three-unit increase increases the expected price by about $38,970.\n\nThe coefficients can also be returned directly using the params attribute of the fitted model_sm object:\n\nmodel_sm.params\n\nIntercept     -4122.035744\nengineSize    12988.281021\ndtype: float64\n\n\nVisualize the regression line\n\nax = sns.scatterplot(x=train.engineSize, y=train.price, color=\"orange\")\nsns.lineplot(x=train.engineSize, y=model_sm.fittedvalues, color=\"blue\")\nplt.xlim(-1, 7)\nplt.xlabel(\"Engine size (in litres)\")\nplt.ylabel(\"Car price\")\nax.yaxis.set_major_formatter(\"${x:,.0f}\")\n\nlegend_elements = [\n    Line2D([0], [0], color=\"blue\", lw=4, label=\"Predicted (Model)\"),\n    Line2D([0], [0], marker=\"o\", color=\"w\", label=\"Actual\", markerfacecolor=\"orange\", markersize=10),\n ]\nax.legend(handles=legend_elements, loc=\"upper left\");\n\n\n\n\n\n\n\n\nThe same plot can be made with seaborn.regplot(). It fits a simple linear regression with y as the response and x as the predictor, then overlays the fitted line on the scatterplot.\n\nax = sns.regplot(\n    x=\"engineSize\",\n    y=\"price\",\n    data=train,\n    color=\"orange\",\n    line_kws={\"color\": \"blue\"},\n )\nplt.xlim(-1, 7)\nplt.xlabel(\"Engine size (in litres)\")\nplt.ylabel(\"Car price\")\nax.yaxis.set_major_formatter(\"${x:,.0f}\")\nax.legend(handles=legend_elements, loc=\"upper left\");\n\n# Note: some `engineSize` values appear to be 0; those are likely data issues\n# and should ideally be handled (e.g., imputed/filtered) before modeling.\n\n\n\n\n\n\n\n\nThe light shaded region around the blue line is a 95% confidence interval for the mean response.\n\n\n2.3.2 Model Prediction and Error Metrics\nNow evaluate the fitted model on the test set.\n\n# Predict car prices for the test set using the fitted statsmodels model\n# (statsmodels will match the predictor name `engineSize` from `testf`).\npred_price = model_sm.predict(testf)\n\nVisualize predicted prices against actual prices (perfect predictions would lie on the line \\(y=x\\)).\n\nsns.scatterplot(x = testp.price, y = pred_price, color = 'orange')\n#In case of a perfect prediction, all the points must lie on the line x = y.\nax = sns.lineplot(x = [0,testp.price.max()], y = [0,testp.price.max()],color='blue') #Plotting the line x = y.\nplt.xlabel('Actual price')\nplt.ylabel('Predicted price')\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\nplt.xticks(rotation=20);\n\n\n\n\n\n\n\n\nThe predictions are limited because we are using only one predictor. We can likely improve performance by adding more predictors in multiple linear regression.\nstatsmodels does not include common error metrics, so we compute RMSE and MAE with NumPy. (The \\(R^2\\) value is shown in the model summary above.)\n\nsm_rmse = np.sqrt(np.mean((testp.price - pred_price)**2))\nsm_mae = np.mean(np.abs(testp.price - pred_price))\nprint(f'Statsmodels Linear Regression Test RMSE: ${sm_rmse:,.2f}')\nprint(f'Statsmodels Linear Regression Test MAE: ${sm_mae:,.2f}')\n\nStatsmodels Linear Regression Test RMSE: $12,995.11\nStatsmodels Linear Regression Test MAE: $9,411.33\n\n\nOn this test set, the RMSE is around $13k.\n\n\n2.3.3 Confidence interval vs.¬†prediction interval\nNext we compute confidence and prediction intervals for the test-set predictions.\n\n#Using the get_prediction() function associated with the 'model_sm' object to get the intervals\nintervals = model_sm.get_prediction(testf)\n\n\n#The function requires specifying alpha (probability of Type 1 error) instead of the confidence level to get the intervals\nintervals.summary_frame(alpha=0.05)\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n1\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n2\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n3\n8866.245277\n316.580850\n8245.606701\n9486.883853\n-16254.905974\n33987.396528\n\n\n4\n47831.088340\n468.949360\n46911.740050\n48750.436631\n22700.782946\n72961.393735\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2667\n47831.088340\n468.949360\n46911.740050\n48750.436631\n22700.782946\n72961.393735\n\n\n2668\n34842.807319\n271.666459\n34310.220826\n35375.393812\n9723.677232\n59961.937406\n\n\n2669\n8866.245277\n316.580850\n8245.606701\n9486.883853\n-16254.905974\n33987.396528\n\n\n2670\n21854.526298\n184.135754\n21493.538727\n22215.513869\n-3261.551421\n46970.604017\n\n\n2671\n21854.526298\n184.135754\n21493.538727\n22215.513869\n-3261.551421\n46970.604017\n\n\n\n\n2672 rows √ó 6 columns\n\n\n\nShow the confidence and prediction intervals for the test-set prices.\n\ninterval_table = intervals.summary_frame(alpha=0.05)\n\n\nax = sns.scatterplot(x = testf.engineSize, y = pred_price,color = 'orange', s = 10)\nsns.lineplot(x = testf.engineSize, y = pred_price, color = 'red')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_lower, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.mean_ci_upper, color = 'blue')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_lower, color = 'green')\nsns.lineplot(x = testf.engineSize, y = interval_table.obs_ci_upper, color = 'green')\n\nlegend_elements = [Line2D([0], [0], color='red', label='Mean prediction'),\n                   Line2D([0], [0], color='blue', label='Confidence interval'),\n                  Line2D([0], [0], color='green', label='Prediction interval')]\nax.legend(handles=legend_elements, loc='upper left')\nplt.xlabel('Engine size (in litres)')\nplt.ylabel('Car price')\nax.yaxis.set_major_formatter('${x:,.0f}');\n\n\n\n\n\n\n\n\nThe prediction interval accounts for both noise and uncertainty in coefficient estimates, while the confidence interval only reflects coefficient uncertainty. The prediction interval is wider.\n\n\n2.3.4 Training with sklearn\nIn sklearn, we typically work with:\n\nX: a feature matrix (DataFrame or ndarray)\ny: a 1D target vector (Series or 1D ndarray)\n\nWe can fit the model and evaluate it, but (unlike statsmodels) we do not get p-values or confidence intervals from the fitted object.\n\nfrom sklearn.linear_model import LinearRegression\n\n\n# Prepare training data (X must be 2D; y should be 1D for sklearn metrics)\nX_train = train[[\"engineSize\"]]  # Note the double brackets to get a DataFrame (2D)\ny_train = train[\"price\"]\n\n# Create the sklearn linear regression model\nmodel_skl = LinearRegression()\n\n# Fit the model\nmodel_skl.fit(X_train, y_train)\n\n# Inspect fitted parameters\nslope = float(model_skl.coef_[0])\nintercept = float(model_skl.intercept_)\nprint(f\"Fitted model: price = {intercept:,.2f} + {slope:,.2f} * engineSize\")\n\nFitted model: price = -4,122.04 + 12,988.28 * engineSize\n\n\nLet‚Äôs make prediction next\n\n# Prepare test data\nX_test = test[[\"engineSize\"]]\ny_test = test[\"price\"]\n\n# Predict on test\ny_pred = model_skl.predict(X_test)\n\n# Evaluate (train + test)\ny_pred_train = model_skl.predict(X_train)\n\nsklearn provides metrics, so we can import RMSE, MAE, and \\(R^2\\) directly, the libary will take care of computing.\n\nfrom sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n\nmetrics = pd.DataFrame(\n    {\n        \"Split\": [\"Train\", \"Test\"],\n        \"RMSE\": [\n            root_mean_squared_error(y_train, y_pred_train),\n            root_mean_squared_error(y_test, y_pred),\n        ],\n        \"MAE\": [\n            mean_absolute_error(y_train, y_pred_train),\n            mean_absolute_error(y_test, y_pred),\n        ],\n        \"R2\": [\n            r2_score(y_train, y_pred_train),\n            r2_score(y_test, y_pred),\n        ],\n    }\n )\n\nmetrics\n\n\n\n\n\n\n\n\nSplit\nRMSE\nMAE\nR2\n\n\n\n\n0\nTrain\n12807.526232\n9260.285302\n0.390498\n\n\n1\nTest\n12995.106452\n9411.325913\n0.386990\n\n\n\n\n\n\n\nTest performance is worse than training performance, which is expected since the model was fit on the training set.\n\n# Quick diagnostics on the test set\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Actual vs Predicted\nsns.scatterplot(x=y_test, y=y_pred, color=\"orange\", s=20, ax=axes[0])\nmin_price = float(min(y_test.min(), y_pred.min()))\nmax_price = float(max(y_test.max(), y_pred.max()))\naxes[0].plot([min_price, max_price], [min_price, max_price], color=\"blue\")\naxes[0].set_title(\"Test: Actual vs Predicted\")\naxes[0].set_xlabel(\"Actual price\")\naxes[0].set_ylabel(\"Predicted price\")\naxes[0].xaxis.set_major_formatter(\"${x:,.0f}\")\naxes[0].yaxis.set_major_formatter(\"${x:,.0f}\")\n\n# Residuals vs Predicted\nresiduals = y_test - y_pred\nsns.scatterplot(x=y_pred, y=residuals, color=\"orange\", s=20, ax=axes[1])\naxes[1].axhline(0, color=\"blue\")\naxes[1].set_title(\"Test: Residuals vs Predicted\")\naxes[1].set_xlabel(\"Predicted price\")\naxes[1].set_ylabel(\"Residual (Actual - Predicted)\")\naxes[1].yaxis.set_major_formatter(\"${x:,.0f}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n2.3.5 Training with statsmodels (API)\nstatsmodels also provides a non-formula API (statsmodels.api) that works with matrices, similar to sklearn. This requires explicitly adding a constant term for the intercept but gives the same inference outputs (p-values, confidence intervals, etc.) as the formula API.\n\nimport statsmodels.api as sm\n\n# Prepare the predictor matrix (need to add a constant term for the intercept)\nX_train_sm = sm.add_constant(train[[\"engineSize\"]])\ny_train_sm = train[\"price\"]\n\nprint(\"X_train with constant term:\")\nprint(X_train_sm.head())\n\nX_train with constant term:\n   const  engineSize\n0    1.0         3.0\n1    1.0         3.0\n2    1.0         3.0\n3    1.0         3.0\n4    1.0         3.0\n\n\n\n# Create and fit the OLS model using the matrix API\nmodel_sm_api = sm.OLS(y_train_sm, X_train_sm).fit()\n\n# Display the model summary\nmodel_sm_api.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.390\n\n\nModel:\nOLS\nAdj. R-squared:\n0.390\n\n\nMethod:\nLeast Squares\nF-statistic:\n3177.\n\n\nDate:\nFri, 16 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n09:29:33\nLog-Likelihood:\n-53949.\n\n\nNo. Observations:\n4960\nAIC:\n1.079e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.079e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-4122.0357\n522.260\n-7.893\n0.000\n-5145.896\n-3098.176\n\n\nengineSize\n1.299e+04\n230.450\n56.361\n0.000\n1.25e+04\n1.34e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1271.986\nDurbin-Watson:\n0.517\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n6490.719\n\n\nSkew:\n1.137\nProb(JB):\n0.00\n\n\nKurtosis:\n8.122\nCond. No.\n7.64\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNotice the coefficients match the formula API results exactly:\n\nIntercept (const): -4122.04\nSlope (engineSize): 1.299e+04\n\nBoth approaches give the same model, but the matrix API requires manually adding the constant term using sm.add_constant().\n\n# Make predictions on the test set (remember to add constant term)\nX_test_sm = sm.add_constant(test[[\"engineSize\"]])\npred_price_api = model_sm_api.predict(X_test_sm)\n\n# Compare predictions from both statsmodels approaches\ncomparison = pd.DataFrame({\n    \"Formula API\": pred_price[:5],\n    \"Matrix API\": pred_price_api[:5],\n    \"Difference\": pred_price[:5] - pred_price_api[:5]\n})\nprint(\"First 5 predictions comparison:\")\nprint(comparison)\n\nFirst 5 predictions comparison:\n    Formula API    Matrix API    Difference\n0  34842.807319  34842.807319  7.275958e-12\n1  34842.807319  34842.807319  7.275958e-12\n2  34842.807319  34842.807319  7.275958e-12\n3   8866.245277   8866.245277  0.000000e+00\n4  47831.088340  47831.088340  0.000000e+00",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec1_SimpleLinearRegression.html#summary-and-key-takeaways",
    "href": "Lec1_SimpleLinearRegression.html#summary-and-key-takeaways",
    "title": "2¬† Simple Linear Regression",
    "section": "2.4 Summary and Key Takeaways",
    "text": "2.4 Summary and Key Takeaways\n\nAll three approaches estimate the same coefficients when the intercept is handled consistently.\nscikit-learn (LinearRegression) is simple and prediction-focused, with easy access to metrics, but no inference statistics.\nstatsmodels formula API provides rich statistical output and automatic intercept handling.\nstatsmodels matrix API provides the same inference, with more explicit control, but requires adding the intercept manually.\nChoose based on whether you need inference, prediction, or explicit model control.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html",
    "href": "Lec2_MultipleLinearRegression.html",
    "title": "3¬† Multiple Linear Regression",
    "section": "",
    "text": "3.1 Learning objectives\nRead section 3.2 of the book before using these notes.\nNote: these notes focus on implementation. The book provides the conceptual explanations.\nBy the end of this notebook you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#learning-objectives",
    "href": "Lec2_MultipleLinearRegression.html#learning-objectives",
    "title": "3¬† Multiple Linear Regression",
    "section": "",
    "text": "Fit a multiple linear regression model using statsmodels and sklearn.\nInterpret coefficients and overall model fit.\nEvaluate model performance on a held-out test set (RMSE, MAE, \\(R^2\\)).\nExplain why \\(R^2\\) can increase when adding irrelevant predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#dataset",
    "href": "Lec2_MultipleLinearRegression.html#dataset",
    "title": "3¬† Multiple Linear Regression",
    "section": "3.2 Dataset",
    "text": "3.2 Dataset\nWe will use the same dataset as SLR, specifically,\n\nTraining set: Datasets/Car_features_train.csv, Datasets/Car_prices_train.csv\nTest set: Datasets/Car_features_test.csv, Datasets/Car_prices_test.csv\n\nWe will predict car price using engineSize, age, mileage, and mpg.\n\n# Read training and test data\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntrain = pd.merge(trainf, trainp)\n\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\ntest = pd.merge(testf, testp)\n\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\ntrain.head()\n# Create age feature using the most recent model year in the training data\nreference_year = train[\"year\"].max()\ntrain[\"age\"] = reference_year - train[\"year\"]\ntest[\"age\"] = reference_year - test[\"year\"]\n\nTrain shape: (4960, 11)\nTest shape: (2672, 11)\n\n\nWe convert model year to age using reference_year = train[\"year\"].max(), then age = reference_year - year. This anchors age to the most recent model year in the training set and keeps ages non-negative.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "href": "Lec2_MultipleLinearRegression.html#multiple-linear-regression",
    "title": "3¬† Multiple Linear Regression",
    "section": "3.3 Multiple Linear Regression",
    "text": "3.3 Multiple Linear Regression\nInstead of only using engineSize, we will use age, mileage, mpg, and engineSize.\n\n3.3.1 Training with statsmodels (formula API)\nWe fit an OLS model with a formula string.\n\n# Fit multiple linear regression with statsmodels (formula API)\nmodel_sm = smf.ols(\n    formula='price ~ age + mileage + mpg + engineSize',\n    data=train,\n).fit()\n\nmodel_sm.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nFri, 16 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n10:40:20\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n1.102e+04\n738.393\n14.918\n0.000\n9567.676\n1.25e+04\n\n\nage\n-1817.7366\n73.751\n-24.647\n0.000\n-1962.322\n-1673.151\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n1.94e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.94e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nThe fitted equation (from this dataset) is approximately:\n\\(\\hat{price}\\) = b0 + b1 * age + b2 * mileage + b3 * mpg + b4 * engineSize\nInterpretation: holding the other variables fixed, a one-unit increase in each predictor changes the expected price by its coefficient amount.\n\n\n3.3.2 Model Prediction and Error Metrics with statsmodels\nPredict car prices for the test set.\n\npred_price_sm = model_sm.predict(test)\n\n#Compute RMSE and MAE using NumPy (since statsmodels does not provide these metrics directly).\nsm_rmse = np.sqrt(np.mean((testp.price - pred_price_sm) ** 2))\nsm_mae = np.mean(np.abs(testp.price - pred_price_sm))\nprint(f'Statsmodels Test RMSE: ${sm_rmse:,.2f}')\nprint(f'Statsmodels Test MAE: ${sm_mae:,.2f}')\n\nStatsmodels Test RMSE: $9,956.82\nStatsmodels Test MAE: $6,407.74\n\n\n\n\n3.3.3 Training with sklearn\nThe procedure mirrors simple linear regression: prepare X and y, fit the model, and inspect coefficients.\n\n# Prepare training data for sklearn\npredictors = [\"age\", \"mileage\", \"mpg\", \"engineSize\"]\nX_train = train[predictors]\ny_train = train[\"price\"]\n\nmodel_skl = LinearRegression()\nmodel_skl.fit(X_train, y_train)\n\ncoef_table = pd.Series(model_skl.coef_, index=predictors, name=\"coefficient\")\nprint(\"Intercept:\", model_skl.intercept_)\ncoef_table\n\nIntercept: 11015.252952206762\n\n\nage           -1817.736579\nmileage          -0.147450\nmpg             -79.312573\nengineSize    12178.359942\nName: coefficient, dtype: float64\n\n\n\n\n3.3.4 Model Prediction and Error Metrics with sklearn\nPredict on the test set and prepare metrics. Compute RMSE, MAE, and \\(R^2\\) for train and test.\n\nX_test = test[predictors]\ny_test = test[\"price\"]\n\ny_pred_skl = model_skl.predict(X_test)\ny_pred_train = model_skl.predict(X_train)\n\nmetrics = pd.DataFrame(\n    {\n        \"Split\": [\"Train\", \"Test\"],\n        \"RMSE\": [\n            root_mean_squared_error(y_train, y_pred_train),\n            root_mean_squared_error(y_test, y_pred_skl),\n        ],\n        \"MAE\": [\n            mean_absolute_error(y_train, y_pred_train),\n            mean_absolute_error(y_test, y_pred_skl),\n        ],\n        \"R2\": [\n            r2_score(y_train, y_pred_train),\n            r2_score(y_test, y_pred_skl),\n        ],\n    }\n)\nmetrics\n\n\n\n\n\n\n\n\nSplit\nRMSE\nMAE\nR2\n\n\n\n\n0\nTrain\n9558.926176\n6292.020876\n0.660482\n\n\n1\nTest\n9956.824980\n6407.739919\n0.640127\n\n\n\n\n\n\n\n\n\n3.3.5 Diagnostics (test set)\nCompare predicted vs actual prices. Perfect predictions lie on the line \\(y=x\\).\n\nsns.scatterplot(x=y_test, y=y_pred_skl, color=\"orange\", s=20)\nax = sns.lineplot(x=[0, y_test.max()], y=[0, y_test.max()], color=\"blue\")\nplt.xlabel(\"Actual price\")\nplt.ylabel(\"Predicted price\")\nax.yaxis.set_major_formatter('${x:,.0f}')\nax.xaxis.set_major_formatter('${x:,.0f}')\nplt.xticks(rotation=20);",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec2_MultipleLinearRegression.html#effect-of-adding-noisy-predictors-on-r2",
    "href": "Lec2_MultipleLinearRegression.html#effect-of-adding-noisy-predictors-on-r2",
    "title": "3¬† Multiple Linear Regression",
    "section": "3.4 Effect of adding noisy predictors on \\(R^2\\)",
    "text": "3.4 Effect of adding noisy predictors on \\(R^2\\)\nEven a useless predictor can increase training \\(R^2\\) because the model gains extra flexibility.\n\n# Add a random predictor and refit with statsmodels\nnp.random.seed(1)\ntrain = train.copy()\ntrain[\"rand_col\"] = np.random.rand(train.shape[0])\n\nmodel_sm_noise = smf.ols(\n    formula='price ~ age + mileage + mpg + engineSize + rand_col',\n    data=train,\n).fit()\n\nprint(f\"Base model R2: {model_sm.rsquared:.4f}\")\nprint(f\"With random predictor R2: {model_sm_noise.rsquared:.4f}\")\n\nBase model R2: 0.6605\nWith random predictor R2: 0.6605\n\n\n\n3.4.0.1 Same idea with sklearn\nCompare training \\(R^2\\) with and without the random predictor.\n\nX_train_base = train[predictors]\nX_train_noise = train[predictors + [\"rand_col\"]]\n\nmodel_skl_base = LinearRegression().fit(X_train_base, y_train)\nmodel_skl_noise = LinearRegression().fit(X_train_noise, y_train)\n\nr2_base = r2_score(y_train, model_skl_base.predict(X_train_base))\nr2_noise = r2_score(y_train, model_skl_noise.predict(X_train_noise))\n\npd.DataFrame(\n    {\n        \"Model\": [\"Base predictors\", \"With random predictor\"],\n        \"Train R2\": [r2_base, r2_noise],\n    }\n)\n\n\n\n\n\n\n\n\nModel\nTrain R2\n\n\n\n\n0\nBase predictors\n0.660482\n\n\n1\nWith random predictor\n0.660545\n\n\n\n\n\n\n\nAdding noise can increase training \\(R^2\\), but it does not guarantee better performance on new data. Use validation or adjusted \\(R^2\\) in MLR to guard against overfitting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html",
    "href": "Lec3_VariableTransformations_and_Interactions.html",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "",
    "text": "4.1 Overview\nThis notebook demonstrates advanced techniques for enhancing linear regression models by incorporating:\nPrerequisites: Read sections 3.3.1 and 3.3.2 of the textbook before using these notes.\nNote: These notes focus on practical implementation. The textbook provides the theoretical foundation.\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nimport statsmodels.formula.api as smf\nimport warnings\n\n# Configuration\npd.set_option('display.float_format', '{:.2f}'.format)\nnp.random.seed(42)  # For reproducibility\nsns.set_theme(style=\"whitegrid\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#overview",
    "href": "Lec3_VariableTransformations_and_Interactions.html#overview",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "",
    "text": "Interaction terms between variables\nQualitative (categorical) predictors\nPolynomial transformations\nMixed interactions between categorical and continuous variables",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#learning-objectives",
    "href": "Lec3_VariableTransformations_and_Interactions.html#learning-objectives",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.2 Learning Objectives",
    "text": "4.2 Learning Objectives\nBy the end of this notebook, you should be able to:\n\n4.2.1 Core Concepts\n\n‚úÖ Understand when and why to use interaction terms in regression models\n‚úÖ Interpret coefficients in models with interactions\n‚úÖ Apply different encoding strategies for categorical predictors\n\n\n\n4.2.2 Technical Skills\n\n‚úÖ Implement interaction terms using both statsmodels and sklearn\n‚úÖ Use PolynomialFeatures to generate polynomial and interaction terms\n‚úÖ Handle categorical variables with OneHotEncoder and get_dummies\n‚úÖ Create robust preprocessing pipelines with ColumnTransformer\n\n\n\n4.2.3 Advanced Applications\n\n‚úÖ Model complex relationships between qualitative and continuous predictors\n‚úÖ Apply polynomial transformations to capture non-linear patterns\n‚úÖ Evaluate and compare different model specifications\n‚úÖ Avoid common pitfalls like data leakage and multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#dataset",
    "href": "Lec3_VariableTransformations_and_Interactions.html#dataset",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.3 Dataset",
    "text": "4.3 Dataset\n\nTraining set: Datasets/Car_features_train.csv, Datasets/Car_prices_train.csv\nTest set: Datasets/Car_features_test.csv, Datasets/Car_prices_test.csv\n\nWe replace year with age, computed as reference_year - year, where reference_year is the most recent model year in the training data.\n\n# Read data\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\ntrain = pd.merge(trainf, trainp)\ntest = pd.merge(testf, testp)\n\n# Create age feature using the most recent model year in the training data\nreference_year = train[\"year\"].max()\ntrain[\"age\"] = reference_year - train[\"year\"]\ntest[\"age\"] = reference_year - test[\"year\"]\n\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\nage\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.33\n3.00\n37980\n0\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.04\n3.00\n33980\n1\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.44\n3.00\n36850\n0\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.51\n3.00\n25998\n3\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.49\n3.00\n18990\n5",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#part-1-working-with-statsmodels",
    "href": "Lec3_VariableTransformations_and_Interactions.html#part-1-working-with-statsmodels",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.4 Part 1: Working with statsmodels",
    "text": "4.4 Part 1: Working with statsmodels\n\n4.4.1 Interaction Terms (Continuous Variables)\n\n4.4.1.1 The Problem with Additive Models\nIn a standard linear model: price = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óage + Œ≤‚ÇÇ√óengineSize + Œµ\nThis assumes the effect of engineSize on price is constant regardless of the car‚Äôs age. But what if:\n\nLarger engines depreciate faster with age?\nThe value premium for larger engines changes over time?\n\n\n\n4.4.1.2 Solution: Add an Interaction Term\nWe relax the constant-association assumption by adding an interaction between engineSize and age:\nprice = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óage + Œ≤‚ÇÇ√óengineSize + Œ≤‚ÇÉ√ó(age √ó engineSize) + Œµ\nNow the effect of engineSize depends on age, and vice versa.\n\n# Fit model with interaction between engineSize and age\n# The '*' operator creates both main effects and their interaction\nmodel_sm_interaction = smf.ols(\n    formula='price ~ age*engineSize + mileage + mpg',\n    data=train,\n).fit()\n\nmodel_sm_interaction.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.682\n\n\nModel:\nOLS\nAdj. R-squared:\n0.681\n\n\nMethod:\nLeast Squares\nF-statistic:\n2121.\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n08:15:24\nLog-Likelihood:\n-52338.\n\n\nNo. Observations:\n4960\nAIC:\n1.047e+05\n\n\nDf Residuals:\n4954\nBIC:\n1.047e+05\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4313.7409\n804.871\n5.360\n0.000\n2735.838\n5891.644\n\n\nage\n275.3833\n135.695\n2.029\n0.042\n9.361\n541.405\n\n\nengineSize\n1.52e+04\n248.298\n61.231\n0.000\n1.47e+04\n1.57e+04\n\n\nage:engineSize\n-896.7687\n49.431\n-18.142\n0.000\n-993.676\n-799.861\n\n\nmileage\n-0.1525\n0.008\n-17.954\n0.000\n-0.169\n-0.136\n\n\nmpg\n-84.3417\n9.048\n-9.322\n0.000\n-102.079\n-66.604\n\n\n\n\n\n\n\n\nOmnibus:\n2330.413\nDurbin-Watson:\n0.524\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n29977.437\n\n\nSkew:\n1.908\nProb(JB):\n0.00\n\n\nKurtosis:\n14.423\nCond. No.\n2.22e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.22e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n\n4.4.1.3 Understanding the Interaction Model\nThe complete model equation is: \\[\\text{price} = \\beta_0 + \\beta_1 \\cdot \\text{age} + \\beta_2 \\cdot \\text{engineSize} + \\beta_3 \\cdot (\\text{age} \\times \\text{engineSize}) + \\beta_4 \\cdot \\text{mileage} + \\beta_5 \\cdot \\text{mpg}\\]\n\n\n4.4.1.4 Key Insights:\n1. The effect of engineSize on price now depends on age: - For a car of age \\(t\\): \\(\\frac{\\partial \\text{price}}{\\partial \\text{engineSize}} = \\beta_2 + \\beta_3 \\cdot t\\)\n2. The effect of age on price depends on engineSize: - For a car with engine size \\(s\\): \\(\\frac{\\partial \\text{price}}{\\partial \\text{age}} = \\beta_1 + \\beta_3 \\cdot s\\)\n3. Interpretation of Œ≤‚ÇÉ (interaction coefficient): - How much the effect of engine size changes for each additional year of age - How much the effect of age changes for each additional liter of engine size\n\n# Evaluate model performance on test data\npred_price = model_sm_interaction.predict(test)\nrmse_interaction = np.sqrt(((test.price - pred_price) ** 2).mean())\nmae_interaction = np.mean(np.abs(test.price - pred_price))\nr2_interaction = 1 - np.sum((test.price - pred_price)**2) / np.sum((test.price - np.mean(test.price))**2)\n\nprint(\"Model Performance on Test Set:\")\nprint(\"=\" * 40)\nprint(f\"RMSE: ${rmse_interaction:,.2f}\")\nprint(f\"MAE:  ${mae_interaction:,.2f}\")\nprint(f\"R¬≤:   {r2_interaction:.4f}\")\n\n# Store results for later comparison\ninteraction_results = {\n    'RMSE': rmse_interaction,\n    'MAE': mae_interaction,\n    'R2': r2_interaction\n}\n\nModel Performance on Test Set:\n========================================\nRMSE: $9,423.60\nMAE:  $6,140.26\nR¬≤:   0.6776\n\n\n\n\n4.4.1.5 Key Takeaway: The Bias-Variance Trade-off\nBenefits of Interaction Terms:\n\n‚úÖ Capture complex, realistic relationships between variables\n‚úÖ Improve model flexibility and potentially better fit\n‚úÖ Allow effects to vary contextually\n\nPotential Drawbacks:\n\n‚ö†Ô∏è Increased model complexity ‚Üí higher variance\n‚ö†Ô∏è Risk of overfitting, especially with limited data\n‚ö†Ô∏è More difficult to interpret\n‚ö†Ô∏è Require larger sample sizes for reliable estimation\n\nBest Practice: Always validate interaction terms using cross-validation or hold-out data! (will be covered later)\n\n\n\n4.4.2 Qualitative (Categorical) Predictors\n\n4.4.2.1 Understanding Categorical Variables in Regression\nWhen we include a categorical variable in regression, we need to convert it to numerical form. The most common approach is dummy coding (also called one-hot encoding).\n\n\n4.4.2.2 Example: Transmission Type\nLet‚Äôs develop a model predicting price based on engineSize and the categorical variable transmission.\n\n# Explore the categorical variable\nprint(\"Transmission Type Distribution:\")\nprint(\"=\" * 40)\ntransmission_counts = train.transmission.value_counts()\nprint(transmission_counts)\n\nTransmission Type Distribution:\n========================================\ntransmission\nManual       1948\nAutomatic    1660\nSemi-Auto    1351\nOther           1\nName: count, dtype: int64\n\n\n\n\n4.4.2.3 Handling Rare Categories\nThe Other category has very few observations (&lt; 1% of data). This can cause issues:\n\nUnreliable estimates due to small sample size\nOverfitting risk\nUnstable predictions on new data\n\nCommon Solutions:\n\nRemove the rare category (what we‚Äôll do here)\nCombine with the most similar category\nCreate a ‚ÄúRare‚Äù category grouping multiple small categories\n\n\n# Remove the rare 'Other' category\ntrain_updated = train[train.transmission != 'Other'].copy()\n\nprint(f\"\\nUpdated distribution:\")\nprint(train_updated.transmission.value_counts())\n\n\nUpdated distribution:\ntransmission\nManual       1948\nAutomatic    1660\nSemi-Auto    1351\nName: count, dtype: int64\n\n\n\n# Fit model with categorical predictor\n# statsmodels automatically creates dummy variables\nmodel_sm_trans = smf.ols(\n    formula='price ~ engineSize + transmission',\n    data=train_updated,\n).fit()\n\nprint(\"Model with Categorical Predictor:\")\nprint(\"=\" * 50)\nmodel_sm_trans.summary()\n\nModel with Categorical Predictor:\n==================================================\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.459\n\n\nModel:\nOLS\nAdj. R-squared:\n0.458\n\n\nMethod:\nLeast Squares\nF-statistic:\n1400.\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n08:16:23\nLog-Likelihood:\n-53644.\n\n\nNo. Observations:\n4959\nAIC:\n1.073e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.073e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3042.6765\n661.190\n4.602\n0.000\n1746.451\n4338.902\n\n\ntransmission[T.Manual]\n-6770.6165\n442.116\n-15.314\n0.000\n-7637.360\n-5903.873\n\n\ntransmission[T.Semi-Auto]\n4994.3112\n442.989\n11.274\n0.000\n4125.857\n5862.765\n\n\nengineSize\n1.023e+04\n247.485\n41.323\n0.000\n9741.581\n1.07e+04\n\n\n\n\n\n\n\n\nOmnibus:\n1575.518\nDurbin-Watson:\n0.579\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n11006.609\n\n\nSkew:\n1.334\nProb(JB):\n0.00\n\n\nKurtosis:\n9.793\nCond. No.\n11.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n4.4.2.4 Understanding Dummy Variable Encoding\nKey Concepts:\n\nReference Category (Baseline): The category that‚Äôs omitted to avoid perfect multicollinearity\n\nHere: ‚ÄúAutomatic‚Äù (chosen alphabetically by default)\nAll other coefficients are interpreted relative to this baseline\n\nCoefficient Interpretation:\n\ntransmission[T.Manual]: How much Manual cars cost compared to Automatic cars, holding engine size constant\ntransmission[T.Semi-Auto]: How much Semi-Auto cars cost compared to Automatic cars, holding engine size constant\n\nWhy Drop One Category?\n\nWith k categories, we only need k-1 dummy variables\nIncluding all k would create perfect multicollinearity (dummy trap)\n\n\nQ: Interpret the intercept term.\nA: For a hypothetical car with zero engine size and Automatic transmission, the estimated mean price is the intercept.\nQ: Interpret the coefficient of transmission[T.Manual].\nA: Holding engine size fixed, the estimated mean price of a manual car differs from an automatic car by that coefficient.\n\n# Visualize the model with parallel lines\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Model predictions\nx = np.linspace(train_updated.engineSize.min(), train_updated.engineSize.max(), 100)\nparams = model_sm_trans.params\n\n# Calculate predictions for each transmission type\nauto_pred = params['engineSize'] * x + params['Intercept']\nmanual_pred = params['engineSize'] * x + params['Intercept'] + params['transmission[T.Manual]']\nsemi_pred = params['engineSize'] * x + params['Intercept'] + params['transmission[T.Semi-Auto]']\n\nax1.plot(x, auto_pred, 'r-', linewidth=2, label='Automatic')\nax1.plot(x, semi_pred, 'b-', linewidth=2, label='Semi-Automatic')\nax1.plot(x, manual_pred, 'g-', linewidth=2, label='Manual')\n\nax1.legend()\nax1.set_xlabel('Engine Size (liters)')\nax1.set_ylabel('Predicted Price ($)')\nax1.set_title('Parallel Lines Model\\n(Constant Engine Size Effect)')\nax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Actual data points\nfor i, transmission in enumerate(['Automatic', 'Semi-Auto', 'Manual']):\n    data = train_updated[train_updated.transmission == transmission]\n    colors = ['red', 'blue', 'green']\n    ax2.scatter(data.engineSize, data.price, \n               alpha=0.6, s=30, c=colors[i], label=transmission)\n\nax2.set_xlabel('Engine Size (liters)')\nax2.set_ylabel('Actual Price ($)')\nax2.set_title('Actual Data Points')\nax2.legend()\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üìä Model Assumption: Parallel lines indicate that the effect of engine size\")\nprint(\"   is the same across all transmission types (additive model).\")\n\n\n\n\n\n\n\n\nüìä Model Assumption: Parallel lines indicate that the effect of engine size\n   is the same across all transmission types (additive model).\n\n\n\n\n\n4.4.3 Interactions Between Categorical and Continuous Variables\n\n4.4.3.1 Problem with Parallel Lines\nThe model above assumes the effect of engine size is identical across transmission types (parallel lines). But what if:\n\nManual cars lose value differently with engine size than automatics?\nSports cars (often manual) command higher premiums for large engines?\nDifferent transmission types have different market dynamics?\n\n\n\n4.4.3.2 Solution: Add Interaction Terms\nWe can relax the parallel lines assumption by adding interactions between engineSize and transmission.\n\n# Fit model with interaction between continuous and categorical variables\n# The '*' creates main effects AND interaction terms\nmodel_sm_trans_inter = smf.ols(\n    formula='price ~ engineSize*transmission',\n    data=train_updated,\n).fit()\n\nprint(\"Model with Categorical-Continuous Interaction:\")\nprint(\"=\" * 55)\nmodel_sm_trans_inter.summary()\n\nModel with Categorical-Continuous Interaction:\n=======================================================\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.479\n\n\nModel:\nOLS\nAdj. R-squared:\n0.478\n\n\nMethod:\nLeast Squares\nF-statistic:\n909.9\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n08:17:28\nLog-Likelihood:\n-53550.\n\n\nNo. Observations:\n4959\nAIC:\n1.071e+05\n\n\nDf Residuals:\n4953\nBIC:\n1.072e+05\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n3754.7238\n895.221\n4.194\n0.000\n1999.695\n5509.753\n\n\ntransmission[T.Manual]\n1768.5856\n1294.071\n1.367\n0.172\n-768.366\n4305.538\n\n\ntransmission[T.Semi-Auto]\n-5282.7164\n1416.472\n-3.729\n0.000\n-8059.628\n-2505.805\n\n\nengineSize\n9928.6082\n354.511\n28.006\n0.000\n9233.610\n1.06e+04\n\n\nengineSize:transmission[T.Manual]\n-5285.9059\n646.175\n-8.180\n0.000\n-6552.695\n-4019.117\n\n\nengineSize:transmission[T.Semi-Auto]\n4162.2428\n552.597\n7.532\n0.000\n3078.908\n5245.578\n\n\n\n\n\n\n\n\nOmnibus:\n1379.846\nDurbin-Watson:\n0.622\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n9799.471\n\n\nSkew:\n1.139\nProb(JB):\n0.00\n\n\nKurtosis:\n9.499\nCond. No.\n30.8\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nQ: Interpret the coefficient of transmission[T.Manual].\nA: It shifts the intercept for manual cars relative to automatic cars.\nQ: Interpret the coefficient of engineSize:transmission[T.Manual].\nA: It adjusts the slope of engine size for manual cars relative to automatic cars.\n\n# Visualize the interaction model (non-parallel lines)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Model with interactions (non-parallel lines)\nx = np.linspace(train_updated.engineSize.min(), train_updated.engineSize.max(), 100)\nparams = model_sm_trans_inter.params\n\n# Calculate predictions for each transmission type (now with different slopes!)\nauto_pred = params['engineSize'] * x + params['Intercept']\n\nmanual_pred = ((params['engineSize'] + params['engineSize:transmission[T.Manual]']) * x + \n               params['Intercept'] + params['transmission[T.Manual]'])\n\nsemi_pred = ((params['engineSize'] + params['engineSize:transmission[T.Semi-Auto]']) * x + \n             params['Intercept'] + params['transmission[T.Semi-Auto]'])\n\nax1.plot(x, auto_pred, 'r-', linewidth=2, label='Automatic')\nax1.plot(x, semi_pred, 'b-', linewidth=2, label='Semi-Automatic') \nax1.plot(x, manual_pred, 'g-', linewidth=2, label='Manual')\n\nax1.legend()\nax1.set_xlabel('Engine Size (liters)')\nax1.set_ylabel('Predicted Price ($)')\nax1.set_title('Interaction Model\\n(Different Engine Size Effects)')\nax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Comparison of models at specific engine sizes\nengine_sizes = [1.0, 2.0, 3.0, 4.0]\ntransmission_types = ['Automatic', 'Manual', 'Semi-Auto']\ncolors = ['red', 'green', 'blue']\n\nx_pos = np.arange(len(engine_sizes))\nwidth = 0.25\n\nfor i, trans_type in enumerate(['Automatic', 'Manual', 'Semi-Auto']):\n    if trans_type == 'Automatic':\n        pred_values = [params['engineSize'] * es + params['Intercept'] for es in engine_sizes]\n    elif trans_type == 'Manual':\n        pred_values = [((params['engineSize'] + params['engineSize:transmission[T.Manual]']) * es + \n                       params['Intercept'] + params['transmission[T.Manual]']) for es in engine_sizes]\n    else:  # Semi-Auto\n        pred_values = [((params['engineSize'] + params['engineSize:transmission[T.Semi-Auto]']) * es + \n                       params['Intercept'] + params['transmission[T.Semi-Auto]']) for es in engine_sizes]\n    \n    ax2.bar(x_pos + i*width, pred_values, width, label=trans_type, color=colors[i], alpha=0.7)\n\nax2.set_xlabel('Engine Size (liters)')\nax2.set_ylabel('Predicted Price ($)')\nax2.set_title('Price Predictions by Engine Size & Transmission')\nax2.set_xticks(x_pos + width)\nax2.set_xticklabels(engine_sizes)\nax2.legend()\nax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate and display model performance\n# Handle potential missing categories in test data first (before prediction)\ntest_filtered = test[test.transmission.isin(['Automatic', 'Manual', 'Semi-Auto'])].copy()\npred_interaction_filtered = model_sm_trans_inter.predict(test_filtered)\n\nrmse_trans_inter = np.sqrt(((test_filtered.price - pred_interaction_filtered) ** 2).mean())\nmae_trans_inter = np.mean(np.abs(test_filtered.price - pred_interaction_filtered))\nr2_trans_inter = 1 - np.sum((test_filtered.price - pred_interaction_filtered)**2) / np.sum((test_filtered.price - np.mean(test_filtered.price))**2)\n\nprint(f\"\\nüìà Model Performance:\")\nprint(\"=\" * 40)\nprint(f\"RMSE: ${rmse_trans_inter:,.2f}\")\nprint(f\"MAE:  ${mae_trans_inter:,.2f}\")\nprint(f\"R¬≤:   {r2_trans_inter:.4f}\")\n\n# Note about filtered data\nprint(f\"\\nNote: Evaluated on {len(test_filtered)} test samples\")\nprint(f\"      (excluded {len(test) - len(test_filtered)} samples with 'Other' transmission)\")\n\n\nüìà Model Performance:\n========================================\nRMSE: $11,847.59\nMAE:  $8,342.65\nR¬≤:   0.4906\n\nNote: Evaluated on 2671 test samples\n      (excluded 1 samples with 'Other' transmission)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#part-2-working-with-scikit-learn",
    "href": "Lec3_VariableTransformations_and_Interactions.html#part-2-working-with-scikit-learn",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.5 Part 2: Working with scikit-learn",
    "text": "4.5 Part 2: Working with scikit-learn\n\n# Prepare data for scikit-learn\nprint(\"üîß Preparing Data for Scikit-Learn\")\n\n# Select features and target\nfeature_columns = ['mileage', 'engineSize', 'age', 'mpg']\nX_train = train[feature_columns].copy()\ny_train = train['price'].copy()\nX_test = test[feature_columns].copy() \ny_test = test['price'].copy()\n\nüîß Preparing Data for Scikit-Learn\n\n\n\n4.5.1 Generating Interaction Terms with PolynomialFeatures\n\n4.5.1.1 Understanding PolynomialFeatures\nThe PolynomialFeatures transformer is incredibly versatile:\nKey Parameters:\n\ndegree: Maximum degree of polynomial features\ninteraction_only: If True, only interaction terms (no x¬≤)\ninclude_bias: Whether to include intercept column\n\nExample with 2 features [x‚ÇÅ, x‚ÇÇ]:\n\n\n\nSetting\nOutput Features\n\n\n\n\ndegree=1\n[x‚ÇÅ, x‚ÇÇ]\n\n\ndegree=2, interaction_only=False\n[x‚ÇÅ, x‚ÇÇ, x‚ÇÅ¬≤, x‚ÇÅx‚ÇÇ, x‚ÇÇ¬≤]\n\n\ndegree=2, interaction_only=True\n[x‚ÇÅ, x‚ÇÇ, x‚ÇÅx‚ÇÇ]\n\n\n\n\n# Generate interaction-only features (no squared terms)\nprint(\"Creating Interaction-Only Features\")\nprint(\"=\" * 45)\n\npoly_inter = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n\n# Fit on training data and transform both sets\nX_train_inter = poly_inter.fit_transform(X_train)\nX_test_inter = poly_inter.transform(X_test)\n\n# Display the transformation\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"After interaction features: {X_train_inter.shape[1]}\")\nprint(f\"Added interaction terms: {X_train_inter.shape[1] - X_train.shape[1]}\")\n\nprint(f\"\\nFeature names:\")\nfeature_names = poly_inter.get_feature_names_out(input_features=X_train.columns)\nfor i, name in enumerate(feature_names):\n    marker = \" (interaction)\" if \" \" in name else \" (original)\"\n    print(f\"  {i+1:2d}. {name}{marker}\")\n\n# Train model with interactions\nmodel_inter = LinearRegression()\nmodel_inter.fit(X_train_inter, y_train)\n\n# Evaluate performance\npred_price_inter = model_inter.predict(X_test_inter)\nrmse_inter = root_mean_squared_error(y_test, pred_price_inter)\nmae_inter = mean_absolute_error(y_test, pred_price_inter)\nr2_inter = r2_score(y_test, pred_price_inter)\n\nprint(f\"\\nüìä Model Performance (Interaction-Only):\")\nprint(\"=\" * 45)\nprint(f\"RMSE: ${rmse_inter:,.2f}\")\nprint(f\"MAE:  ${mae_inter:,.2f}\")\nprint(f\"R¬≤:   {r2_inter:.4f}\")\n\nCreating Interaction-Only Features\n=============================================\nOriginal features: 4\nAfter interaction features: 10\nAdded interaction terms: 6\n\nFeature names:\n   1. mileage (original)\n   2. engineSize (original)\n   3. age (original)\n   4. mpg (original)\n   5. mileage engineSize (interaction)\n   6. mileage age (interaction)\n   7. mileage mpg (interaction)\n   8. engineSize age (interaction)\n   9. engineSize mpg (interaction)\n  10. age mpg (interaction)\n\nüìä Model Performance (Interaction-Only):\n=============================================\nRMSE: $8,729.91\nMAE:  $5,400.07\nR¬≤:   0.7234\n\n\n\n\n\n4.5.2 Full Polynomial Features (Including Squared Terms)\n\n\n4.5.3 Limitations of PolynomialFeatures\n‚ö†Ô∏è Key Limitation: Unlike statsmodels, PolynomialFeatures applies transformations to ALL features:\n\n‚úÖ Advantage: Quick, systematic feature generation\n‚ùå Disadvantage: No selective control over which interactions to include\n‚ùå Risk: Can create too many features with high-dimensional data\n\nTrade-off: Convenience vs.¬†Control\nLet‚Äôs compare interaction-only vs.¬†full polynomial features:\n\n# Generate full polynomial features (includes squared terms)\nprint(\"Creating Full Polynomial Features (degree=2)\")\nprint(\"=\" * 55)\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_train_poly = poly.fit_transform(X_train)\nX_test_poly = poly.transform(X_test)\n\nprint(f\"Original features: {X_train.shape[1]}\")\nprint(f\"After polynomial features: {X_train_poly.shape[1]}\")\nprint(f\"Added features: {X_train_poly.shape[1] - X_train.shape[1]}\")\n\nprint(f\"\\nAll generated features:\")\nfeature_names_poly = poly.get_feature_names_out(input_features=X_train.columns)\nfor i, name in enumerate(feature_names_poly):\n    if \"^2\" in name:\n        marker = \" (squared)\"\n    elif \" \" in name:\n        marker = \" (interaction)\"  \n    else:\n        marker = \" (original)\"\n    print(f\"  {i+1:2d}. {name}{marker}\")\n\nCreating Full Polynomial Features (degree=2)\n=======================================================\nOriginal features: 4\nAfter polynomial features: 14\nAdded features: 10\n\nAll generated features:\n   1. mileage (original)\n   2. engineSize (original)\n   3. age (original)\n   4. mpg (original)\n   5. mileage^2 (squared)\n   6. mileage engineSize (interaction)\n   7. mileage age (interaction)\n   8. mileage mpg (interaction)\n   9. engineSize^2 (squared)\n  10. engineSize age (interaction)\n  11. engineSize mpg (interaction)\n  12. age^2 (squared)\n  13. age mpg (interaction)\n  14. mpg^2 (squared)\n\n\n\n# Train model with full polynomial features\nprint(\"Training Full Polynomial Model\")\nprint(\"=\" * 40)\n\nmodel_poly = LinearRegression()\nmodel_poly.fit(X_train_poly, y_train)\n\n# Show the most important coefficients\ncoeffs = model_poly.coef_\nfeature_importance = pd.DataFrame({\n    'Feature': feature_names_poly,\n    'Coefficient': coeffs,\n    'Abs_Coefficient': np.abs(coeffs)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint(\"Top 10 Most Important Features:\")\ndisplay(feature_importance.head(10))\n\nTraining Full Polynomial Model\n========================================\nTop 10 Most Important Features:\n\n\n\n\n\n\n\n\n\nFeature\nCoefficient\nAbs_Coefficient\n\n\n\n\n1\nengineSize\n22746.56\n22746.56\n\n\n2\nage\n-3022.42\n3022.42\n\n\n9\nengineSize age\n-644.88\n644.88\n\n\n8\nengineSize^2\n-361.71\n361.71\n\n\n10\nengineSize mpg\n-136.77\n136.77\n\n\n11\nage^2\n104.62\n104.62\n\n\n3\nmpg\n39.42\n39.42\n\n\n12\nage mpg\n16.68\n16.68\n\n\n13\nmpg^2\n0.66\n0.66\n\n\n0\nmileage\n-0.10\n0.10\n\n\n\n\n\n\n\n\n# Evaluate and compare polynomial models\npred_poly = model_poly.predict(X_test_poly)\n\n# Calculate metrics for full polynomial model\nrmse_poly = root_mean_squared_error(y_test, pred_poly)\nmae_poly = mean_absolute_error(y_test, pred_poly)\nr2_poly = r2_score(y_test, pred_poly)\n\nprint(\"üìä Model Performance Comparison\")\nprint(\"=\" * 45)\n\n# Cross-validation for full polynomial\ncv_scores_poly = cross_val_score(\n    LinearRegression(), X_train_poly, y_train, \n    cv=5, scoring='neg_root_mean_squared_error'\n)\n\n# Create comprehensive comparison\nresults_df = pd.DataFrame({\n    'Model': [\n        'Baseline (Linear)', \n        'Interaction-Only',\n        'Full Polynomial (degree=2)'\n    ],\n    'Features': [\n        X_train.shape[1],\n        X_train_inter.shape[1], \n        X_train_poly.shape[1]\n    ],\n    'Test_RMSE': [\n        np.nan,  # We'll calculate baseline\n        rmse_inter,\n        rmse_poly\n    ],\n    'Test_MAE': [\n        np.nan,\n        mae_inter, \n        mae_poly\n    ],\n    'Test_R2': [\n        np.nan,\n        r2_inter,\n        r2_poly\n    ]\n\n})\n\n# Calculate baseline model for comparison\nbaseline_model = LinearRegression()\nbaseline_model.fit(X_train, y_train)\nbaseline_pred = baseline_model.predict(X_test)\n\n\n# Update baseline results\nresults_df.loc[0, 'Test_RMSE'] = root_mean_squared_error(y_test, baseline_pred)\nresults_df.loc[0, 'Test_MAE'] = mean_absolute_error(y_test, baseline_pred)\nresults_df.loc[0, 'Test_R2'] = r2_score(y_test, baseline_pred)\n\n\ndisplay(results_df.round(4))\n\nüìä Model Performance Comparison\n=============================================\n\n\n\n\n\n\n\n\n\nModel\nFeatures\nTest_RMSE\nTest_MAE\nTest_R2\n\n\n\n\n0\nBaseline (Linear)\n4\n9956.83\n6407.74\n0.64\n\n\n1\nInteraction-Only\n10\n8729.91\n5400.07\n0.72\n\n\n2\nFull Polynomial (degree=2)\n14\n8896.17\n5335.30\n0.71",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#part-3-categorical-variables-in-scikit-learn",
    "href": "Lec3_VariableTransformations_and_Interactions.html#part-3-categorical-variables-in-scikit-learn",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.6 Part 3: Categorical Variables in Scikit-Learn",
    "text": "4.6 Part 3: Categorical Variables in Scikit-Learn\nScikit-learn models require numerical input. We need to convert categorical variables to numbers.\n\n4.6.1 Two Main Approaches:\n\n\n\n\n\n\n\n\n\nMethod\nPros\nCons\nBest For\n\n\n\n\npd.get_dummies()\nSimple, intuitive\nColumn mismatch risk, no pipeline integration\nQuick prototyping\n\n\nOneHotEncoder\nRobust, handles unseen categories, pipeline-friendly\nSlightly more setup\nProduction code\n\n\n\n\n\n4.6.2 Common Pitfalls:\n‚ùå Data Leakage: Fitting encoder on full dataset\n‚ùå Column Mismatch: Train/test having different categories\n‚ùå Dummy Trap: Including all dummy variables ‚Üí multicollinearity\n\n\n4.6.3 Method 1: Using pd.get_dummies()\n\n4.6.3.1 ‚ö†Ô∏è The Column Alignment Problem\nWhen using get_dummies(), train and test sets might have different categorical values:\n# Train has: ['A', 'B', 'C'] ‚Üí columns: ['B', 'C'] (A is reference)\n# Test has: ['A', 'B', 'D'] ‚Üí columns: ['B', 'D'] (A is reference) \n# Result: Mismatched feature matrices! üö®\nSolution: Use reindex() to align columns.\n\n# Method 1: get_dummies approach\nprint(\"üîß Method 1: Using pd.get_dummies()\")\nprint(\"=\" * 45)\n\n# Create dummy variables (watch for potential mismatched columns)\ntrain_dum = pd.get_dummies(train_updated, columns=['transmission'], drop_first=True, prefix='trans')\ntest_dum = pd.get_dummies(test, columns=['transmission'], drop_first=True, prefix='trans')\n\nprint(\"Original column alignment:\")\nprint(f\"Train dummy columns: {[col for col in train_dum.columns if 'trans_' in col]}\")\nprint(f\"Test dummy columns:  {[col for col in test_dum.columns if 'trans_' in col]}\")\n\n# Critical step: Align test columns to match training columns\ntest_dum = test_dum.reindex(columns=train_dum.columns, fill_value=0)\n\nprint(f\"\\nAfter reindexing:\")\nprint(f\"Train shape: {train_dum.shape}\")\nprint(f\"Test shape:  {test_dum.shape}\")\nprint(f\"Columns match: {list(train_dum.columns) == list(test_dum.columns)}\")\n\n# Prepare feature matrices\nfeature_cols = ['engineSize', 'trans_Manual', 'trans_Semi-Auto']\nX_train_dum = train_dum[feature_cols]\ny_train_dum = train_dum['price']\nX_test_dum = test_dum[feature_cols]\ny_test_dum = test_dum['price']\n\nprint(f\"\\nFinal feature matrix:\")\ndisplay(X_train_dum.head())\n\n# Train and evaluate model\nmodel_dum = LinearRegression()\nmodel_dum.fit(X_train_dum, y_train_dum)\npred_dum = model_dum.predict(X_test_dum)\nrmse_dum = root_mean_squared_error(y_test_dum, pred_dum)\n\nprint(f\"\\nüìä Performance:\")\nprint(f\"RMSE: ${rmse_dum:,.2f}\")\n\n# Show coefficients with interpretation\ncoeffs_dum = pd.DataFrame({\n    'Feature': X_train_dum.columns,\n    'Coefficient': model_dum.coef_\n})\nprint(f\"\\nModel Coefficients:\")\ndisplay(coeffs_dum)\n\nüîß Method 1: Using pd.get_dummies()\n=============================================\nOriginal column alignment:\nTrain dummy columns: ['trans_Manual', 'trans_Semi-Auto']\nTest dummy columns:  ['trans_Manual', 'trans_Other', 'trans_Semi-Auto']\n\nAfter reindexing:\nTrain shape: (4959, 13)\nTest shape:  (2672, 13)\nColumns match: True\n\nFinal feature matrix:\n\n\n\n\n\n\n\n\n\nengineSize\ntrans_Manual\ntrans_Semi-Auto\n\n\n\n\n0\n3.00\nFalse\nTrue\n\n\n1\n3.00\nFalse\nTrue\n\n\n2\n3.00\nFalse\nTrue\n\n\n3\n3.00\nFalse\nTrue\n\n\n4\n3.00\nFalse\nFalse\n\n\n\n\n\n\n\n\nüìä Performance:\nRMSE: $12,192.23\n\nModel Coefficients:\n\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n0\nengineSize\n10226.76\n\n\n1\ntrans_Manual\n-6770.62\n\n\n2\ntrans_Semi-Auto\n4994.31\n\n\n\n\n\n\n\n\n\n\n4.6.4 Method 2: Using OneHotEncoder (Recommended)\n\n4.6.4.1 ‚úÖ Why OneHotEncoder is Better:\n\nNo Data Leakage: Fit only on training data\nHandles Unknown Categories: Use handle_unknown='ignore'\nPipeline Integration: Works seamlessly with sklearn workflows\nConsistent Behavior: Same transformation guaranteed for train/test\n\n\n\n4.6.4.2 Two Approaches:\n\nWith Pipeline (recommended): Clean, maintainable code\nManual: More control, better for understanding\n\n\n# Method 2a: OneHotEncoder with Pipeline (RECOMMENDED)\nprint(\"Method 2a: OneHotEncoder with Pipeline\")\nprint(\"=\" * 50)\n\n# Prepare data (mixing categorical and numerical)\nX_train_mixed = train_updated[['engineSize', 'transmission']].copy()\ny_train_mixed = train_updated['price'].copy()\nX_test_mixed = test[['engineSize', 'transmission']].copy()\ny_test_mixed = test['price'].copy()\n\nprint(\"Input data structure:\")\nprint(f\"Features: {list(X_train_mixed.columns)}\")\nprint(f\"Data types: {X_train_mixed.dtypes.to_dict()}\")\n\n# Create preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), ['transmission']),\n        # Note: numerical features passed through unchanged via remainder='passthrough'\n    ],\n    remainder='passthrough'  # Keep other columns as-is\n)\n\n# Create complete pipeline\npipeline_ohe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\nprint(f\"\\n Pipeline Structure:\")\nprint(f\"Step 1: ColumnTransformer\")\nprint(f\"  - OneHotEncoder for ['transmission']\")\nprint(f\"  - PassThrough for ['engineSize']\")\nprint(f\"Step 2: LinearRegression\")\n\n# Fit pipeline\npipeline_ohe.fit(X_train_mixed, y_train_mixed)\n\n# The beauty of pipelines: preprocessing happens automatically!\npred_ohe_pipeline = pipeline_ohe.predict(X_test_mixed)\nrmse_ohe_pipeline = root_mean_squared_error(y_test_mixed, pred_ohe_pipeline)\n\nprint(f\"\\n Pipeline Performance:\")\nprint(f\"RMSE: ${rmse_ohe_pipeline:,.2f}\")\n\n# Inspect the transformation\nX_transformed = preprocessor.fit_transform(X_train_mixed)\nfeature_names_ohe = (\n    preprocessor.named_transformers_['cat'].get_feature_names_out(['transmission']).tolist() +\n    ['engineSize']  # remainder columns\n)\nprint(f\"\\nTransformed features: {feature_names_ohe}\")\nprint(f\"Transformed shape: {X_transformed.shape}\")\n\ntry:\n    # Display coefficients (if accessible)\n    final_model = pipeline_ohe.named_steps['regressor']\n    coeff_df = pd.DataFrame({\n        'Feature': feature_names_ohe,\n        'Coefficient': final_model.coef_\n    })\n    print(f\"\\nPipeline Model Coefficients:\")\n    display(coeff_df)\nexcept Exception as e:\n    print(f\"Note: Coefficient extraction failed: {e}\")\n\nMethod 2a: OneHotEncoder with Pipeline\n==================================================\nInput data structure:\nFeatures: ['engineSize', 'transmission']\nData types: {'engineSize': dtype('float64'), 'transmission': dtype('O')}\n\n Pipeline Structure:\nStep 1: ColumnTransformer\n  - OneHotEncoder for ['transmission']\n  - PassThrough for ['engineSize']\nStep 2: LinearRegression\n\n Pipeline Performance:\nRMSE: $12,192.23\n\nTransformed features: ['transmission_Manual', 'transmission_Semi-Auto', 'engineSize']\nTransformed shape: (4959, 3)\n\nPipeline Model Coefficients:\n\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n0\ntransmission_Manual\n-6770.62\n\n\n1\ntransmission_Semi-Auto\n4994.31\n\n\n2\nengineSize\n10226.76\n\n\n\n\n\n\n\n\n# Method 2b: OneHotEncoder without Pipeline (Manual Control)\nprint(\"üîß Method 2b: OneHotEncoder Manual Approach\")\nprint(\"=\" * 50)\n\n# Separate numerical and categorical features\nX_train_num = train_updated[['engineSize']].copy()\nX_test_num = test[['engineSize']].copy()\n\nprint(\"Step 1: Initialize and fit OneHotEncoder\")\nohe = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n\n# Fit on training categorical data only (no data leakage!)\nX_train_cat_encoded = ohe.fit_transform(train_updated[['transmission']])\nX_test_cat_encoded = ohe.transform(test[['transmission']])  # Transform using fitted encoder\n\nprint(f\"Categories found during fit: {ohe.categories_}\")\nprint(f\"Feature names: {ohe.get_feature_names_out(['transmission'])}\")\nprint(f\"Encoded categorical shape: {X_train_cat_encoded.shape}\")\n\nprint(\"\\nStep 2: Combine numerical and encoded categorical features\")\n# Stack numerical and categorical features\nX_train_combined = np.column_stack([X_train_num.values, X_train_cat_encoded])\nX_test_combined = np.column_stack([X_test_num.values, X_test_cat_encoded])\n\nprint(f\"Combined feature matrix shape: {X_train_combined.shape}\")\n\n# Create feature names for clarity\nnumerical_features = X_train_num.columns.tolist()\ncategorical_features = ohe.get_feature_names_out(['transmission']).tolist()\nall_feature_names = numerical_features + categorical_features\n\nprint(f\"All features: {all_feature_names}\")\n\nprint(\"\\nStep 3: Train model\")\nmodel_ohe_manual = LinearRegression()\nmodel_ohe_manual.fit(X_train_combined, y_train_mixed)\n\n# Evaluate\npred_ohe_manual = model_ohe_manual.predict(X_test_combined)\nrmse_ohe_manual = root_mean_squared_error(y_test_mixed, pred_ohe_manual)\n\nprint(f\"\\nüìä Manual Approach Performance:\")\nprint(f\"RMSE: ${rmse_ohe_manual:,.2f}\")\n\n# Show coefficients\nmanual_coeffs = pd.DataFrame({\n    'Feature': all_feature_names,\n    'Coefficient': model_ohe_manual.coef_\n})\nprint(f\"\\nManual Model Coefficients:\")\ndisplay(manual_coeffs)\n\n# Verify both approaches give same results\nprint(f\"\\n‚úÖ Verification:\")\nprint(f\"Pipeline RMSE: ${rmse_ohe_pipeline:,.2f}\")\nprint(f\"Manual RMSE:   ${rmse_ohe_manual:,.2f}\")\nprint(f\"Difference:    ${abs(rmse_ohe_pipeline - rmse_ohe_manual):,.6f}\")\nprint(f\"Results match: {np.isclose(rmse_ohe_pipeline, rmse_ohe_manual)}\")\n\nüîß Method 2b: OneHotEncoder Manual Approach\n==================================================\nStep 1: Initialize and fit OneHotEncoder\nCategories found during fit: [array(['Automatic', 'Manual', 'Semi-Auto'], dtype=object)]\nFeature names: ['transmission_Manual' 'transmission_Semi-Auto']\nEncoded categorical shape: (4959, 2)\n\nStep 2: Combine numerical and encoded categorical features\nCombined feature matrix shape: (4959, 3)\nAll features: ['engineSize', 'transmission_Manual', 'transmission_Semi-Auto']\n\nStep 3: Train model\n\nüìä Manual Approach Performance:\nRMSE: $12,192.23\n\nManual Model Coefficients:\n\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n0\nengineSize\n10226.76\n\n\n1\ntransmission_Manual\n-6770.62\n\n\n2\ntransmission_Semi-Auto\n4994.31\n\n\n\n\n\n\n\n\n‚úÖ Verification:\nPipeline RMSE: $12,192.23\nManual RMSE:   $12,192.23\nDifference:    $0.000000\nResults match: True",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "Lec3_VariableTransformations_and_Interactions.html#summary-and-best-practices",
    "href": "Lec3_VariableTransformations_and_Interactions.html#summary-and-best-practices",
    "title": "4¬† Extending LR: Interactions and Qualitative Features",
    "section": "4.7 Summary and Best Practices",
    "text": "4.7 Summary and Best Practices\n\n4.7.1 When to Use Each Approach:\n\n\n\n\n\n\n\n\nSituation\nRecommended Method\nWhy\n\n\n\n\nQuick prototyping\npd.get_dummies()\nFast and simple\n\n\nProduction models\nOneHotEncoder + Pipeline\nRobust, scalable\n\n\nComplex preprocessing\nColumnTransformer\nHandles mixed data types\n\n\nStatistical inference\nstatsmodels\nRich statistical output\n\n\n\n\n\n4.7.2 Best Practices Checklist:\n\n4.7.2.1 Data Preprocessing:\n\n‚úÖ Always fit transformers on training data only\n‚úÖ Handle rare categories before encoding\n\n‚úÖ Use drop='first' or drop_first=True to avoid multicollinearity\n‚úÖ Set handle_unknown='ignore' for robustness\n\n\n\n4.7.2.2 Model Development:\n\n‚úÖ Use cross-validation for model selection\n‚úÖ Compare multiple interaction specifications\n‚úÖ Monitor for overfitting with polynomial features\n‚úÖ Keep pipelines simple and maintainable\n\n\n\n4.7.2.3 Common Pitfalls to Avoid:\n\n‚ùå Data leakage: Fitting on test data\n‚ùå Column mismatch: Forgetting to align train/test features\n\n‚ùå Dummy trap: Including all categorical levels\n‚ùå Overfitting: Adding too many interaction terms without validation",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Extending LR: Interactions and Qualitative Features</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html",
    "href": "polynominal_features.html",
    "title": "5¬† Extending LR: Feature Transformations",
    "section": "",
    "text": "5.1 Learning objectives\nBy the end of this notebook you should be able to:\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n\nsns.set_theme(style=\"whitegrid\", font_scale=1.2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Extending LR: Feature Transformations</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html#learning-objectives",
    "href": "polynominal_features.html#learning-objectives",
    "title": "5¬† Extending LR: Feature Transformations",
    "section": "",
    "text": "Explain why a log transform of the target (using log(price)) can be helpful, and correctly back-transform predictions with exp().\nFit polynomial regression models in statsmodels using the formula API (e.g., quadratic and cubic terms with I(mileage**2) / I(mileage**3)).\nBuild polynomial feature models in scikit-learn with PolynomialFeatures and LinearRegression.\nImplement polynomial feature models using a scikit-learn Pipeline for a clean, reproducible workflow.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Extending LR: Feature Transformations</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html#feature-transformations-in-statsmodels",
    "href": "polynominal_features.html#feature-transformations-in-statsmodels",
    "title": "5¬† Extending LR: Feature Transformations",
    "section": "5.2 Feature transformations in statsmodels",
    "text": "5.2 Feature transformations in statsmodels\n\n5.2.1 Why feature transformations matter\nA linear regression model is linear in its coefficients, not necessarily linear in the original variables. That means we can extend a ‚Äúlinear‚Äù model to capture many real-world patterns by transforming or expanding the predictors.\nFeature transformations are important because they let us:\n\nCapture nonlinearity while staying in the linear-model framework: adding terms like \\(mileage^2\\) or \\(mileage^3\\) can model curved relationships without switching to a completely different model class.\nImprove model assumptions: transforming variables (often the target) can reduce heteroscedasticity (non-constant variance), make residuals closer to normal, and improve linearity of the relationship.\nEnable interpretable extensions: polynomial terms and interactions are still interpretable, especially when you state clearly what was transformed and how predictions are back-transformed.\n\nIn this section, we use the same car dataset throughout for continuity.\n\n# Read data\ntrainf = pd.read_csv('./Datasets/Car_features_train.csv')\ntrainp = pd.read_csv('./Datasets/Car_prices_train.csv')\ntestf = pd.read_csv('./Datasets/Car_features_test.csv')\ntestp = pd.read_csv('./Datasets/Car_prices_test.csv')\n\ntrain = pd.merge(trainf, trainp)\ntest = pd.merge(testf, testp)\n\n# Create age feature using the most recent model year in the training data\nreference_year = train[\"year\"].max()\ntrain[\"age\"] = reference_year - train[\"year\"]\ntest[\"age\"] = reference_year - test[\"year\"]\n\ntrain.head()\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\nage\n\n\n\n\n0\n18473\nbmw\n6 Series\n2020\nSemi-Auto\n11\nDiesel\n145\n53.3282\n3.0\n37980\n0\n\n\n1\n15064\nbmw\n6 Series\n2019\nSemi-Auto\n10813\nDiesel\n145\n53.0430\n3.0\n33980\n1\n\n\n2\n18268\nbmw\n6 Series\n2020\nSemi-Auto\n6\nDiesel\n145\n53.4379\n3.0\n36850\n0\n\n\n3\n18480\nbmw\n6 Series\n2017\nSemi-Auto\n18895\nDiesel\n145\n51.5140\n3.0\n25998\n3\n\n\n4\n18492\nbmw\n6 Series\n2015\nAutomatic\n62953\nDiesel\n160\n51.4903\n3.0\n18990\n5\n\n\n\n\n\n\n\n\ntrain_updated = train[train.transmission != 'Other']\n\nLet‚Äôs see the distribution of the target price\n\n# draw the distribution of the price\nplt.figure(figsize=(10, 6))\nsns.histplot(train_updated['price'], bins=30, kde=True)\nplt.title('Distribution of Car Prices')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nThe price distribution is right-skewed, indicating that car prices have increasing variance at higher price levels. Linear models work best with normally distributed data and generally perform better when this assumption is met.\nBenefits of using log(price) as the dependent variable:\n\nReduces Heteroscedasticity: Car prices often have increasing variance at higher price levels; log transformation stabilizes this variance\nHandles Multiplicative Effects: Captures percentage changes rather than absolute changes, which is more natural for price data\nImproves Model Fit: Often leads to better R¬≤ values and improved residual behavior\nInterpretability: Coefficients represent percentage changes in price, making economic interpretation more intuitive\n\nImportant: When using log-transformed dependent variables, predictions must be transformed back using np.exp() to get actual price predictions.\n\n# R-squared of the model with just mileage (log-transformed price)\n# Log transformation helps with heteroscedasticity and non-linear relationships\nmodel_sm_mileage = smf.ols('np.log(price) ~ mileage', data=train_updated).fit()\nprint(f\"R-squared (log price ~ mileage): {model_sm_mileage.rsquared:.4f}\")\n\n# Compare with original model without log transformation\nmodel_sm_mileage_orig = smf.ols('price ~ mileage', data=train_updated).fit()\nprint(f\"R-squared (price ~ mileage):     {model_sm_mileage_orig.rsquared:.4f}\")\n\nR-squared (log price ~ mileage): 0.3601\nR-squared (price ~ mileage):     0.2293\n\n\nThe model‚Äôs R¬≤ increased from 22% to 36% with the log transformation, but 36% is still relatively low. Let‚Äôs explore how we can improve it further by examining the relationship between log-transformed car price and mileage.\n\n# Relationship between log(price) and mileage\nax = sns.regplot(x=train_updated.mileage, y=np.log(train_updated.price), color='orange', line_kws={'color': 'blue'})\nplt.xlabel('Mileage')\nplt.ylabel('Log(Car Price)')\nax.xaxis.set_major_formatter('{x:,.0f}')\n\n\n\n\n\n\n\n\nThe plot shows a reasonably linear relationship between mileage and log(price), but the R¬≤ of 36% suggests there‚Äôs still substantial unexplained variation. While the log transformation improved the model from 22% to 36%, we can explore polynomial terms to capture any remaining nonlinear patterns.\n\n5.2.1.1 Quadratic transformation\nAdd a squared term for mileage using the I() operator to keep it as a separate predictor.\n\n# Quadratic model with log-transformed price\nmodel_sm_quad = smf.ols('np.log(price) ~ mileage + I(mileage**2)', data=train_updated).fit()\nprint(\"Log-transformed price with quadratic mileage term:\")\nprint(f\"R-squared: {model_sm_quad.rsquared:.4f}\")\nmodel_sm_quad.summary()\n\nLog-transformed price with quadratic mileage term:\nR-squared: 0.3825\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.382\n\n\nModel:\nOLS\nAdj. R-squared:\n0.382\n\n\nMethod:\nLeast Squares\nF-statistic:\n1535.\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n09:30:37\nLog-Likelihood:\n-4007.5\n\n\nNo. Observations:\n4959\nAIC:\n8021.\n\n\nDf Residuals:\n4956\nBIC:\n8040.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.3538\n0.013\n803.018\n0.000\n10.329\n10.379\n\n\nmileage\n-2.452e-05\n6.47e-07\n-37.919\n0.000\n-2.58e-05\n-2.32e-05\n\n\nI(mileage ** 2)\n8.112e-11\n6.06e-12\n13.385\n0.000\n6.92e-11\n9.3e-11\n\n\n\n\n\n\n\n\nOmnibus:\n25.307\nDurbin-Watson:\n0.323\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n26.219\n\n\nSkew:\n-0.154\nProb(JB):\n2.03e-06\n\n\nKurtosis:\n3.178\nCond. No.\n4.81e+09\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 4.81e+09. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n# Visualize the quadratic fit (transform log predictions back to price scale)\npred_price_log = model_sm_quad.predict(train_updated)\npred_price = np.exp(pred_price_log)  # Transform back from log scale\n\nax = sns.scatterplot(x='mileage', y='price', data=train_updated, color='orange', alpha=0.6)\nsns.lineplot(x=train_updated.mileage, y=pred_price, color='blue', linewidth=2)\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nplt.title('Quadratic Model with Log-Transformed Price')\n\nax.yaxis.set_major_formatter('${x:,.0f}')\n\nax.xaxis.set_major_formatter('{x:,.0f}')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.2.1.2 Cubic transformation\nAdd a cubic term for additional flexibility.\n\n# Cubic model with log-transformed price\nmodel_sm_cubic = smf.ols('np.log(price) ~ mileage + I(mileage**2) + I(mileage**3)', data=train_updated).fit()\nprint(\"Log-transformed price with cubic mileage terms:\")\nprint(f\"R-squared: {model_sm_cubic.rsquared:.4f}\")\nmodel_sm_cubic.summary()\n\nLog-transformed price with cubic mileage terms:\nR-squared: 0.3835\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.383\n\n\nModel:\nOLS\nAdj. R-squared:\n0.383\n\n\nMethod:\nLeast Squares\nF-statistic:\n1027.\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n09:31:01\nLog-Likelihood:\n-4003.4\n\n\nNo. Observations:\n4959\nAIC:\n8015.\n\n\nDf Residuals:\n4955\nBIC:\n8041.\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n10.3730\n0.015\n714.209\n0.000\n10.345\n10.402\n\n\nmileage\n-2.705e-05\n1.09e-06\n-24.723\n0.000\n-2.92e-05\n-2.49e-05\n\n\nI(mileage ** 2)\n1.328e-10\n1.9e-11\n6.985\n0.000\n9.55e-11\n1.7e-10\n\n\nI(mileage ** 3)\n-2.22e-16\n7.74e-17\n-2.868\n0.004\n-3.74e-16\n-7.02e-17\n\n\n\n\n\n\n\n\nOmnibus:\n28.207\nDurbin-Watson:\n0.325\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n29.444\n\n\nSkew:\n-0.161\nProb(JB):\n4.04e-07\n\n\nKurtosis:\n3.196\nCond. No.\n7.73e+14\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 7.73e+14. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n# Visualize the cubic fit (transform log predictions back to price scale)\npred_price_log = model_sm_cubic.predict(train_updated)\npred_price = np.exp(pred_price_log)  # Transform back from log scale\n\nax = sns.scatterplot(x='mileage', y='price', data=train_updated, color='orange', alpha=0.6)\nsns.lineplot(x=train_updated.mileage, y=pred_price, color='blue', linewidth=2)\nplt.xlabel('Mileage')\nplt.ylabel('Predicted car price')\nplt.title('Cubic Model with Log-Transformed Price')\n\nax.yaxis.set_major_formatter('${x:,.0f}')\n\nax.xaxis.set_major_formatter('{x:,.0f}')\n\nplt.show()\n\n\n\n\n\n\n\n\nDo polynomial transformations work?\nAdding polynomial terms yields: - Quadratic model: R¬≤ = 38.2% (improvement of 2.2 percentage points) - Cubic model: R¬≤ = 38.3% (improvement of 0.1 percentage points over quadratic)\nInterpretation: The quadratic term provides a modest improvement (~2 percentage points), suggesting some nonlinearity exists in the mileage-price relationship. However, the cubic term adds almost nothing (0.1%), indicating diminishing returns.\nWhile statistically these improvements may be significant, the practical gain is limited. The quadratic transformation is moderately useful but not transformative. The cubic term appears to be overfitting rather than capturing meaningful patterns. For this dataset, the log transformation itself provides the most substantial improvement, and a simple quadratic term may be sufficient if polynomial features are desired.\n\n\n5.2.1.3 Putting interaction and transformation together\nCombine the interaction term with a quadratic mileage term.\n\n# Full model with interactions and log-transformed price\nmodel_sm_full = smf.ols(\n    formula='np.log(price) ~ age*engineSize + mileage + I(mileage**2)',\n    data=train_updated,\n).fit()\n\nprint(\"Full model with log-transformed price:\")\nprint(f\"R-squared: {model_sm_full.rsquared:.4f}\")\nmodel_sm_full.summary()\n\nFull model with log-transformed price:\nR-squared: 0.7993\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.799\n\n\nModel:\nOLS\nAdj. R-squared:\n0.799\n\n\nMethod:\nLeast Squares\nF-statistic:\n3944.\n\n\nDate:\nFri, 23 Jan 2026\nProb (F-statistic):\n0.00\n\n\nTime:\n11:37:33\nLog-Likelihood:\n-1221.1\n\n\nNo. Observations:\n4959\nAIC:\n2454.\n\n\nDf Residuals:\n4953\nBIC:\n2493.\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.4393\n0.019\n496.336\n0.000\n9.402\n9.477\n\n\nage\n-0.1396\n0.005\n-30.210\n0.000\n-0.149\n-0.131\n\n\nengineSize\n0.4634\n0.008\n58.287\n0.000\n0.448\n0.479\n\n\nage:engineSize\n0.0075\n0.002\n4.487\n0.000\n0.004\n0.011\n\n\nmileage\n-8.758e-06\n4.6e-07\n-19.044\n0.000\n-9.66e-06\n-7.86e-06\n\n\nI(mileage ** 2)\n3.237e-11\n3.53e-12\n9.160\n0.000\n2.54e-11\n3.93e-11\n\n\n\n\n\n\n\n\nOmnibus:\n456.087\nDurbin-Watson:\n0.491\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n1602.553\n\n\nSkew:\n0.433\nProb(JB):\n0.00\n\n\nKurtosis:\n5.647\nCond. No.\n1.35e+10\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.35e+10. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n# RMSE on test data (transform log predictions back to price scale)\npred_price_log = model_sm_full.predict(test)\npred_price = np.exp(pred_price_log)  # Transform back from log scale\n\n\nrmse_full = np.sqrt(((test.price - pred_price) ** 2).mean())\n\nprint(f\"RMSE on test data (log-transformed model): ${rmse_full:,.2f}\")\n\n\n\nrmse_log_scale = np.sqrt(((np.log(test.price) - pred_price_log) ** 2).mean())\n# For comparison, calculate RMSE in log scale too\nprint(f\"RMSE in log scale: {rmse_log_scale:.4f}\")\n\nRMSE on test data (log-transformed model): $9,646.67\nRMSE in log scale: 0.3425",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Extending LR: Feature Transformations</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html#polynomial-features-in-scikit-learn-same-car-dataset",
    "href": "polynominal_features.html#polynomial-features-in-scikit-learn-same-car-dataset",
    "title": "5¬† Extending LR: Feature Transformations",
    "section": "5.3 Polynomial Features in scikit-learn (same car dataset)",
    "text": "5.3 Polynomial Features in scikit-learn (same car dataset)\nWe keep the same train/test split and use log(price) to match the statsmodels section.\n\n5.3.1 Handling Zero and Negative Values with Log Transformations\nThe Problem: log(0) is Undefined\nIn practice, Python returns -inf or throws an error when you try to compute log(0). This is problematic when your data contains zero values.\nSolution: log1p() - Best Solution for Most Cases\nUse np.log1p(x) which computes log(1 + x):\n# Instead of:\ny_transformed = np.log(y)  # Fails if y contains 0\n\n# Use:\ny_transformed = np.log1p(y)  # Works even when y = 0\nBack-transformation:\n# To reverse log1p, use expm1:\ny_original = np.expm1(y_transformed)  # Computes exp(x) - 1\nBenefits: - Handles zero values naturally: log1p(0) = log(1) = 0 - More numerically stable for small values - Common in financial and count data\nNote: For this car price dataset, all prices are positive, so we can safely use np.log() directly.\n\n# Create feature matrix and target vector (log price)\nfeature_cols = ['mileage', 'engineSize', 'age', 'mpg']\nX_train = train_updated[feature_cols]\nX_test = test[feature_cols]\ny_train = np.log(train_updated['price'])\ny_test = np.log(test['price'])\ny_train_price = train_updated['price']\ny_test_price = test['price']\n\nX_train.head()\n\n\n\n\n\n\n\n\nmileage\nengineSize\nage\nmpg\n\n\n\n\n0\n11\n3.0\n0\n53.3282\n\n\n1\n10813\n3.0\n1\n53.0430\n\n\n2\n6\n3.0\n0\n53.4379\n\n\n3\n18895\n3.0\n3\n51.5140\n\n\n4\n62953\n3.0\n5\n51.4903\n\n\n\n\n\n\n\n\n\n5.3.2 Train/test split\n\n# We use the provided train/test split from the dataset.\n# Show train/test shapes\nprint(\"Training set shape:\", X_train.shape)\nprint(\"Testing set shape:\", X_test.shape)\n\nTraining set shape: (4000, 2)\nTesting set shape: (1000, 2)\n\n\n\n\n5.3.3 Baseline Model (original Features)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import root_mean_squared_error, r2_score\n\n# Fit a baseline linear regression model on log(price)\nbaseline_model = LinearRegression()\nbaseline_model.fit(X_train, y_train)\n\n# Predict on train and test splits (log scale)\ny_pred_baseline = baseline_model.predict(X_test)\ny_train_pred_baseline = baseline_model.predict(X_train)\n\n# Evaluate on test and training sets (log scale)\nrmse_baseline = root_mean_squared_error(y_test, y_pred_baseline)\nr2_baseline = r2_score(y_test, y_pred_baseline)\n\nrmse_train_baseline = root_mean_squared_error(y_train, y_train_pred_baseline)\nr2_train_baseline = r2_score(y_train, y_train_pred_baseline)\n\n# Back-transform to price scale (see log-trap note above)\npred_price_baseline = np.exp(y_pred_baseline)\nrmse_price_baseline = root_mean_squared_error(y_test_price, pred_price_baseline)\n\nprint()\nprint(\"Baseline Model Performance (log scale):\")\nprint(\"Training Set:\")\nprint(\"RMSE:\", rmse_train_baseline)\nprint(\"R2:\", r2_train_baseline)\nprint()\nprint(\"Testing Set:\")\nprint(\"RMSE:\", rmse_baseline)\nprint(\"R2:\", r2_baseline)\nprint(\"RMSE (price scale):\", f\"${rmse_price_baseline:,.2f}\")\n\n\nBaseline Model Performance (log scale):\nTraining Set:\nRMSE: 0.3126761090785014\nR2: 0.7951679756008143\n\nTesting Set:\nRMSE: 0.34009843313619664\nR2: 0.759217125194597\nRMSE (price scale): $9,802.82\n\n\n\n\n5.3.4 Transform Features with PolynomialFeatures (degree = 2)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Create PolynomialFeatures object with degree=2 (includes interaction terms)\npoly_2 = PolynomialFeatures(degree=2, include_bias=False)\n\n# Transform the training and testing features\nX_train_poly_2 = poly_2.fit_transform(X_train)\nX_test_poly_2 = poly_2.transform(X_test)\n\n# Display the transformed feature names\nprint()\nprint(\"Transformed Feature Names:\")\nprint(poly_2.get_feature_names_out())\n\n\nTransformed Feature Names:\n['mileage' 'engineSize' 'age' 'mpg' 'mileage^2' 'mileage engineSize'\n 'mileage age' 'mileage mpg' 'engineSize^2' 'engineSize age'\n 'engineSize mpg' 'age^2' 'age mpg' 'mpg^2']\n\n\n\n5.3.4.1 Linear Model with transformed Features (degree = 2)\n\n# Fit a linear regression model on degree-2 polynomial features (log price)\npoly_2_model = LinearRegression()\npoly_2_model.fit(X_train_poly_2, y_train)\n\n# Predict on train and test splits (log scale)\ny_pred_poly_2 = poly_2_model.predict(X_test_poly_2)\ny_train_pred_poly_2 = poly_2_model.predict(X_train_poly_2)\n\n# Evaluate on test and training sets (log scale)\nrmse_poly_2 = root_mean_squared_error(y_test, y_pred_poly_2)\nr2_poly_2 = r2_score(y_test, y_pred_poly_2)\n\nrmse_train_poly_2 = root_mean_squared_error(y_train, y_train_pred_poly_2)\nr2_train_poly_2 = r2_score(y_train, y_train_pred_poly_2)\n\n# Back-transform to price scale (see log-trap note above)\npred_price_poly_2 = np.exp(y_pred_poly_2)\nrmse_price_poly_2 = root_mean_squared_error(y_test_price, pred_price_poly_2)\n\nprint()\nprint(\"Polynomial Model Performance (degree=2, log scale):\")\nprint(\"Training Set:\")\nprint(\"RMSE:\", rmse_train_poly_2)\nprint(\"R2:\", r2_train_poly_2)\nprint()\nprint(\"Testing Set:\")\nprint(\"RMSE:\", rmse_poly_2)\nprint(\"R2:\", r2_poly_2)\nprint(\"RMSE (price scale):\", f\"${rmse_price_poly_2:,.2f}\")\n\n\nPolynomial Model Performance (degree=2, log scale):\nTraining Set:\nRMSE: 0.3002407156431113\nR2: 0.8111366745155597\n\nTesting Set:\nRMSE: 0.30894008525487293\nR2: 0.8013151022053762\nRMSE (price scale): $8,479.10\n\n\n\n\n\n5.3.5 Transform features with PolynomialFeatures (degree = 3)\n\n# Create PolynomialFeatures object with degree=3\npoly_3 = PolynomialFeatures(degree=3, include_bias=False)\n\n# Transform the training and testing features\nX_train_poly_3 = poly_3.fit_transform(X_train)\nX_test_poly_3 = poly_3.transform(X_test)\n\n# Display the transformed feature names\nprint()\nprint(\"Transformed Feature Names:\")\nprint(poly_3.get_feature_names_out())\n\n\nTransformed Feature Names:\n['mileage' 'engineSize' 'age' 'mpg' 'mileage^2' 'mileage engineSize'\n 'mileage age' 'mileage mpg' 'engineSize^2' 'engineSize age'\n 'engineSize mpg' 'age^2' 'age mpg' 'mpg^2' 'mileage^3'\n 'mileage^2 engineSize' 'mileage^2 age' 'mileage^2 mpg'\n 'mileage engineSize^2' 'mileage engineSize age' 'mileage engineSize mpg'\n 'mileage age^2' 'mileage age mpg' 'mileage mpg^2' 'engineSize^3'\n 'engineSize^2 age' 'engineSize^2 mpg' 'engineSize age^2'\n 'engineSize age mpg' 'engineSize mpg^2' 'age^3' 'age^2 mpg' 'age mpg^2'\n 'mpg^3']\n\n\n\n5.3.5.1 Linear Model with transformed Features (degree = 3)\n\n# Fit a linear regression model on degree-3 polynomial features (log price)\npoly_3_model = LinearRegression()\npoly_3_model.fit(X_train_poly_3, y_train)\n\n# Predict on train and test splits (log scale)\ny_pred_poly_3 = poly_3_model.predict(X_test_poly_3)\ny_pred_train_poly_3 = poly_3_model.predict(X_train_poly_3)\n\n# Evaluate on test and training sets (log scale)\nrmse_poly_3 = root_mean_squared_error(y_test, y_pred_poly_3)\nr2_poly_3 = r2_score(y_test, y_pred_poly_3)\n\nrmse_poly_3_train = root_mean_squared_error(y_train, y_pred_train_poly_3)\nr2_poly_3_train = r2_score(y_train, y_pred_train_poly_3)\n\n# Back-transform to price scale (see log-trap note above)\npred_price_poly_3 = np.exp(y_pred_poly_3)\nrmse_price_poly_3 = root_mean_squared_error(y_test_price, pred_price_poly_3)\n\nprint()\nprint(\"Polynomial Model Performance (degree=3, log scale):\")\nprint(\"Training Set:\")\nprint(\"RMSE:\", rmse_poly_3_train)\nprint(\"R2:\", r2_poly_3_train)\nprint()\nprint(\"Testing Set:\")\nprint(\"RMSE:\", rmse_poly_3)\nprint(\"R2:\", r2_poly_3)\nprint(\"RMSE (price scale):\", f\"${rmse_price_poly_3:,.2f}\")\n\n\nPolynomial Model Performance (degree=3, log scale):\nTraining Set:\nRMSE: 0.3811542627298689\nR2: 0.695624268973846\n\nTesting Set:\nRMSE: 0.3804705656720708\nR2: 0.6986588360960538\nRMSE (price scale): $12,105.56\n\n\n\n\n\n5.3.6 degree = 4\n\n# Use polynomial degree of 4 to see if it improves the model\npoly_4 = PolynomialFeatures(degree=4, include_bias=False)\n\n# Transform the training and testing features\nX_train_poly_4 = poly_4.fit_transform(X_train)\nX_test_poly_4 = poly_4.transform(X_test)\n\n# Create a linear regression model for the polynomial features\npoly_4_model = LinearRegression()\n\n# Train the model on the transformed features\npoly_4_model.fit(X_train_poly_4, y_train)\n\n# Make predictions on the test and training sets (log scale)\ny_pred_poly_4 = poly_4_model.predict(X_test_poly_4)\ny_pred_train_poly_4 = poly_4_model.predict(X_train_poly_4)\n\n# Evaluate the polynomial model (log scale)\nrmse_poly_4 = root_mean_squared_error(y_test, y_pred_poly_4)\nr2_poly_4 = r2_score(y_test, y_pred_poly_4)\n\nrmse_poly_4_train = root_mean_squared_error(y_train, y_pred_train_poly_4)\nr2_poly_4_train = r2_score(y_train, y_pred_train_poly_4)\n\n# Back-transform to price scale (see log-trap note above)\npred_price_poly_4 = np.exp(y_pred_poly_4)\nrmse_price_poly_4 = root_mean_squared_error(y_test_price, pred_price_poly_4)\n\nprint()\nprint(\"Polynomial Model Performance (degree=4, log scale):\")\nprint(\"Training Set:\")\nprint(\"RMSE:\", rmse_poly_4_train)\nprint(\"R2:\", r2_poly_4_train)\nprint()\nprint(\"Testing Set:\")\nprint(\"RMSE:\", rmse_poly_4)\nprint(\"R2:\", r2_poly_4)\nprint(\"RMSE (price scale):\", f\"${rmse_price_poly_4:,.2f}\")\n\n\nPolynomial Model Performance (degree=4, log scale):\nTraining Set:\nRMSE: 0.4825146268287559\nR2: 0.5122138946437124\n\nTesting Set:\nRMSE: 0.48497471912881407\nR2: 0.5103852040335006\nRMSE (price scale): $14,132.89\n\n\n\n# get the feature names for the polynomial degree 4 model\nprint(\"\\nNumber of Features:\", len(poly_4.get_feature_names_out()))\nprint(\"\\nTransformed Feature Names:\")\nprint(poly_4.get_feature_names_out())\n\n\nNumber of Features: 69\n\nTransformed Feature Names:\n['mileage' 'engineSize' 'age' 'mpg' 'mileage^2' 'mileage engineSize'\n 'mileage age' 'mileage mpg' 'engineSize^2' 'engineSize age'\n 'engineSize mpg' 'age^2' 'age mpg' 'mpg^2' 'mileage^3'\n 'mileage^2 engineSize' 'mileage^2 age' 'mileage^2 mpg'\n 'mileage engineSize^2' 'mileage engineSize age' 'mileage engineSize mpg'\n 'mileage age^2' 'mileage age mpg' 'mileage mpg^2' 'engineSize^3'\n 'engineSize^2 age' 'engineSize^2 mpg' 'engineSize age^2'\n 'engineSize age mpg' 'engineSize mpg^2' 'age^3' 'age^2 mpg' 'age mpg^2'\n 'mpg^3' 'mileage^4' 'mileage^3 engineSize' 'mileage^3 age'\n 'mileage^3 mpg' 'mileage^2 engineSize^2' 'mileage^2 engineSize age'\n 'mileage^2 engineSize mpg' 'mileage^2 age^2' 'mileage^2 age mpg'\n 'mileage^2 mpg^2' 'mileage engineSize^3' 'mileage engineSize^2 age'\n 'mileage engineSize^2 mpg' 'mileage engineSize age^2'\n 'mileage engineSize age mpg' 'mileage engineSize mpg^2' 'mileage age^3'\n 'mileage age^2 mpg' 'mileage age mpg^2' 'mileage mpg^3' 'engineSize^4'\n 'engineSize^3 age' 'engineSize^3 mpg' 'engineSize^2 age^2'\n 'engineSize^2 age mpg' 'engineSize^2 mpg^2' 'engineSize age^3'\n 'engineSize age^2 mpg' 'engineSize age mpg^2' 'engineSize mpg^3' 'age^4'\n 'age^3 mpg' 'age^2 mpg^2' 'age mpg^3' 'mpg^4']\n\n\n\n\n5.3.7 Putting all together\n\n# Summarize model performance side by side\nmodels = ['Baseline', 'Polynomial Degree 2', 'Polynomial Degree 3', 'Polynomial Degree 4']\nfeatures = [\n    X_train.shape[1],\n    len(poly_2.get_feature_names_out()),\n    len(poly_3.get_feature_names_out()),\n    len(poly_4.get_feature_names_out()),\n]\ntraining_rmse = [rmse_train_baseline, rmse_train_poly_2, rmse_poly_3_train, rmse_poly_4_train]\ntesting_rmse = [rmse_baseline, rmse_poly_2, rmse_poly_3, rmse_poly_4]\ntesting_rmse_price = [rmse_price_baseline, rmse_price_poly_2, rmse_price_poly_3, rmse_price_poly_4]\ntraining_r2 = [r2_train_baseline, r2_train_poly_2, r2_poly_3_train, r2_poly_4_train]\ntesting_r2 = [r2_baseline, r2_poly_2, r2_poly_3, r2_poly_4]\n\nmodel_comparison = pd.DataFrame(\n    {\n        'Model': models,\n        'Features': features,\n        'Training RMSE (log)': training_rmse,\n        'Testing RMSE (log)': testing_rmse,\n        'Testing RMSE (price)': testing_rmse_price,\n        'Training R2': training_r2,\n        'Testing R2': testing_r2,\n    }\n)\n\nmodel_comparison\n\n\n\n\n\n\n\n\nModel\nFeatures\nTraining RMSE (log)\nTesting RMSE (log)\nTesting RMSE (price)\nTraining R2\nTesting R2\n\n\n\n\n0\nBaseline\n4\n0.312676\n0.340098\n9802.816771\n0.795168\n0.759217\n\n\n1\nPolynomial Degree 2\n14\n0.300241\n0.308940\n8479.095395\n0.811137\n0.801315\n\n\n2\nPolynomial Degree 3\n34\n0.381154\n0.380471\n12105.564864\n0.695624\n0.698659\n\n\n3\nPolynomial Degree 4\n69\n0.482515\n0.484975\n14132.893458\n0.512214\n0.510385\n\n\n\n\n\n\n\nWhen the polynomial degree increases, training error typically decreases because the model becomes more flexible. However, test performance may stop improving‚Äîor even get worse‚Äîwhich indicates overfitting. In our results, degree 3 and 4 appear to overfit, while degree 2 provides the best generalization.\nNext, we‚Äôll implement the degree-2 model using a scikit-learn Pipeline.\n\n# Pipeline for the best model (degree=2)\nfrom sklearn.pipeline import Pipeline\n\npoly2_pipeline = Pipeline(\n    steps=[\n        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n        ('model', LinearRegression()),\n    ]\n)\n\npoly2_pipeline.fit(X_train, y_train)\npred_log_pipe = poly2_pipeline.predict(X_test)\nrmse_log_pipe = root_mean_squared_error(y_test, pred_log_pipe)\nrmse_price_pipe = root_mean_squared_error(y_test_price, np.exp(pred_log_pipe))\n\nprint(\"Pipeline (degree=2) RMSE (log):\", rmse_log_pipe)\nprint(\"Pipeline (degree=2) RMSE (price):\", f\"${rmse_price_pipe:,.2f}\")\n\nPipeline (degree=2) RMSE (log): 0.30894008525487293\nPipeline (degree=2) RMSE (price): $8,479.10\n\n\n\n\n5.3.8 Key takeaway:\nIn scikit-learn, the built-in PolynomialFeatures transformer is somewhat ‚Äúall or nothing‚Äù: by default, it generates all polynomial terms (including interactions) up to a certain degree. You can toggle:\n\ninteraction_only=True to generate only cross-terms\ninclude_bias=False to exclude the constant (bias) term,\ndegree to control how high the polynomial powers go.\n\nHowever, if you want fine-grained control over exactly which terms get generated (for example, only certain interaction terms, or only a subset of polynomial terms), you will need to create those features manually or write a custom transformer (skipped for beginner level)\nAnd also when using PolynomialFeatures (or any other scikit-learn transformer), the fitting step is always done on the training data‚Äînot on the test data. This is a fundamental principle of machine learning pipelines: we do not use the test set for any part of model training (including feature encoding, feature generation, scaling, etc.).",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Extending LR: Feature Transformations</span>"
    ]
  },
  {
    "objectID": "polynominal_features.html#summary",
    "href": "polynominal_features.html#summary",
    "title": "5¬† Extending LR: Feature Transformations",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nFeature transformations extend the capabilities of linear regression by capturing nonlinear relationships while maintaining the interpretability and computational efficiency of linear models.\nKey Takeaways:\n\nExploratory Data Analysis First: Use EDA to examine relationships between features and the target variable. Visualizations help identify nonlinear patterns that suggest transformation needs.\nLog Transformation Benefits:\n\nApplies to both the target variable and predictors\nReduces heteroscedasticity (non-constant variance)\nHandles multiplicative effects and percentage changes\nImproves model fit for skewed distributions\n\nPolynomial Features:\n\nCommon approach for capturing nonlinear relationships\nHigher degrees increase model flexibility\nCan model curved patterns (quadratic, cubic, etc.)\n\nBias-Variance Trade-off:\n\nAdding predictors/features typically decreases bias (underfitting)\nBut increases variance (risk of overfitting)\nMore complex models fit training data better but may generalize poorly\n\nModel Evaluation is Critical:\n\nAlways compare training and test performance\nLarge gaps indicate overfitting\nUse test metrics to guide model selection\n\n\nBest Practices: - Start simple, add complexity gradually - Monitor both training and test performance - Choose models that generalize well over those that merely fit training data",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Extending LR: Feature Transformations</span>"
    ]
  },
  {
    "objectID": "potential_issue_3.html",
    "href": "potential_issue_3.html",
    "title": "6¬† Beyond Fit (implementation)",
    "section": "",
    "text": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\n# Load the Boston Housing dataset (for demonstration purposes)\ndf = pd.read_csv('datasets/Housing.csv')\ndf.head()\n\n\n\n\n\n\n\n\nprice\narea\nbedrooms\nbathrooms\nstories\nmainroad\nguestroom\nbasement\nhotwaterheating\nairconditioning\nparking\nprefarea\nfurnishingstatus\n\n\n\n\n0\n13300000\n7420\n4\n2\n3\nyes\nno\nno\nno\nyes\n2\nyes\nfurnished\n\n\n1\n12250000\n8960\n4\n4\n4\nyes\nno\nno\nno\nyes\n3\nno\nfurnished\n\n\n2\n12250000\n9960\n3\n2\n2\nyes\nno\nyes\nno\nno\n2\nyes\nsemi-furnished\n\n\n3\n12215000\n7500\n4\n2\n2\nyes\nno\nyes\nno\nyes\n3\nyes\nfurnished\n\n\n4\n11410000\n7420\n4\n1\n2\nyes\nyes\nyes\nno\nyes\n2\nno\nfurnished\n\n\n\n\n\n\n\n\n# build a formular api model using price as the target, the rest of the variables as predictors\nmodel = smf.ols('price ~ area + bedrooms + bathrooms + stories + mainroad + guestroom + basement + hotwaterheating + airconditioning + parking + prefarea + furnishingstatus', data=df)\nmodel = model.fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.682\nModel:                            OLS   Adj. R-squared:                  0.674\nMethod:                 Least Squares   F-statistic:                     87.52\nDate:                Wed, 05 Feb 2025   Prob (F-statistic):          9.07e-123\nTime:                        08:46:02   Log-Likelihood:                -8331.5\nNo. Observations:                 545   AIC:                         1.669e+04\nDf Residuals:                     531   BIC:                         1.675e+04\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n======================================================================================================\n                                         coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------\nIntercept                           4.277e+04   2.64e+05      0.162      0.872   -4.76e+05    5.62e+05\nmainroad[T.yes]                     4.213e+05   1.42e+05      2.962      0.003    1.42e+05    7.01e+05\nguestroom[T.yes]                    3.005e+05   1.32e+05      2.282      0.023    4.18e+04    5.59e+05\nbasement[T.yes]                     3.501e+05    1.1e+05      3.175      0.002    1.33e+05    5.67e+05\nhotwaterheating[T.yes]              8.554e+05   2.23e+05      3.833      0.000    4.17e+05    1.29e+06\nairconditioning[T.yes]               8.65e+05   1.08e+05      7.983      0.000    6.52e+05    1.08e+06\nprefarea[T.yes]                     6.515e+05   1.16e+05      5.632      0.000    4.24e+05    8.79e+05\nfurnishingstatus[T.semi-furnished] -4.634e+04   1.17e+05     -0.398      0.691   -2.75e+05    1.83e+05\nfurnishingstatus[T.unfurnished]    -4.112e+05   1.26e+05     -3.258      0.001   -6.59e+05   -1.63e+05\narea                                 244.1394     24.289     10.052      0.000     196.425     291.853\nbedrooms                            1.148e+05   7.26e+04      1.581      0.114   -2.78e+04    2.57e+05\nbathrooms                           9.877e+05   1.03e+05      9.555      0.000    7.85e+05    1.19e+06\nstories                             4.508e+05   6.42e+04      7.026      0.000    3.25e+05    5.77e+05\nparking                             2.771e+05   5.85e+04      4.735      0.000    1.62e+05    3.92e+05\n==============================================================================\nOmnibus:                       97.909   Durbin-Watson:                   1.209\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              258.281\nSkew:                           0.895   Prob(JB):                     8.22e-57\nKurtosis:                       5.859   Cond. No.                     3.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using studentized residuals)\n# ------------------------------------------\n# Outliers can be detected using studentized residuals\noutliers_studentized = model.get_influence().resid_studentized_external\noutlier_threshold = 3  # Common threshold for studentized residuals\n\n\n# Plot studentized residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(outliers_studentized)), outliers_studentized, alpha=0.7)\nplt.axhline(y=outlier_threshold, color='r', linestyle='--', label='Outlier Threshold')\nplt.axhline(y=-outlier_threshold, color='r', linestyle='--')\nplt.title('Studentized Residuals for Outlier Detection')\nplt.xlabel('Observation Index')\nplt.ylabel('Studentized Residuals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Identify observations with high studentized residuals\noutlier_indices_studentized = np.where(np.abs(outliers_studentized) &gt; outlier_threshold)[0]\nprint(f\"Outliers detected at indices: {outlier_indices_studentized}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using standardized residuals)\n# ------------------------------------------\n# Outliers can be detected using standardized  residuals\noutliers_standardized = model.get_influence().resid_studentized_internal\noutlier_threshold = 3  # Common threshold for standardized residuals\n\n\n# Identify observations with high standardized residuals\noutlier_indices_standardized = np.where(np.abs(outliers_standardized) &gt; outlier_threshold)[0]\nprint(f\"Outliers detected at indices: {outlier_indices_standardized}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# Plot studentized residuals\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(outliers_standardized)), outliers_standardized, alpha=0.7)\nplt.axhline(y=outlier_threshold, color='r', linestyle='--', label='Outlier Threshold')\nplt.axhline(y=-outlier_threshold, color='r', linestyle='--')\nplt.title('standardized Residuals for Outlier Detection')\nplt.xlabel('Observation Index')\nplt.ylabel('standardized Residuals')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# ------------------------------------------\n# 1. Identifying Outliers (using boxplot)\n# ------------------------------------------\n# Outliers can be detected using boxplot of standardized residuals\nplt.figure(figsize=(10, 6))\nsns.boxplot(model.resid)\nplt.title('Boxplot of Standardized Residuals');\n\n\n\n\n\n\n\n\n\n# use 3 standard deviation rule to identify outliers\noutlier_indices = np.where(np.abs(model.resid) &gt; 3 * model.resid.std())[0]\nprint(f\"Outliers detected at indices: {outlier_indices}\")\n\nOutliers detected at indices: [ 0  2  3  4 13 15 20 27]\n\n\n\n# ------------------------------------------\n# 2. Identifying High Leverage Points\n# ------------------------------------------\n# High leverage points can be detected using the hat matrix (leverage values)\nleverage = model.get_influence().hat_matrix_diag\nleverage_threshold = 2 * (df.shape[1] / df.shape[0])  # Common threshold for leverage\n\n\n6.0.0.1 Identifying High Leverage Points\nA common threshold for identifying high leverage points in regression analysis is:\n\\(h_i &gt; \\frac{2p}{n}\\)\nwhere:\n- \\(h_i\\) is the leverage value for the ( i )-th observation,\n- \\(p\\) is the number of predictors (including the intercept), and\n- \\(n\\) is the total number of observations.\n\n# Plot leverage values\nplt.figure(figsize=(10, 6))\nplt.scatter(range(len(leverage)), leverage, alpha=0.7)\nplt.axhline(y=leverage_threshold, color='r', linestyle='--', label='Leverage Threshold')\nplt.title('Leverage Values for High Leverage Points')\nplt.xlabel('Observation Index')\nplt.ylabel('Leverage')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Identify observations with high leverage\nhigh_leverage_indices = np.where(leverage &gt; leverage_threshold)[0]\nprint(f\"High leverage points detected at indices: {high_leverage_indices}\")\n\nHigh leverage points detected at indices: [  1   5   7  11  13  20  28  36  66  73  74  75  80  84  89 109 112 125\n 143 165 196 247 270 298 321 334 350 356 363 364 378 395 403 464 490 499\n 530]\n\n\n\n# ------------------------------------------\n# 3. Cook's Distance for Influential Observations\n# ------------------------------------------\n# Cook's distance measures the influence of each observation on the model\ncooks_distance = model.get_influence().cooks_distance[0]\n\n# Plot Cook's distance\nplt.figure(figsize=(10, 6))\nplt.stem(range(len(cooks_distance)), cooks_distance, markerfmt=\",\")\nplt.title(\"Cook's Distance for Influential Observations\")\nplt.xlabel('Observation Index')\nplt.ylabel(\"Cook's Distance\")\nplt.show()\n\n\n\n\n\n\n\n\nCook‚Äôs distance is considered high if it is greater than 0.5 and extreme if it is greater than 1.\n\n# Identify influential observations\ninfluential_threshold = 4 / (df.shape[1] - 1 ) # Common threshold for Cook's distance\ninfluential_indices = np.where(cooks_distance &gt; influential_threshold)[0]\nprint(f\"Influential observations detected at indices: {influential_indices}\")\n\nInfluential observations detected at indices: []\n\n\n\n# =======================================\n# 4. Checking Multicollinearity (VIF)\n# =======================================\n# VIF calculation\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif(X):\n    vif_data = pd.DataFrame()\n    vif_data[\"Variable\"] = X.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return vif_data\n\nX = df[['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'furnishingstatus']]\n\n# one-hot encoding for categorical variables\nX = pd.get_dummies(X, drop_first=True, dtype=float)\n\n\nvif_data = calculate_vif(X)\nprint(\"\\nVariance Inflation Factors:\")\nprint(vif_data.sort_values('VIF', ascending=False))\n\n\nVariance Inflation Factors:\n                           Variable        VIF\n1                          bedrooms  16.652387\n2                         bathrooms   9.417643\n0                              area   8.276447\n3                           stories   7.880730\n5                      mainroad_yes   6.884806\n11  furnishingstatus_semi-furnished   2.386831\n7                      basement_yes   2.019858\n12     furnishingstatus_unfurnished   2.008632\n4                           parking   1.986400\n9               airconditioning_yes   1.767753\n10                     prefarea_yes   1.494211\n6                     guestroom_yes   1.473234\n8               hotwaterheating_yes   1.091568\n\n\n\n# Rule of thumb: VIF &gt; 10 indicates significant multicollinearity\nmulticollinear_features = vif_data[vif_data['VIF'] &gt; 10]['Variable']\nprint(f\"Features with significant multicollinearity: {multicollinear_features.tolist()}\")\n\nFeatures with significant multicollinearity: ['bedrooms']\n\n\n\n# =======================================\n# 4. Checking Multicollinearity (Correlation Matrix)\n# =======================================\n# Correlation matrix\ncorrelation_matrix = X.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix');\n\n\n\n\n\n\n\n\n\n#output the correlation of other predictors with the bedrooms\nX.corr()['bedrooms'].abs().sort_values(ascending=False)\n\nbedrooms                           1.000000\nstories                            0.408564\nbathrooms                          0.373930\nairconditioning_yes                0.160603\narea                               0.151858\nparking                            0.139270\nfurnishingstatus_unfurnished       0.126252\nbasement_yes                       0.097312\nguestroom_yes                      0.080549\nprefarea_yes                       0.079023\nfurnishingstatus_semi-furnished    0.050040\nhotwaterheating_yes                0.046049\nmainroad_yes                       0.012033\nName: bedrooms, dtype: float64\n\n\n\n# ------------------------------------------\n# 5. Analyzing Residual Patterns\n# ------------------------------------------\n# Residuals vs Fitted Values Plot\nfitted_values = model.fittedvalues\nresiduals = model.resid\n\nplt.figure(figsize=(10, 6))\nsns.residplot(x=fitted_values, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 1})\nplt.title('Residuals vs Fitted Values')\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals');\n\n# QQ Plot for Normality of Residuals\nplt.figure(figsize=(10, 6))\nsm.qqplot(residuals, line='s')\nplt.title('QQ Plot of Residuals');\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 5))\nsns.histplot(residuals, kde=True, bins=20)\nplt.axvline(residuals.mean(), color='red', linestyle='--', label=\"Mean Residual\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Density\")\nplt.title(\"Histogram of Residuals\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Beyond Fit (implementation)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html",
    "href": "Lec5_Potential_issues.html",
    "title": "7¬† Beyond Fit (statistical theory)",
    "section": "",
    "text": "7.1 Outliers\nRead section 3.3.3 (4, 5, & 6) of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLet us continue with the car price prediction example from the previous chapter.\nAn outlier is a point for which the true response (\\(y_i\\)) is far from the value predicted by the model. Residual plots can be used to identify outliers.\nIf the the response at the \\(i^{th}\\) observation is \\(y_i\\), the prediction is \\(\\hat{y}_i\\), then the residual \\(e_i\\) is:\n\\[e_i = y_i - \\hat{y_i}\\]\n#Plotting residuals vs fitted values\nsns.set(rc={'figure.figsize':(10,6)})\nsns.scatterplot(x = (model_log.fittedvalues), y=(model_log.resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals');\nSome of the errors may be high. However, it is difficult to decide how large a residual needs to be before we can consider a point to be an outlier. To address this problem, we have standardized residuals, which are defined as:\n\\[r_i = \\frac{e_i}{RSE(\\sqrt{1-h_{ii}})},\\] where \\(r_i\\) is the standardized residual, \\(RSE\\) is the residual standard error, and \\(h_{ii}\\) is the leverage (introduced in the next section) of the \\(i^{th}\\) observation.\nStandardized residuals, allow the residuals to be compared on a standard scale.\nIssue with standardized residuals:, If the observation corresponding to the standardized residual has a high leverage, then it will drag the regression line / plane / hyperplane towards it, thereby influencing the estimate of the residual itself.\nStudentized residuals: To address the issue with standardized residuals, studentized residual for the \\(i^{th}\\) observation is computed as the standardized residual, but with the \\(RSE\\) (residual standard error) computed after removing the \\(i^{th}\\) observation from the data. Studentized residual, \\(t_i\\) for the \\(i^{th}\\) observation is given as:\n\\[t_i = \\frac{e_i}{RSE_{i}(\\sqrt{1-h_{ii}})},\\] where \\(RSE_{i}\\) is the residual standard error of the model developed on the data without the \\(i^{th}\\) observation.\nDistribution of studentized residuals: If the regression model is appropriate such that no case is outlying because of a change in the model, then each studentized residual will follow a \\(t\\) distribution with \\((n‚Äìp‚Äì1)\\) degrees of freedom.\nAs the studentized residuals follow a \\(t\\) distribution, we can conduct a hypothesis test to identify whether an observation is an outlier or not for a given sigificance level. Note that the test will be two-sided since we are not concerned with the sign of the residuals, but only their absolute values.\nIn the current example, for a signficance level of 5%, the critical \\(t\\)-statistic is \\(t\\big(1 - \\frac{\\alpha}{2}, n - p - 1\\big)\\), as calculated below.\nn = train.shape[0]\np = model_log.df_model\nalpha = 0.05\n\n# Critical value\nstats.t.ppf(1 - alpha/2, n - p - 1)\n\n1.9604435402730618\nIf we were conducting the test for a single observation, we‚Äôll compare the studentized residual for that observation with the critical \\(t\\)-statistic, and if the residual is greater than the critical value, we‚Äôll consider that observation as an outlier.\nHowever, typically, we‚Äôll be interested in conducting this test for all observations, and thus we‚Äôll need a more conservative critical value for the same signficance level. This critical value is given by the Bonferroni correction as \\(t\\big(1 - \\frac{\\alpha}{2n}, n - p - 1\\big)\\).\nThus, the minimum value of studentized residual for which the observation will be classified as an outlier is:\ncritical_value = stats.t.ppf(1-alpha/(2*n), n - p - 1)\ncritical_value\n\n4.4200129981725365\nThe studentized residuals can be obtained using the outlier_test() method of the object returned by the fit() method of an OLS object. Let us find the studentized residuals in our car price prediction model.\n#Studentized residuals\nout = model_log.outlier_test()\nout\n\n\n\n\n\n\n\n\nstudent_resid\nunadj_p\nbonf(p)\n\n\n\n\n0\n-1.164204\n0.244398\n1.0\n\n\n1\n-0.801879\n0.422661\n1.0\n\n\n2\n-1.263820\n0.206354\n1.0\n\n\n3\n-0.614171\n0.539131\n1.0\n\n\n4\n0.027929\n0.977720\n1.0\n\n\n...\n...\n...\n...\n\n\n4955\n-0.523361\n0.600747\n1.0\n\n\n4956\n-0.509538\n0.610398\n1.0\n\n\n4957\n-1.718808\n0.085712\n1.0\n\n\n4958\n-0.077594\n0.938154\n1.0\n\n\n4959\n-0.482388\n0.629551\n1.0\n\n\n\n\n4960 rows √ó 3 columns\nStudentized residuals are in the first column of the above table. Let us plot the studentized residuals against fitted values. In the figure below, the studentized residuals above the top dotted green line and below the bottom dotted green line are outliers.\n#Plotting studentized residuals vs fitted values\nsns.scatterplot(x = (model_log.fittedvalues), y=(out.student_resid),color = 'orange')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [0,0],color = 'blue')\nax = sns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [critical_value, critical_value],\n             color = 'green')\nsns.lineplot(x = [model_log.fittedvalues.min(),model_log.fittedvalues.max()],y = [-critical_value, -critical_value],\n             color = 'green')\nax.lines[1].set_linestyle(\"--\")\nax.lines[2].set_linestyle(\"--\")\n\nplt.xlabel('Fitted values')\nplt.ylabel('Studentized Residuals');\nOutliers: Observations whose studentized residuals have a magnitude greater than \\(t\\big(1 - \\frac{\\alpha}{2n}, n - p - 1\\big)\\).\nImpact of outliers: Outliers do not have a large impact on the OLS line / plane / hyperplane as long as they don‚Äôt have a high leverage (discussed in the next section). However, outliers do inflate the residual standard error (RSE). RSE in turn is used to compute the standard errors of regression coefficients. As a result, statistically significant variables may appear to be insignificant, and \\(R^2\\) may appear to be lower.\nAre there outliers in our example?\n#Number of points with absolute studentized residuals greater than critical_value\nnp.sum(np.abs(out.student_resid) &gt; critical_value)\n\n19\nLet us analyze the outliers.\nind = (np.abs(out.student_resid) &gt; critical_value)\npd.concat([train.loc[ind,:], np.exp(model_log.fittedvalues[ind])], axis = 1)\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n0\n\n\n\n\n2042\n18228\nbmw\ni3\n2017\nAutomatic\n24041\nHybrid\n0\n78.2726\n0.0\n21495\n5537.337460\n\n\n2046\n17362\nbmw\ni3\n2016\nAutomatic\n68000\nHybrid\n0\n78.0258\n0.0\n15990\n4107.811771\n\n\n2050\n19224\nbmw\ni3\n2016\nAutomatic\n20013\nHybrid\n0\n77.9310\n0.0\n19875\n4784.986021\n\n\n2051\n13913\nbmw\ni3\n2014\nAutomatic\n34539\nHybrid\n0\n78.3838\n0.0\n14495\n3269.686113\n\n\n2055\n16512\nbmw\ni3\n2017\nAutomatic\n28169\nHybrid\n0\n77.9799\n0.0\n23751\n5454.207333\n\n\n2059\n15844\nbmw\ni3\n2016\nAutomatic\n19995\nHybrid\n0\n78.2825\n0.0\n19850\n4773.707307\n\n\n2060\n12107\nbmw\ni3\n2016\nAutomatic\n8421\nHybrid\n0\n77.9125\n0.0\n19490\n5028.048305\n\n\n2061\n18215\nbmw\ni3\n2014\nAutomatic\n37161\nHybrid\n0\n77.7505\n0.0\n14182\n3259.101789\n\n\n2063\n15617\nbmw\ni3\n2017\nAutomatic\n41949\nHybrid\n140\n78.1907\n0.0\n19998\n5173.402125\n\n\n2064\n18020\nbmw\ni3\n2015\nAutomatic\n9886\nHybrid\n0\n78.1810\n0.0\n17481\n4214.053932\n\n\n2143\n12972\nbmw\ni8\n2017\nAutomatic\n9992\nHybrid\n135\n69.2767\n1.5\n59950\n14675.519883\n\n\n2144\n13826\nbmw\ni8\n2015\nAutomatic\n43323\nHybrid\n0\n69.2683\n1.5\n44990\n9289.744847\n\n\n2150\n18949\nbmw\ni8\n2015\nAutomatic\n43102\nHybrid\n0\n69.0922\n1.5\n42890\n9300.576839\n\n\n2151\n18977\nbmw\ni8\n2016\nAutomatic\n10087\nHybrid\n0\n68.9279\n1.5\n48998\n12607.867130\n\n\n2744\n18866\nmerc\nM Class\n2004\nAutomatic\n121000\nDiesel\n325\n29.3713\n2.7\n19950\n4068.883513\n\n\n3548\n13149\naudi\nS4\n2019\nAutomatic\n4900\nDiesel\n145\n40.7030\n0.0\n45000\n10679.644966\n\n\n4116\n16420\naudi\nSQ5\n2020\nAutomatic\n1500\nDiesel\n145\n34.7968\n0.0\n56450\n13166.374034\n\n\n4117\n17611\naudi\nSQ5\n2019\nAutomatic\n1500\nDiesel\n145\n34.5016\n0.0\n48800\n11426.005642\n\n\n4851\n16577\nbmw\nZ3\n2002\nAutomatic\n16500\nPetrol\n325\n29.7614\n2.2\n14995\n3426.196256\nDo you notice some unique characteristics of these observations due to which they may be outliers?\nWhat methods you can propose to estimate the price of these outliers more accurately, which will also result in the overall reduction in RMSE?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#high-leverage-points",
    "href": "Lec5_Potential_issues.html#high-leverage-points",
    "title": "7¬† Beyond Fit (statistical theory)",
    "section": "7.2 High leverage points",
    "text": "7.2 High leverage points\nHigh leverage points are those with an unsual value of the predictor(s). They have the potential to have a relatively higher impact on the OLS line / plane / hyperplane, as compared to the outliers.\nLeverage statistic (page 99 of the book): In order to quantify an observation‚Äôs leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage. For simple linear regression, \\[\\begin{equation}\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar x)^2}{\\sum_{i'=1}^{n}(x_{i'} - \\bar x)^2}.\n\\end{equation}\\]\nIt is clear from this equation that \\(h_i\\) increases with the distance of \\(x_i\\) from \\(\\bar x\\). A large value of \\(h_i\\) indicates that the \\(i^{th}\\) observation is distance from the center of all the other observations in terms of predictor values.\nThe leverage statistic \\(h_i\\) is always between \\(1/n\\) and \\(1\\), and the average leverage for all the observations is always equal to \\((p+1)/n\\):\n\\[\\begin{equation}\n\\bar{h} = \\frac{p+1}{n}\n\\end{equation}\\]\nSo if a given observation has a leverage statistic that greatly exceeds \\((p+1)/n\\), then we may suspect that the corresponding point has high leverage.\nIf the \\(i^{th}\\) observation has a large leverage \\(h_i\\), it may exercise substantial leverage in determining the fitted value \\(\\hat{Y}_i\\), because:\n\nThe fitted value \\(\\hat{Y}_i\\) is a linear combination of the observed \\(Y\\) values, and \\(h_i\\) is the weight of observation \\(Y_i\\) in determining this fitted value.\nThe larger the \\(h_i\\), the smaller is the variance of the residual \\(e_i\\), and the closer the fitted value \\(\\hat{Y}_i\\) will tend to be the observed value \\(Y_i\\).\n\nThumb rules:\n\nA leverage \\(h_i\\) is usually considered large if it is more than twice as large as the mean value \\(\\bar{h}\\).\nAnother suggested guideline is that \\(h_i\\) values exceeding 0.5 indicate very high leverage, whereas those between 0.2 and 0.5 indicate moderate leverage.\n\nInfluential points: Note that if a high leverage point falls in line with the regression line, then it will not affect the regression line. However, it may inflate \\(R\\)-squared and increase the significance of predictors. If a high leverage point falls away from the regression line, then it is also an outlier, and will affect the regression line. The points whose presence significantly affects the regression line are called influential points. A point that is both a high leverage point and an outlier is likely to be an influential point. However, a high leverage point is not necessarily an influential point.\nSource for influential points: https://online.stat.psu.edu/stat501/book/export/html/973\nLet us see if there are any high leverage points in our regression model.\n\n#Model with an interaction term and a variable transformation term\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.803\n\n\nModel:\nOLS\nAdj. R-squared:\n0.803\n\n\nMethod:\nLeast Squares\nF-statistic:\n1834.\n\n\nDate:\nSun, 10 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n16:53:39\nLog-Likelihood:\n-1173.8\n\n\nNo. Observations:\n4960\nAIC:\n2372.\n\n\nDf Residuals:\n4948\nBIC:\n2450.\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-238.2125\n25.790\n-9.237\n0.000\n-288.773\n-187.652\n\n\nyear\n0.1227\n0.013\n9.608\n0.000\n0.098\n0.148\n\n\nengineSize\n13.8349\n5.795\n2.387\n0.017\n2.475\n25.195\n\n\nmileage\n0.0005\n0.000\n3.837\n0.000\n0.000\n0.001\n\n\nmpg\n-1.2446\n0.345\n-3.610\n0.000\n-1.921\n-0.569\n\n\nyear:engineSize\n-0.0067\n0.003\n-2.324\n0.020\n-0.012\n-0.001\n\n\nyear:mileage\n-2.67e-07\n6.8e-08\n-3.923\n0.000\n-4e-07\n-1.34e-07\n\n\nyear:mpg\n0.0006\n0.000\n3.591\n0.000\n0.000\n0.001\n\n\nengineSize:mileage\n-2.668e-07\n4.08e-07\n-0.654\n0.513\n-1.07e-06\n5.33e-07\n\n\nengineSize:mpg\n0.0028\n0.000\n6.842\n0.000\n0.002\n0.004\n\n\nmileage:mpg\n7.235e-08\n1.79e-08\n4.036\n0.000\n3.72e-08\n1.08e-07\n\n\nI(mileage ** 2)\n1.828e-11\n5.64e-12\n3.240\n0.001\n7.22e-12\n2.93e-11\n\n\n\n\n\n\n\n\nOmnibus:\n711.514\nDurbin-Watson:\n0.498\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2545.807\n\n\nSkew:\n0.699\nProb(JB):\n0.00\n\n\nKurtosis:\n6.220\nCond. No.\n1.73e+13\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.73e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Computing the leverage statistic for each observation\ninfluence = model_log.get_influence()\nleverage = influence.hat_matrix_diag\n\n\n#Visualizng leverage against studentized residuals\nsns.set(rc={'figure.figsize':(15,8)})\nsm.graphics.influence_plot(model_log);\n\n\n\n\n\n\n\n\nLet us identify the high leverage points in the data, as they may be affecting the regression line if they are outliers as well, i.e., if they are influential points. Note that there is no defined threshold for a point to be classified as a high leverage point. Some statisticians consider points having twice the average leverage as high leverage points, some consider points having thrice the average leverage as high leverage points, and so on.\n\nout = model_log.outlier_test()\n\n\n#Average leverage of points\naverage_leverage = (model_log.df_model+1)/model_log.nobs\naverage_leverage\n\n0.0024193548387096775\n\n\nLet us consider points having four times the average leverage as high leverage points.\n\n#We will remove all observations that have leverage higher than the threshold value.\nhigh_leverage_threshold = 3*average_leverage\n\n\n#Number of high leverage points in the dataset\nnp.sum(leverage&gt;high_leverage_threshold)\n\n269\n\n\n\n7.2.1 Identifying extrapolation using leverage\nLeverage can be used to check if prediction on a particular point will lead to extrapolation.\nBelow is the function that can be used to find the leverage at for a particular observation xnew. Note that xnew has to be a single-dimensional array, and X has to be the predictor matrix (also called the design matrix).\n\ndef leverage_compute(xnew, X):\n    return(xnew.reshape(-1, 1).T.dot(np.linalg.inv(X.T.dot(X))).dot(xnew.reshape(-1, 1))[0][0])\n\nAs expected, the function will return the same leverage as provided by the hat_matrix_diag attribute of the objected returned by the get_influence() method of model_log as shown below:\n\nleverage[0]\n\n0.0026426981240353694\n\n\nAs the observation for prediction is required we need to create the predictor matrix X to create all the observations with the interactions specified in the model.\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\n\n\nleverage_compute(X[0,:], X)\n\n0.0026426973869101977\n\n\nIf the leverage for a new observation is higher than the maximum leverage among all the observations in the training dataset, then prediction at the new observation will be extrapolation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#influential-points",
    "href": "Lec5_Potential_issues.html#influential-points",
    "title": "7¬† Beyond Fit (statistical theory)",
    "section": "7.3 Influential points",
    "text": "7.3 Influential points\nObservations that are both high leverage points and outliers are influential points that may affect the regression line. Let‚Äôs remove these influential points from the data and see if it improves the model prediction accuracy on test data.\n\n#Dropping influential points from data\ntrain_filtered = train.drop(np.intersect1d(np.where(np.abs(out.student_resid) &gt; critical_value)[0],\n                                           (np.where(leverage&gt;high_leverage_threshold)[0])))\n\nNote that as the Bonferroni‚Äôs critical value is very conservative estimate, we have rounded off the critical value to 4, instead of 4.42.\n\ntrain_filtered.shape\n\n(4948, 11)\n\n\n\n#Number of points removed as they were influential\ntrain.shape[0]-train_filtered.shape[0]\n\n12\n\n\nWe removed 12 influential data points from the training data.\n\n#Model after removing the influential observations\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(price)\nR-squared:\n0.815\n\n\nModel:\nOLS\nAdj. R-squared:\n0.814\n\n\nMethod:\nLeast Squares\nF-statistic:\n1971.\n\n\nDate:\nSun, 10 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n16:54:08\nLog-Likelihood:\n-1027.9\n\n\nNo. Observations:\n4948\nAIC:\n2080.\n\n\nDf Residuals:\n4936\nBIC:\n2158.\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-256.2339\n25.421\n-10.080\n0.000\n-306.070\n-206.398\n\n\nyear\n0.1317\n0.013\n10.462\n0.000\n0.107\n0.156\n\n\nengineSize\n18.4650\n5.663\n3.261\n0.001\n7.364\n29.566\n\n\nmileage\n0.0006\n0.000\n4.288\n0.000\n0.000\n0.001\n\n\nmpg\n-1.1810\n0.338\n-3.489\n0.000\n-1.845\n-0.517\n\n\nyear:engineSize\n-0.0090\n0.003\n-3.208\n0.001\n-0.015\n-0.004\n\n\nyear:mileage\n-2.933e-07\n6.7e-08\n-4.374\n0.000\n-4.25e-07\n-1.62e-07\n\n\nyear:mpg\n0.0006\n0.000\n3.458\n0.001\n0.000\n0.001\n\n\nengineSize:mileage\n-4.316e-07\n4e-07\n-1.080\n0.280\n-1.21e-06\n3.52e-07\n\n\nengineSize:mpg\n0.0048\n0.000\n11.537\n0.000\n0.004\n0.006\n\n\nmileage:mpg\n7.254e-08\n1.75e-08\n4.140\n0.000\n3.82e-08\n1.07e-07\n\n\nI(mileage ** 2)\n1.668e-11\n5.53e-12\n3.017\n0.003\n5.84e-12\n2.75e-11\n\n\n\n\n\n\n\n\nOmnibus:\n718.619\nDurbin-Watson:\n0.521\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n2512.509\n\n\nSkew:\n0.714\nProb(JB):\n0.00\n\n\nKurtosis:\n6.185\nCond. No.\n1.75e+13\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.75e+13. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nLet us compare the square root of 5-fold cross-validated mean squared error of the model with and without the influential points.\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train)\nnp.sqrt(mean_squared_error(np.exp(cross_val_predict(LinearRegression(), X, y)), np.exp(y)))\n\n9811.74078331643\n\n\n\ny, X = dmatrices('np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', data = train_filtered)\nnp.sqrt(mean_squared_error(np.exp(cross_val_predict(LinearRegression(), X, y)), np.exp(y)))\n\n9800.202063309154\n\n\nWhy can‚Äôt we use cross_val_score() instead of cross_val_predict() here?\nThere seems to be a slight improvement in prediction error after removing influential points. Note that none of the points had ‚Äúvery high leverage‚Äù, and thus the change is not substantial.\nNote that we obtain a higher R-squared value of 81.5% as compared to 80% with the complete data. Removing the influential points helped obtain a slightly better model fit. However, that may also happen just by reducing observations.\n\n#Computing RMSE on test data\npred_price_log = model_log.predict(testf)\nnp.sqrt(((testp.price - np.exp(pred_price_log))**2).mean())\n\n8922.977452912108\n\n\nThe RMSE on test data has also reduced. This shows that some of the influential points were impacting the regression line. With those points removed, the model better captures the general trend in the data.\n\n7.3.1 Influence on single fitted value (DFFITS)\n\nA useful measure of the influence that the \\(i^{th}\\) observation has on the fitted value \\(\\hat{Y}_i\\) is:\n\n\\[\\begin{equation}\n(DFFITS)_i = \\frac{\\hat{Y}_i-\\hat{Y}_{i(i)}}{\\sqrt{MSE_{i}h_i}}\n\\end{equation}\\]\n\nNote that the denominator in the above fraction is the estimated standard deviation of \\(\\hat{Y}_i\\), but uses the error mean square when the \\(i^{th}\\) observation is omitted.\n\\(DFFITS\\) for the \\(i^{th}\\) observation represents the number of estimated standard deviations of \\(\\hat{Y}_i\\) that the fitted value \\(\\hat{Y}_i\\) increases or decreases with the inclusion of the \\(i^{th}\\) observation in fitting the regression model.\nIt can be shown that:\n\n\\[\\begin{equation}\n(DFFITS)_i = t_i\\sqrt{\\frac{h_i}{1-h_i}}\n\\end{equation}\\]\nwhere \\(t_i\\) is the studentized deleted residual for the \\(i^{th}\\) observation.\n\nWe can see that if an observation has high leverage and is an outlier, it is likely to be influential\nFor large datasets, an observation is considered influential if the magnitude of \\(DFFITS\\) for it exceeds \\(2\\sqrt{\\frac{p}{n}}\\)\n\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.dffits[0])\nplt.xlabel('Observation index')\nplt.ylabel('DFFITS (model)');\n\n\n\n\n\n\n\n\nLet us analyze the point with the highest \\(DFFITS\\).\n\nnp.where(influence.dffits[0]&gt;1)\n\n(array([ 813, 4851], dtype=int64),)\n\n\n\ntrain.loc[813,:]\n\ncarID                12454\nbrand                   vw\nmodel            Caravelle\nyear                  2012\ntransmission     Semi-Auto\nmileage             212000\nfuelType            Diesel\ntax                    325\nmpg                34.4424\nengineSize             2.0\nprice                11995\nName: 813, dtype: object\n\n\n\ntrain.loc[train.model == ' Caravelle','mileage'].describe()\n\ncount        65.000000\nmean      25638.692308\nstd       42954.135726\nmin          10.000000\n25%        3252.000000\n50%        6900.000000\n75%       30414.000000\nmax      212000.000000\nName: mileage, dtype: float64\n\n\n\n# Prediction with model developed based on all points\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', \n                     data = train)\nmodel_log = ols_object.fit();\nnp.exp(model_log.predict(train.loc[[813],:]))\n\n813    5502.647323\ndtype: float64\n\n\n\n# Prediction with model developed based on all points except the 813th point\nols_object = smf.ols(formula = 'np.log(price)~(year+engineSize+mileage+mpg)**2+I(mileage**2)', \n                     data = train.drop(index = 813))\nmodel_log = ols_object.fit();\nnp.exp(model_log.predict(train.loc[[813],:]))\n\n813    4581.374593\ndtype: float64\n\n\nLet us see the leverage and studentized residual for this observation.\n\n# Leverage\nleverage[813]\n\n0.19038697461006687\n\n\n\n# Studentized residual\nout.student_resid[813]\n\n2.823478041409651\n\n\nDo you notice what may be contributing to the high influence of this point?\n\n\n7.3.2 Influence on all fitted values (Cook‚Äôs distance)\nIn contrast to \\(DFFITS\\), which considers the influence of the \\(i^{th}\\) observation on the fitted value \\(\\hat{Y}_i\\), Cook‚Äôs distance considers the influence of the \\(i^{th}\\) observation on all \\(n\\) the fitted values:\n\\[\\begin{equation}\nD_i = \\frac{\\sum_{j=1}^{n} (\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{pMSE}\n\\end{equation}\\]\nIt can be shown that:\n\\[\\begin{equation}\nD_i = \\frac{e_i^2}{pMSE}\\bigg[\\frac{h_i}{(1-h_i)^2}\\bigg]\n\\end{equation}\\]\nThe larger \\(h_i\\) or \\(e_i\\), the larger is \\(D_i\\). \\(D_i\\) can be related to the \\(F(p, n- p)\\) distribution. If the percentile value is 50% or more, the observation is considered as highly influential.\nCook‚Äôs distance is considered high if it is greater than 0.5 and extreme if it is greater than 1.\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.cooks_distance[0])\nplt.xlabel('Observation index')\nplt.ylabel(\"Cook's Distance (model)\");\n\n\n\n\n\n\n\n\n\n# Point with the highest Cook's distance\nnp.where(influence.cooks_distance[0]&gt;0.15)\n\n(array([813], dtype=int64),)\n\n\nThe critical Cook‚Äôs distance value for a point to be highly influential in this dataset is:\n\nstats.f.ppf(0.5, 11, 4949)\n\n0.9402181103263811\n\n\nThus, we don‚Äôt have any highly influential points in the dataset.\n\n\n7.3.3 Influence on regression coefficients (DFBETAS)\n\n\\(DFBETAS\\) measures the influence of the \\(i^{th}\\) observation on the regression coefficient.\n\\(DFBETAS\\) of the \\(i^{th}\\) observation on the \\(k^{th}\\) regression coefficient is:\n\n\\[\\begin{equation}\n(DFBETAS)_{k(i)} = \\frac{\\hat{\\beta}_k-\\hat{\\beta}_{k(i)}}{\\sqrt{MSE_ic_{k}}}\n\\end{equation}\\]\nwhere \\(c_k\\) is the \\(k^{th}\\) diagonal element of \\((X^TX)^{-1}\\).\nFor large datasets, an observation is considered influential if \\(DFBETAS\\) exceeds \\(\\frac{2}{\\sqrt{n}}\\).\nBelow is the plot of \\(DFBETAS\\) for the year predictor against the observation index.\n\nsns.set(font_scale =1.5)\nsns.lineplot(x = range(train.shape[0]), y = influence.dfbetas[:,1])\nplt.xlabel('Observation index')\nplt.ylabel(\"DFBETAS (year)\");\n\n\n\n\n\n\n\n\nLet us analyze the point with the highest magnitude of \\(DFBETAS\\).\n\nnp.where(influence.dfbetas[:,1]&lt;-0.8)\n\n(array([4851], dtype=int64),)\n\n\n\ntrain.year.describe()\n\ncount    4960.000000\nmean     2016.737903\nstd         2.884035\nmin      1997.000000\n25%      2016.000000\n50%      2017.000000\n75%      2019.000000\nmax      2020.000000\nName: year, dtype: float64\n\n\n\ntrain.loc[train.year&lt;=2002,:]\n\n\n\n\n\n\n\n\ncarID\nbrand\nmodel\nyear\ntransmission\nmileage\nfuelType\ntax\nmpg\nengineSize\nprice\n\n\n\n\n330\n13200\naudi\nA8\n1997\nAutomatic\n122000\nPetrol\n265\n19.3511\n4.2\n4650\n\n\n732\n13988\nvw\nBeetle\n2001\nManual\n47729\nPetrol\n330\n32.5910\n2.0\n2490\n\n\n3157\n18794\nford\nPuma\n2002\nManual\n108000\nPetrol\n230\n38.5757\n1.7\n2195\n\n\n3525\n19395\nmerc\nS Class\n2001\nAutomatic\n108800\nDiesel\n325\n31.5473\n3.2\n1695\n\n\n3532\n17531\nmerc\nS Class\n1999\nAutomatic\n34000\nPetrol\n145\n24.8735\n3.2\n5995\n\n\n3533\n18761\nmerc\nS Class\n2001\nAutomatic\n66000\nPetrol\n570\n24.7744\n3.2\n4495\n\n\n3535\n18813\nmerc\nS Class\n1998\nAutomatic\n43534\nPetrol\n265\n23.2962\n6.0\n19990\n\n\n3536\n17891\nmerc\nS Class\n2002\nAutomatic\n24000\nPetrol\n570\n20.7968\n5.0\n6995\n\n\n3707\n18746\nhyundi\nSanta Fe\n2002\nManual\n94000\nPetrol\n325\n30.2671\n2.4\n1200\n\n\n4091\n12995\nmerc\nSLK\n1998\nAutomatic\n113557\nPetrol\n265\n31.8368\n2.3\n1990\n\n\n4094\n19585\nmerc\nSLK\n2001\nAutomatic\n69234\nPetrol\n325\n30.8839\n2.0\n3990\n\n\n4096\n14265\nmerc\nSLK\n2001\nAutomatic\n48172\nPetrol\n325\n29.7058\n2.3\n3990\n\n\n4097\n15821\nmerc\nSLK\n2002\nAutomatic\n61400\nPetrol\n325\n29.6568\n2.3\n3990\n\n\n4098\n13021\nmerc\nSLK\n2001\nAutomatic\n91000\nPetrol\n325\n30.3248\n2.3\n3950\n\n\n4099\n12660\nmerc\nSLK\n2001\nAutomatic\n42087\nPetrol\n325\n29.9404\n2.3\n4490\n\n\n4101\n17521\nmerc\nSLK\n2002\nAutomatic\n75034\nPetrol\n325\n30.1380\n2.3\n4990\n\n\n4107\n13977\nmerc\nSLK\n2000\nAutomatic\n87000\nPetrol\n265\n27.2998\n3.2\n1490\n\n\n4108\n18679\nmerc\nSLK\n2000\nAutomatic\n113237\nPetrol\n270\n26.8765\n3.2\n3990\n\n\n4109\n14598\nmerc\nSLK\n2001\nAutomatic\n64476\nPetrol\n325\n27.4628\n3.2\n4990\n\n\n4847\n17268\nbmw\nZ3\n1997\nManual\n49000\nPetrol\n270\n34.9548\n1.9\n3950\n\n\n4848\n12137\nbmw\nZ3\n1999\nManual\n58000\nPetrol\n270\n35.3077\n1.9\n3950\n\n\n4849\n13288\nbmw\nZ3\n1999\nManual\n74282\nPetrol\n245\n35.4143\n1.9\n3995\n\n\n4850\n19172\nbmw\nZ3\n2001\nManual\n60000\nPetrol\n325\n30.7305\n2.2\n5950\n\n\n4851\n16577\nbmw\nZ3\n2002\nAutomatic\n16500\nPetrol\n325\n29.7614\n2.2\n14995\n\n\n\n\n\n\n\nLet us see the leverage and studentized residual for this observation.\n\n# Leverage\nleverage[4851]\n\n0.047120455781282225\n\n\n\n# Studentized residual\nout.student_resid[4851]\n\n4.938606329343604\n\n\nDo you see what makes this point influential?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Lec5_Potential_issues.html#collinearity",
    "href": "Lec5_Potential_issues.html#collinearity",
    "title": "7¬† Beyond Fit (statistical theory)",
    "section": "7.4 Collinearity",
    "text": "7.4 Collinearity\nCollinearity refers to the situation when two or more predictor variables have a high linear association. Linear association between a pair of variables can be measured by the correlation coefficient. Thus the correlation matrix can indicate some potential collinearity problems.\n\n7.4.1 Why and how is collinearity a problem\n(Source: page 100-101 of book)\nThe presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.\nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for \\(\\hat \\beta_j\\) to grow. Recall that the t-statistic for each predictor is calculated by dividing \\(\\hat \\beta_j\\) by its standard error. Consequently, collinearity results in a decline in the \\(t\\)-statistic. As a result, in the presence of collinearity, we may fail to reject \\(H_0: \\beta_j = 0\\). This means that the power of the hypothesis test‚Äîthe probability of correctly detecting a non-zero coefficient‚Äîis reduced by collinearity.\n\n\n7.4.2 How to measure collinearity/multicollinearity\n(Source: page 102 of book)\nUnfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity. Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is variance inflation factor the ratio of the variance of \\(\\hat \\beta_j\\) when fitting the full model divided by the variance of \\(\\hat \\beta_j\\) if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.\nThe estimated variance of the coefficient \\(\\beta_j\\), of the \\(j^{th}\\) predictor \\(X_j\\), can be expressed as:\n\\[\\hat{var}(\\hat{\\beta_j}) = \\frac{(\\hat{\\sigma})^2}{(n-1)\\hat{var}({X_j})}.\\frac{1}{1-R^2_{X_j|X_{-j}}},\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the \\(R\\)-squared for the regression of \\(X_j\\) on the other covariates (a regression that does not involve the response variable \\(Y\\)).\nIn case of simple linear regression, the variance expression in the equation above does not contain the term \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\), as there is only one predictor. However, in case of multiple linear regression, the variance of the estimate of the \\(j^{th}\\) coefficient (\\(\\hat{\\beta_j}\\)) gets inflated by a factor of \\(\\frac{1}{1-R^2_{X_j|X_{-j}}}\\) (Note that in the complete absence of collinearity, \\(R^2_{X_j|X_{-j}}=0\\), and the value of this factor will be 1).\nThus, the Variance inflation factor, or the VIF for the estimated coefficient of the \\(j^{th}\\) predictor \\(X_j\\) is:\n\\[\\begin{equation}\nVIF(\\hat \\beta_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\n\\end{equation}\\]\n\n#Correlation matrix\ntrain.corr()\n\n\n\n\n\n\n\n\ncarID\nyear\nmileage\ntax\nmpg\nengineSize\nprice\n\n\n\n\ncarID\n1.000000\n0.006251\n-0.001320\n0.023806\n-0.010774\n0.011365\n0.012129\n\n\nyear\n0.006251\n1.000000\n-0.768058\n-0.205902\n-0.057093\n0.014623\n0.501296\n\n\nmileage\n-0.001320\n-0.768058\n1.000000\n0.133744\n0.125376\n-0.006459\n-0.478705\n\n\ntax\n0.023806\n-0.205902\n0.133744\n1.000000\n-0.488002\n0.465282\n0.144652\n\n\nmpg\n-0.010774\n-0.057093\n0.125376\n-0.488002\n1.000000\n-0.419417\n-0.369919\n\n\nengineSize\n0.011365\n0.014623\n-0.006459\n0.465282\n-0.419417\n1.000000\n0.624899\n\n\nprice\n0.012129\n0.501296\n-0.478705\n0.144652\n-0.369919\n0.624899\n1.000000\n\n\n\n\n\n\n\nLet us compute the Variance Inflation Factor (VIF) for the four predictors.\n\nX = train[['mpg','year','mileage','engineSize']]\n\n\nX.columns[1:]\n\nIndex(['year', 'mileage', 'engineSize'], dtype='object')\n\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(X)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\nfor i in range(len(X.columns)):\n    vif_data.loc[i,'VIF'] = variance_inflation_factor(X.values, i)\n\nprint(vif_data)\n\n      feature           VIF\n0       const  1.201579e+06\n1         mpg  1.243040e+00\n2        year  2.452891e+00\n3     mileage  2.490210e+00\n4  engineSize  1.219170e+00\n\n\nAs all the values of VIF are close to one, we do not have the problem of multicollinearity in the model. Note that the VIF of year and mileage is relatively high as they are the most correlated.\nQ1: Why is the VIF of the constant so high?\nQ2: Why do we need to include the constant while finding the VIF?\n\n\n7.4.3 Manual computation of VIF\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'price~mpg', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.137\n\n\nModel:\nOLS\nAdj. R-squared:\n0.137\n\n\nMethod:\nLeast Squares\nF-statistic:\n786.0\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n1.14e-160\n\n\nTime:\n17:04:39\nLog-Likelihood:\n-54812.\n\n\nNo. Observations:\n4960\nAIC:\n1.096e+05\n\n\nDf Residuals:\n4958\nBIC:\n1.096e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n4.144e+04\n676.445\n61.258\n0.000\n4.01e+04\n4.28e+04\n\n\nmpg\n-374.2975\n13.351\n-28.036\n0.000\n-400.471\n-348.124\n\n\n\n\n\n\n\n\nOmnibus:\n2132.208\nDurbin-Watson:\n0.320\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n13751.995\n\n\nSkew:\n1.942\nProb(JB):\n0.00\n\n\nKurtosis:\n10.174\nCond. No.\n158.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n(13.351/9.338)**2\n\n2.044183378279958\n\n\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'price~year+mpg+engineSize+mileage', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n17:01:18\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.661e+06\n1.49e+05\n-24.593\n0.000\n-3.95e+06\n-3.37e+06\n\n\nyear\n1817.7366\n73.751\n24.647\n0.000\n1673.151\n1962.322\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#Manually computing the VIF for year\nols_object = smf.ols(formula = 'year~mpg+engineSize+mileage', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nyear\nR-squared:\n0.592\n\n\nModel:\nOLS\nAdj. R-squared:\n0.592\n\n\nMethod:\nLeast Squares\nF-statistic:\n2400.\n\n\nDate:\nWed, 06 Mar 2024\nProb (F-statistic):\n0.00\n\n\nTime:\n17:00:13\nLog-Likelihood:\n-10066.\n\n\nNo. Observations:\n4960\nAIC:\n2.014e+04\n\n\nDf Residuals:\n4956\nBIC:\n2.017e+04\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2018.3135\n0.140\n1.44e+04\n0.000\n2018.039\n2018.588\n\n\nmpg\n0.0095\n0.002\n5.301\n0.000\n0.006\n0.013\n\n\nengineSize\n0.1171\n0.037\n3.203\n0.001\n0.045\n0.189\n\n\nmileage\n-9.139e-05\n1.08e-06\n-84.615\n0.000\n-9.35e-05\n-8.93e-05\n\n\n\n\n\n\n\n\nOmnibus:\n2949.664\nDurbin-Watson:\n1.161\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n63773.271\n\n\nSkew:\n-2.426\nProb(JB):\n0.00\n\n\nKurtosis:\n19.883\nCond. No.\n1.91e+05\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.91e+05. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\n#VIF for year\n1/(1-0.592)\n\n2.4509803921568625\n\n\nNote that year and mileage have a high linear correlation. Removing one of them should decrease the standard error of the coefficient of the other, without significanty decrease R-squared.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+mileage+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.660\n\n\nModel:\nOLS\nAdj. R-squared:\n0.660\n\n\nMethod:\nLeast Squares\nF-statistic:\n2410.\n\n\nDate:\nTue, 07 Feb 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n21:39:45\nLog-Likelihood:\n-52497.\n\n\nNo. Observations:\n4960\nAIC:\n1.050e+05\n\n\nDf Residuals:\n4955\nBIC:\n1.050e+05\n\n\nDf Model:\n4\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-3.661e+06\n1.49e+05\n-24.593\n0.000\n-3.95e+06\n-3.37e+06\n\n\nmpg\n-79.3126\n9.338\n-8.493\n0.000\n-97.620\n-61.006\n\n\nengineSize\n1.218e+04\n189.969\n64.107\n0.000\n1.18e+04\n1.26e+04\n\n\nmileage\n-0.1474\n0.009\n-16.817\n0.000\n-0.165\n-0.130\n\n\nyear\n1817.7366\n73.751\n24.647\n0.000\n1673.151\n1962.322\n\n\n\n\n\n\n\n\nOmnibus:\n2450.973\nDurbin-Watson:\n0.541\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n31060.548\n\n\nSkew:\n2.045\nProb(JB):\n0.00\n\n\nKurtosis:\n14.557\nCond. No.\n3.83e+07\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 3.83e+07. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nRemoving mileage from the above regression.\n\nols_object = smf.ols(formula = 'price~mpg+engineSize+year', data = train)\nmodel_log = ols_object.fit()\nmodel_log.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nprice\nR-squared:\n0.641\n\n\nModel:\nOLS\nAdj. R-squared:\n0.641\n\n\nMethod:\nLeast Squares\nF-statistic:\n2951.\n\n\nDate:\nTue, 07 Feb 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n21:40:00\nLog-Likelihood:\n-52635.\n\n\nNo. Observations:\n4960\nAIC:\n1.053e+05\n\n\nDf Residuals:\n4956\nBIC:\n1.053e+05\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-5.586e+06\n9.78e+04\n-57.098\n0.000\n-5.78e+06\n-5.39e+06\n\n\nmpg\n-101.9120\n9.500\n-10.727\n0.000\n-120.536\n-83.288\n\n\nengineSize\n1.196e+04\n194.848\n61.392\n0.000\n1.16e+04\n1.23e+04\n\n\nyear\n2771.1844\n48.492\n57.147\n0.000\n2676.118\n2866.251\n\n\n\n\n\n\n\n\nOmnibus:\n2389.075\nDurbin-Watson:\n0.528\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n26920.051\n\n\nSkew:\n2.018\nProb(JB):\n0.00\n\n\nKurtosis:\n13.675\nCond. No.\n1.41e+06\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.41e+06. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nNote that the standard error of the coefficient of year has reduced from 73 to 48, without any large reduction in R-squared.\n\n\n7.4.4 When can we overlook multicollinearity?\n\nThe severity of the problems increases with the degree of the multicollinearity. Therefore, if there is only moderate multicollinearity (5 &lt; VIF &lt; 10), we may overlook it.\nMulticollinearity affects only the standard errors of the coefficients of collinear predictors. Therefore, if multicollinearity is not present for the predictors that we are particularly interested in, we may not need to resolve it.\nMulticollinearity affects the standard error of the coefficients and thereby their \\(p\\)-values, but in general, it does not influence the prediction accuracy, except in the case that the coefficients are so unstable that the predictions are outside of the domain space of the response. If our sole aim is prediction, and we don‚Äôt wish to infer the statistical significance of predictors, then we may avoid addressing multicollinearity. ‚ÄúThe fact that some or all predictor variables are correlated among themselves does not, in general, inhibit our ability to obtain a good fit nor does it tend to affect inferences about mean responses or predictions of new observations, provided these inferences are made within the region of observations‚Äù - Neter, John, Michael H. Kutner, Christopher J. Nachtsheim, and William Wasserman. ‚ÄúApplied linear statistical models.‚Äù (1996): 318.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Beyond Fit (statistical theory)</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html",
    "href": "Logistic Regression.html",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "",
    "text": "8.1 Theory Behind Logistic Regression\nRead sections 4.1 - 4.3 of the book before using these notes.\nNote that in this course, lecture notes are not sufficient, you must read the book for better understanding. Lecture notes are just implementing the concepts of the book on a dataset, but not explaining the concepts elaborately.\nLogistic regression is the go-to linear classification algorithm for two-class problems. It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#theory-behind-logistic-regression",
    "href": "Logistic Regression.html#theory-behind-logistic-regression",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "",
    "text": "8.1.1 Description\nLogistic regression is named for the function used at the core of the method, the logistic function.\nThe logistic function, also called the Sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It‚Äôs an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\\[\\frac{1}{1 + e^{-x}}\\]\n\\(e\\) is the base of the natural logarithms and \\(x\\) is value that you want to transform via the logistic function.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\nx = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(10, 6))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Sigmoid Function\")\n\nText(0.5, 1.0, 'Sigmoid Function')\n\n\n\n\n\n\n\n\n\nThe logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n\\[\\hat{p}=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}x_1}}\\]\nor\n\\[\\hat{p}=\\frac{1.0}{1.0+e^{-(\\hat{\\beta_0}+\\hat{\\beta_1}x_1)}}\\]\n\\(\\hat{\\beta_0}\\) is the estimated intercept term\n\\(\\hat{\\beta_1}\\) is the estimated coefficient for \\(x_1\\)\n\\(\\hat{p}\\) is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n\n\n8.1.2 Learning the Logistic Regression Model\nThe coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using maximum-likelihood estimation.\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\nThe best coefficients should result in a model that would predict a value very close to 1 (e.g.¬†male) for the default class and a value very close to 0 (e.g.¬†female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that maximize the likelihood of the observed data. In other words, in MLE, we estimate the parameter values (Beta values) which are the most likely to produce that data at hand.\nHere is an analogy to understand the idea behind Maximum Likelihood Estimation (MLE). Let us say, you are listening to a song (data). You are not aware of the singer (parameter) of the song. With just the musical piece at hand, you try to guess the singer (parameter) who you feel is the most likely (MLE) to have sung that song. Your are making a maximum likelihood estimate! Out of all the singers (parameter space) you have chosen them as the one who is the most likely to have sung that song (data).\nWe are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\nWhen you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm.\n\n\n8.1.3 Preparing Data for Logistic Regression\nThe assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\nMuch study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\nUltimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n\nBinary Output Variable: This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\nRemove Noise: Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\nGaussian Distribution: Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\nRemove Correlated Inputs: Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\nFail to Converge: It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g.¬†lots of zeros in your input data).",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "href": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "8.2 Logistic Regression: Scikit-learn vs Statsmodels",
    "text": "8.2 Logistic Regression: Scikit-learn vs Statsmodels\nPython gives us two ways to do logistic regression. Statsmodels offers modeling from the perspective of statistics. Scikit-learn offers some of the same models from the perspective of machine learning.\nSo we need to understand the difference between statistics and machine learning! Statistics makes mathematically valid inferences about a population based on sample data. Statistics answers the question, ‚ÄúWhat is the evidence that X is related to Y?‚Äù Machine learning has the goal of optimizing predictive accuracy rather than inference. Machine learning answers the question, ‚ÄúGiven X, what prediction should we make for Y?‚Äù",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#training-a-logistic-regression-model",
    "href": "Logistic Regression.html#training-a-logistic-regression-model",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "8.3 Training a logistic regression model",
    "text": "8.3 Training a logistic regression model\nRead the data on social network ads. The data shows if the person purchased a product when targeted with an ad on social media. Fit a logistic regression model to predict if a user will purchase the product based on their characteristics such as age, gender and estimated salary.\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")\nx = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(6, 4))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Sigmoid Function\");",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels-1",
    "href": "Logistic Regression.html#logistic-regression-scikit-learn-vs-statsmodels-1",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "8.4 Logistic Regression: Scikit-learn vs Statsmodels",
    "text": "8.4 Logistic Regression: Scikit-learn vs Statsmodels\nPython gives us two ways to do logistic regression. Statsmodels offers modeling from the perspective of statistics. Scikit-learn offers some of the same models from the perspective of machine learning.\nSo we need to understand the difference between statistics and machine learning! Statistics makes mathematically valid inferences about a population based on sample data. Statistics answers the question, ‚ÄúWhat is the evidence that X is related to Y?‚Äù Machine learning has the goal of optimizing predictive accuracy rather than inference. Machine learning answers the question, ‚ÄúGiven X, what prediction should we make for Y?‚Äù\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as sm\nfrom sklearn.metrics import precision_recall_curve, roc_curve, auc, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nRead the data on social network ads. The data shows if the person purchased a product when targeted with an ad on social media. Fit a logistic regression model to predict if a user will purchase the product based on their characteristics such as age, gender and estimated salary.\n\ntrain = pd.read_csv('./Datasets/Social_Network_Ads_train.csv') #Develop the model on train data\ntest = pd.read_csv('./Datasets/Social_Network_Ads_test.csv') #Test the model on test data\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n\n\n\n\n\n\n8.4.1 Examining the Distribution of the Target Column, make sure our target is not severely imbalanced\n\ntrain.Purchased.value_counts()\n\nPurchased\n0    194\n1    106\nName: count, dtype: int64\n\n\n\nsns.countplot(x = 'Purchased',data = train);\n\n\n\n\n\n\n\n\n\n\n8.4.2 Fitting a linear regression\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlm = sm.ols(formula = 'Purchased~Age', data = train).fit() #Developing linear regression model\nsns.lineplot(x = 'Age', y= lm.predict(train), data = train, color = 'blue') #Visualizing model\n\n\n\n\n\n\n\n\n\n\n8.4.3 Logistic Regression with Statsmodel\n\nsns.scatterplot(x = 'Age', y = 'Purchased', data = train, color = 'orange') #Visualizing data\nlogit_model = sm.logit(formula = 'Purchased~Age', data = train).fit() #Developing logistic regression model\nsns.lineplot(x = 'Age', y= logit_model.predict(train), data = train, color = 'blue') #Visualizing model\n\nOptimization terminated successfully.\n         Current function value: 0.430107\n         Iterations 7\n\n\n\n\n\n\n\n\n\n\nlogit_model.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\nPurchased\nNo. Observations:\n300\n\n\nModel:\nLogit\nDf Residuals:\n298\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 09 Feb 2025\nPseudo R-squ.:\n0.3378\n\n\nTime:\n18:28:20\nLog-Likelihood:\n-129.03\n\n\nconverged:\nTrue\nLL-Null:\n-194.85\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.805e-30\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-7.8102\n0.885\n-8.825\n0.000\n-9.545\n-6.076\n\n\nAge\n0.1842\n0.022\n8.449\n0.000\n0.141\n0.227\n\n\n\n\n\n\nlogit_model_gender = sm.logit(formula = 'Purchased~Gender', data = train).fit()\nlogit_model_gender.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.648804\n         Iterations 4\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nPurchased\nNo. Observations:\n300\n\n\nModel:\nLogit\nDf Residuals:\n298\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nSun, 09 Feb 2025\nPseudo R-squ.:\n0.001049\n\n\nTime:\n18:28:20\nLog-Likelihood:\n-194.64\n\n\nconverged:\nTrue\nLL-Null:\n-194.85\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.5225\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5285\n0.168\n-3.137\n0.002\n-0.859\n-0.198\n\n\nGender[T.Male]\n-0.1546\n0.242\n-0.639\n0.523\n-0.629\n0.319\n\n\n\n\n\n\n# Predicted probabilities\npredicted_probabilities = logit_model.predict(train)\npredicted_probabilities\n\n0      0.235159\n1      0.348227\n2      0.235159\n3      0.348227\n4      0.046473\n         ...   \n295    0.737081\n296    0.481439\n297    0.065810\n298    0.829688\n299    0.150336\nLength: 300, dtype: float64\n\n\n\n# Predicted classes (binary outcome, 0 or 1)\npredicted_classes = (predicted_probabilities &gt; 0.5).astype(int)\npredicted_classes\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n295    1\n296    0\n297    0\n298    1\n299    0\nLength: 300, dtype: int32\n\n\n\n#Function to compute confusion matrix and prediction accuracy on training data\ndef confusion_matrix_train(model,cutoff=0.5):\n    # Confusion matrix\n    cm_df = pd.DataFrame(model.pred_table(threshold = cutoff))\n    #Formatting the confusion matrix\n    cm_df.columns = ['Predicted 0', 'Predicted 1'] \n    cm_df = cm_df.rename(index={0: 'Actual 0',1: 'Actual 1'})\n    cm = np.array(cm_df)\n    # Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n\n\ncm = confusion_matrix_train(logit_model)\n\nClassification accuracy = 83.3%\n\n\n\n\n\n\n\n\n\n\n# change the cutoff to 0.3\ncm = confusion_matrix_train(logit_model, 0.3)\n\nClassification accuracy = 73.7%\n\n\n\n\n\n\n\n\n\n\n# increase the cutoff to 0.7\ncm = confusion_matrix_train(logit_model, 0.8)\n\nClassification accuracy = 74.7%\n\n\n\n\n\n\n\n\n\nMaking prediction on test set and output the model‚Äôs performance\n\n# Predicted probabilities\npredicted_probabilities = logit_model.predict(test)\n\n\n# Predicted classes (binary outcome, 0 or 1)\npredicted_classes = (predicted_probabilities &gt; 0.5).astype(int)\npredicted_classes\n\n0     0\n1     0\n2     0\n3     0\n4     0\n     ..\n95    1\n96    1\n97    1\n98    0\n99    1\nLength: 100, dtype: int32\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\nconfusion_mat = confusion_matrix(test.Purchased, predicted_classes)\n# Define labels for the confusion matrix\nlabels = ['Actual Negative', 'Actual Positive']\n# Create a formatted confusion matrix\nformatted_confusion_mat = pd.DataFrame(confusion_mat, index=labels, columns=[f'Predicted {label}' for label in labels])\n\nprint(\"Confusion Matrix:\")\nprint(formatted_confusion_mat)\n\nConfusion Matrix:\n                 Predicted Actual Negative  Predicted Actual Positive\nActual Negative                         58                          5\nActual Positive                          9                         28\n\n\n\n\n8.4.4 Logistic Regression with Sklearn\n\nX_train = train[['Age']]\ny_train = train['Purchased']\n\nX_test = test[['Age']]\ny_test = test['Purchased']\n\n\n# turn off regularization\nskn_model = LogisticRegression(penalty=None)\n\n\nskn_model.fit(X_train, y_train)\n\nLogisticRegression(penalty=None)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(penalty=None) \n\n\n\n# Note that in sklearn, .predict returns the classes directly, with 0.5 threshold\ny_pred_test = skn_model.predict(X_test)\ny_pred_test\n\narray([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64)\n\n\n\n# To return the prediction probabilities, we need .predict_proba\n# # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) vs 1 (2nd column in array)\ny_pred_probs = skn_model.predict_proba(X_test)\ny_pred_probs[:5]\n\narray([[0.79634123, 0.20365877],\n       [0.95352574, 0.04647426],\n       [0.944647  , 0.055353  ],\n       [0.8717078 , 0.1282922 ],\n       [0.92191865, 0.07808135]])\n\n\n\ncm=confusion_matrix(y_test,y_pred_test)\n#plt.figure(figsize=(4,4))\nplt.title(\"Confusion Matrix on test data\")\nsns.heatmap(cm, annot=True,fmt='d', cmap='Blues')\nplt.ylabel(\"Actual Values\")\nplt.xlabel(\"Predicted Values\")\n\nText(0.5, 5.183333333333314, 'Predicted Values')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\nfrom sklearn.metrics import precision_score\nprint(\"Precision:\", precision_score(y_test, y_pred_test))\nfrom sklearn.metrics import recall_score\nprint(\"Recall:\", recall_score(y_test, y_pred_test))\nfrom sklearn.metrics import f1_score\nprint(\"F1 score:\",  f1_score(y_test, y_pred_test))\n\nAccuracy: 0.86\nPrecision: 0.8484848484848485\nRecall: 0.7567567567567568\nF1 score: 0.8\n\n\n\n\n8.4.5 Changing the default threshold\n\nnew_threshold = 0.3\n\n\npredicted_classes_new_threshold = (y_pred_probs &gt; new_threshold).astype(int)\npredicted_classes_new_threshold[:5]\n\narray([[1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0],\n       [1, 0]])\n\n\n\nconfusion_mat_new_threshold = confusion_matrix(y_test, predicted_classes_new_threshold[:, 1])\nprint(\"Confusion Matrix (Threshold =\", new_threshold, \"):\")\nprint(confusion_mat_new_threshold)\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\", accuracy_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import precision_score\nprint(\"Precision:\", precision_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import recall_score\nprint(\"Recall:\", recall_score(y_test, predicted_classes_new_threshold[:, 1]))\nfrom sklearn.metrics import f1_score\nprint(\"F1 score:\",  f1_score(y_test, predicted_classes_new_threshold[:, 1]))\n\nConfusion Matrix (Threshold = 0.3 ):\n[[44 19]\n [ 7 30]]\nAccuracy: 0.74\nPrecision: 0.6122448979591837\nRecall: 0.8108108108108109\nF1 score: 0.6976744186046512",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#performance-measurement",
    "href": "Logistic Regression.html#performance-measurement",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "8.5 Performance Measurement",
    "text": "8.5 Performance Measurement\nWe have already seen the confusion matrix, and classification accuracy. Now, let us see some other useful performance metrics that can be computed from the confusion matrix. The metrics below are computed for the confusion matrix immediately above this section (or the confusion matrix on test data corresponding to the model logit_model_diabetes).\n\n8.5.1 Precision-recall\nPrecision measures the accuracy of positive predictions. Also called the precision of the classifier\n\\[\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}\\]\n==&gt; 70.13%\nPrecision is typically used with recall (Sensitivity or True Positive Rate). The ratio of positive instances that are correctly detected by the classifier.\n\\(\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}\\) ==&gt; 88.52%\nPrecision / Recall Tradeoff: Increasing precision reduces recall and vice versa.\nVisualize the precision-recall curve for the model logit_model_diabetes.\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\n\n\n296\n15701537\nMale\n42\n149000\n1\n\n\n297\n15807481\nMale\n28\n79000\n0\n\n\n298\n15603942\nFemale\n51\n134000\n0\n\n\n299\n15690188\nFemale\n33\n28000\n0\n\n\n\n\n300 rows √ó 5 columns\n\n\n\n\ny=train.Purchased\nypred = lm.predict(train)\np, r, thresholds = precision_recall_curve(y, ypred)\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.figure(figsize=(8, 8))\n    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.ylabel(\"Score\")\n    plt.xlabel(\"Decision Threshold\")\n    plt.legend(loc='best')\n    plt.legend()\nplot_precision_recall_vs_threshold(p, r, thresholds)\n\n\n\n\n\n\n\n\nAs the decision threshold probability increases, the precision increases, while the recall decreases.\nQ: How are the values of the thresholds chosen to make the precision-recall curve?\nHint: Look at the documentation for precision_recall_curve.\n\n\n8.5.2 The Receiver Operating Characteristics (ROC) Curve\nA ROC(Receiver Operator Characteristic Curve) is a plot of sensitivity (True Positive Rate) on the y axis against (1‚àíspecificity) (False Positive Rate) on the x axis for varying values of the threshold t. The 45¬∞ diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to random chance. The ROC curve for the gold standard is the line connecting (0,0) to (0,1) and (0,1) to (1,1).\n\n\n\n\n\nHigh Threshold:\n\nHigh specificity\nLow sensitivity\n\nLow Threshold\n\nLow specificity\nHigh sensitivity\n\nThe area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a good blog link.\nHere is good post by google developers on interpreting ROC-AUC, and its advantages / disadvantages.\nVisualize the ROC curve and compute the ROC-AUC for the model logit_model_diabetes.\n\ny=train.Purchased\nypred = lm.predict(train)\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nprint(auc(fpr, tpr))# AUC of ROC\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.figure(figsize=(8,8))\n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.005, 1, 0, 1.005])\n    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate (Recall)\")\n\nfpr, tpr, auc_thresholds = roc_curve(y, ypred)\nplot_roc_curve(fpr, tpr)\n\n0.8593901964598327\n\n\n\n\n\n\n\n\n\nQ: How are the values of the auc_thresholds chosen to make the ROC curve? Why does it look like a step function?\nBelow is a function that prints the confusion matrix along with all the performance metrics we discussed above for a given decision threshold probability, on train / test data. Note that ROC-AUC does not depend on a decision threshold probability.\n\n#Function to compute confusion matrix and prediction accuracy on test/train data\ndef confusion_matrix_data(data,actual_values,model,cutoff=0.5):\n#Predict the values using the Logit model\n    pred_values = model.predict(data)\n# Specify the bins\n    bins=np.array([0,cutoff,1])\n#Confusion matrix\n    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n    cm_df = pd.DataFrame(cm)\n    cm_df.columns = ['Predicted 0','Predicted 1']\n    cm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\n# Calculate the accuracy\n    accuracy = (cm[0,0]+cm[1,1])/cm.sum()\n    fnr = (cm[1,0])/(cm[1,0]+cm[1,1])\n    precision = (cm[1,1])/(cm[0,1]+cm[1,1])\n    fpr = (cm[0,1])/(cm[0,0]+cm[0,1])\n    tpr = (cm[1,1])/(cm[1,0]+cm[1,1])\n    fpr_roc, tpr_roc, auc_thresholds = roc_curve(actual_values, pred_values)\n    auc_value = (auc(fpr_roc, tpr_roc))# AUC of ROC\n    sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='g')\n    plt.ylabel(\"Actual Values\")\n    plt.xlabel(\"Predicted Values\")\n    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n    print(\"Precision = {:.1%}\".format(precision))\n    print(\"TPR or Recall = {:.1%}\".format(tpr))\n    print(\"FNR = {:.1%}\".format(fnr))\n    print(\"FPR = {:.1%}\".format(fpr))\n    print(\"ROC-AUC = {:.1%}\".format(auc_value))\n\n\nconfusion_matrix_data(test,test.Purchased,lm,0.3)\n\nClassification accuracy = 68.2%\nPrecision = 58.3%\nTPR or Recall = 94.6%\nFNR = 5.4%\nFPR = 52.1%\nROC-AUC = 89.4%",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression.html#the-receiver-operating-characteristics-roc-curve-1",
    "href": "Logistic Regression.html#the-receiver-operating-characteristics-roc-curve-1",
    "title": "8¬† Logistic regression: Introduction and Metrics",
    "section": "9.1 The Receiver Operating Characteristics (ROC) Curve",
    "text": "9.1 The Receiver Operating Characteristics (ROC) Curve\nA ROC(Receiver Operator Characteristic Curve) is a plot of sensitivity (True Positive Rate) on the y axis against (1‚àíspecificity) (False Positive Rate) on the x axis for varying values of the threshold t. The 45¬∞ diagonal line connecting (0,0) to (1,1) is the ROC curve corresponding to random chance. The ROC curve for the gold standard is the line connecting (0,0) to (0,1) and (0,1) to (1,1).\n\nHigh Threshold: * High specificity * Low sensitivity\nLow Threshold * Low specificity * High sensitivity\nThe area under ROC is called Area Under the Curve(AUC). AUC gives the rate of successful classification by the logistic model. To get a more in-depth idea of what a ROC-AUC curve is and how is it calculated, here is a link\n\nfrom sklearn.metrics import roc_curve\n\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\nplt.figure(figsize=(9,6)); \nplot_roc_curve(fpr, tpr)\nplt.show();\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, y_pred_test)\n\n0.8386958386958387",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Logistic regression: Introduction and Metrics</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html",
    "title": "9¬† Logistic regression: Others",
    "section": "",
    "text": "9.1 Decision Boundary in Classification\n# Importing the dataset\ndataset = pd.read_csv('datasets/apples_and_oranges.csv')\ndataset.head()\n\n\n\n\n\n\n\n\nWeight\nSize\nClass\n\n\n\n\n0\n69\n4.39\norange\n\n\n1\n69\n4.21\norange\n\n\n2\n65\n4.09\norange\n\n\n3\n72\n5.85\napple\n\n\n4\n67\n4.70\norange\n# No. of apples and oranges\ndataset['Class'].value_counts()\n\nClass\norange    20\napple     20\nName: count, dtype: int64",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#decision-boundary-in-classification",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#decision-boundary-in-classification",
    "title": "9¬† Logistic regression: Others",
    "section": "",
    "text": "9.1.1 Encoding Target\n\nle = LabelEncoder()\ndataset['Class'] = le.fit_transform(dataset['Class'])\nle.classes_\n\narray(['apple', 'orange'], dtype=object)\n\n\nThis implies that, * 0 represents Apple * 1 represents Orange\n\ndataset.head()\n\n\n\n\n\n\n\n\nWeight\nSize\nClass\n\n\n\n\n0\n69\n4.39\n1\n\n\n1\n69\n4.21\n1\n\n\n2\n65\n4.09\n1\n\n\n3\n72\n5.85\n0\n\n\n4\n67\n4.70\n1\n\n\n\n\n\n\n\n\n\n9.1.2 Plotting the dataset\n\nplt.figure(figsize=(9,6))\nplt.title('Apples and Oranges', fontweight='bold', fontsize=16)\nplt.xlabel('Weight')\nplt.ylabel('Size')\nscatter = plt.scatter(dataset['Weight'], dataset['Size'], c=dataset['Class'], cmap='viridis')\nplt.legend(*scatter.legend_elements(),\n           loc = 'upper left',\n           title = 'Class');\n\n\n\n\n\n\n\n\nWe can observe that oranges have lower weight and size compared to apples. Further by drawing a straight line between these two groups of data points, we can clearly distinguish between apples and oranges.\n\n\n9.1.3 Building a Logistic Regression model to distinguish apples and oranges\nAs we can clearly distinguish between apples and oranges using a straight line decision boundary, we can choose the hypothesis y = a0 + a1  x1 + a2 * x2* for Logistic Regression where, a0, a1, a2 are the fitting parameters x1 is Weight x2 is Size\n\n# Defining target and features\ny = dataset['Class']\nx = dataset.drop(columns=['Class'])\n\n\n# Creating object of LogisticRegression class\nlog_reg = LogisticRegression()\n\n\n# Fitting parameters\nlog_reg.fit(x,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Intercept - a0\nlog_reg.intercept_\n\narray([106.60287324])\n\n\n\n# Coefficients - a1, a2 respectively\nlog_reg.coef_\n\narray([[-1.42833694, -1.31285258]])\n\n\n\n# Predicting labels for the given dataset\nlabel_predictions = log_reg.predict(x)\nlabel_predictions\n\narray([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0])\n\n\n\n\n9.1.4 Linear Decision Boundary with naive features\n\n# Parameter values\na0 = log_reg.intercept_[0]\na1 = log_reg.coef_[0][0]\na2 = log_reg.coef_[0][1]\n\n\n# Defining x1 and x2 values for decision boundary\nx1 = np.array([69, 71])\nx2 = - (a0 / a2) - (a1 / a2) * x1\n\n\n# Plotting the decision boundary\nplt.figure(figsize=(9,6))\nplt.title('Apples and Oranges', fontweight='bold', fontsize=16)\nplt.xlabel('Weight')\nplt.ylabel('Size')\nscatter = plt.scatter(dataset['Weight'], dataset['Size'], c=dataset['Class'], cmap='viridis')\nplt.legend(*scatter.legend_elements(),\n           loc = 'upper left',\n           title = 'Class')\nplt.plot(x1, x2, color='red', label='Decision Boundary')\nplt.show()\n\n\n\n\n\n\n\n\nIn this problem, we have just two features x1 and x2, if we use just those as they are we will end up with a straight line which divides our 2D plane into two half-planes.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#non-linear-decision-boundary",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#non-linear-decision-boundary",
    "title": "9¬† Logistic regression: Others",
    "section": "9.2 Non-linear Decision Boundary",
    "text": "9.2 Non-linear Decision Boundary\nIn some occasions, we want to have a more complex boundary, and we can achieve this by transforming our features. For instance, when confronted with a training data distribution as illustrated below,  it becomes imperative to generate polynomial features \\[(x_1^2, x_2^2)\\] in order to enhance the delineation between the two classes. \nLet‚Äôs delve into a concrete example below to illustrate this concept.For the purpose of illustrating the decision boundary, I chose not to split the data into training and test sets.\n\n#Load our Dataset for Logistic Regression\ncomponents = pd.read_csv('datasets/ex2data2.txt', header=None, names = ['feature 1', 'feature 2', 'faulty'])\ncomponents.head()\n\n\n\n\n\n\n\n\nfeature 1\nfeature 2\nfaulty\n\n\n\n\n0\n0.051267\n0.69956\n1\n\n\n1\n-0.092742\n0.68494\n1\n\n\n2\n-0.213710\n0.69225\n1\n\n\n3\n-0.375000\n0.50219\n1\n\n\n4\n-0.513250\n0.46564\n1\n\n\n\n\n\n\n\n\n# check the balance of the dataset\ncomponents['faulty'].value_counts()\n\nfaulty\n0    60\n1    58\nName: count, dtype: int64\n\n\n\n# get positive and negative samples for plotting\npos = components['faulty'] == 1\nneg = components['faulty'] == 0\n\n\n# Visualize Data\nfig, axes = plt.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Non Faculty')\naxes.legend(title='Legend', loc = 'best' )\naxes.set_xlim(-1,1.5)\naxes.set_xlim(-1,1.5)\n\n\n\n\n\n\n\n\nAs we can see that the positive and negative examples are not linearly seperable. So we have to add additional higher order polynomial features.\n\n# define function to map higher order polynomial features\ndef mapFeature(X1, X2, degree):\n    res = np.ones(X1.shape[0])\n    for i in range(1,degree + 1):\n        for j in range(0,i + 1):\n            res = np.column_stack((res, (X1 ** (i-j)) * (X2 ** j)))\n    \n    return res\n\n\n# Get the features \nX = components.iloc[:, :2]\n\n\ndegree = 2\n\n\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n\n\n# Get the target variable\ny = components.iloc[:, 2]\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\ndef costFunc(theta, X, y):\n    m = y.shape[0]\n    z = X.dot(theta)\n    h = sigmoid(z)\n    term1 = y * np.log(h)\n    term2 = (1- y) * np.log(1 - h)\n    J = -np.sum(term1 + term2, axis = 0) / m\n    return J \n\n\n# Set initial values for our parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)\n\n\n# Now call the optimization routine\n#NOTE: This automatically picks the learning rate\nfrom scipy.optimize import minimize\nres = minimize(costFunc, initial_theta.flatten(), args=(X_poly, y))\n\n\n# our optimizated coefficients\ntheta = res.x\n\n\n# define a function to plot the decision boundary\ndef plotDecisionBoundary(theta,degree, axes):\n    u = np.linspace(-1, 1.5, 50)\n    v = np.linspace(-1, 1.5, 50)\n    U,V = np.meshgrid(u,v)\n    # convert U, V to vectors for calculating additional features\n    # using vectorized implementation\n    U = np.ravel(U)\n    V = np.ravel(V)\n    Z = np.zeros((len(u) * len(v)))\n    \n    X_poly = mapFeature(U, V, degree)\n    Z = X_poly.dot(theta)\n    \n    # reshape U, V, Z back to matrix\n    U = U.reshape((len(u), len(v)))\n    V = V.reshape((len(u), len(v)))\n    Z = Z.reshape((len(u), len(v)))\n    \n    cs = axes.contour(U,V,Z,levels=[0],cmap= \"Greys_r\")\n    axes.legend(labels=['Non Faculty', 'faulty', 'Decision Boundary'])\n    return cs\n\n\n# Plot Decision boundary\nfig, axes = plt.subplots();\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color = 'r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color = 'g', marker='o', label='Good')\n#axes.legend(title='Legend', loc = 'best' )\n\nplotDecisionBoundary(theta, degree, axes);\n\n\n\n\n\n\n\n\nof course, you can increase the degree of the polynomial you want to fit, but the overfitting could become a problem\n\n# set degree = 6\ndegree = 6\n# map features to the degree\nX_poly = mapFeature(X.iloc[:, 0], X.iloc[:, 1], degree)\n# set initial parameters\ninitial_theta = np.zeros(X_poly.shape[1]).reshape(X_poly.shape[1], 1)\n\n\n# Run the optimization function\nres = minimize(costFunc, initial_theta.flatten(), args=(X_poly, y))\ntheta = res.x.reshape(res.x.shape[0], 1)\n\n# Plot Decision boundary\nfig, axes = plt.subplots()\naxes.set_xlabel('Feature 1')\naxes.set_ylabel('Feature 2')\naxes.scatter(components.loc[pos, 'feature 1'], components.loc[pos, 'feature 2'], color='r', marker='x', label='Faulty')\naxes.scatter(components.loc[neg, 'feature 1'], components.loc[neg, 'feature 2'], color='g', marker='o', label='Good')\n#axes.legend(title='Legend', loc='best')\n\nplotDecisionBoundary(theta, degree, axes)\n\n\n\n\n\n\n\n\nAs we can see the model tries pretty hard to capture every single example perfectly and overfits the data. This kind of model has overfitting issue. i.e The model has not pre-conceived notion about the seperation of the positive and negative examples and pretty much can fit any kind of data. Such model will fail in predicting the correct classification when it sees new examples.\nOne of techiques is to use regularization, which we will cover later. The idea is to penalize the algorithm when it tries to overfit by adding a regularization term to the cost function.\nThe New Cost function with the regularization is specified as\n\\(J(\\theta ) = \\frac{1}{m} \\sum_{i=1}^m[-y_i log(h_\\theta (z_i) ‚Äì (1 ‚Äì y_i) log(1-h_\\theta (z_i))] +\n                    \\frac{\\lambda}{2m} \\sum_{j=1}^n[\\theta_j^2]\\)\nwhere \\(\\lambda\\) = regularization factor  n = number of features.  (NOTE: The regularization term does include the intercept term \\(\\theta_0\\)\n\n9.2.1 By adding Polynomial Features\n\ncomponents\n\n\n\n\n\n\n\n\nfeature 1\nfeature 2\nfaulty\n\n\n\n\n0\n0.051267\n0.699560\n1\n\n\n1\n-0.092742\n0.684940\n1\n\n\n2\n-0.213710\n0.692250\n1\n\n\n3\n-0.375000\n0.502190\n1\n\n\n4\n-0.513250\n0.465640\n1\n\n\n...\n...\n...\n...\n\n\n113\n-0.720620\n0.538740\n0\n\n\n114\n-0.593890\n0.494880\n0\n\n\n115\n-0.484450\n0.999270\n0\n\n\n116\n-0.006336\n0.999270\n0\n\n\n117\n0.632650\n-0.030612\n0\n\n\n\n\n118 rows √ó 3 columns\n\n\n\n\nX = components[['feature 1', 'feature 2']]\ny = components['faulty']\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)\n\n\nfeature_names = poly.get_feature_names_out()\nprint(feature_names)\n\n['1' 'feature 1' 'feature 2' 'feature 1^2' 'feature 1 feature 2'\n 'feature 2^2']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_poly, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nlabel_predictions = model.predict(X_poly)\n\n\n9.2.1.1 Accuracy Score\n\naccuracy_score(y, label_predictions)\n\n0.8135593220338984\n\n\n\n\n9.2.1.2 Confusion Matrix\n\ncm = confusion_matrix(y, label_predictions)\ncm\n\narray([[47, 13],\n       [ 9, 49]], dtype=int64)\n\n\n\n\n\n9.2.2 By Transforming Continuous Variable\nVariable transformation is an important technique to create robust models using logistic regression. Because the predictors are linear in the log of the odds, it is often helpful to transform the continuous variables to create a more linear relationship. To determine the best transformation of a continuous variable, a univariate plot is very helpful. Remember the nice univariate plot of Y variable against X variable in linear regression? This is not easily attained, because Y is dichotomous in logistic regression.\nThere are different recommended solutions. Among them, * One is to create several variations (in forms of squared, cubed, or logged transformations etc.). * Another solution is to break some continuous variables into segments and treat them as categorical variables. This may work well to pick up nonlinear trends. The biggest drawback is that it loses the benefit of the linear trend relationship in the curve1. It also may lead to over fitting.\n\ntrain = pd.read_csv('./Datasets/Social_Network_Ads_train.csv') #Develop the model on train data\ntest = pd.read_csv('./Datasets/Social_Network_Ads_test.csv') #Test the model on test data\n\n\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\n\n\n3\n15665760\nMale\n39\n122000\n1\n\n\n4\n15794661\nFemale\n26\n118000\n0\n\n\n\n\n\n\n\n\ntrain.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 300 entries, 0 to 299\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   User ID          300 non-null    int64 \n 1   Gender           300 non-null    object\n 2   Age              300 non-null    int64 \n 3   EstimatedSalary  300 non-null    int64 \n 4   Purchased        300 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 11.8+ KB\n\n\n\ntrain.Gender.value_counts()\n\nGender\nFemale    151\nMale      149\nName: count, dtype: int64\n\n\n\ntmp_1 = pd.get_dummies(train['Gender'], drop_first=True)\ntrain = pd.concat([train, tmp_1], axis=1)\ntrain.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n\n\n\n\n\n\n\n\ntmp_1 = pd.get_dummies(test['Gender'], drop_first=True)\ntest = pd.concat([test, tmp_1], axis=1)\ntest.head()\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\n\n\n\n\n0\n15810944\nMale\n35\n20000\n0\nTrue\n\n\n1\n15668575\nFemale\n26\n43000\n0\nFalse\n\n\n2\n15603246\nFemale\n27\n57000\n0\nFalse\n\n\n3\n15694829\nFemale\n32\n150000\n1\nFalse\n\n\n4\n15697686\nMale\n29\n80000\n0\nTrue\n\n\n\n\n\n\n\n\n# Separating features and target on training set\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\"], axis = 1)\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nMale\n\n\n\n\n0\n36\n33000\nTrue\n\n\n1\n39\n61000\nFalse\n\n\n2\n36\n118000\nTrue\n\n\n3\n39\n122000\nTrue\n\n\n4\n26\n118000\nFalse\n\n\n...\n...\n...\n...\n\n\n295\n48\n96000\nFalse\n\n\n296\n42\n149000\nTrue\n\n\n297\n28\n79000\nTrue\n\n\n298\n51\n134000\nFalse\n\n\n299\n33\n28000\nFalse\n\n\n\n\n300 rows √ó 3 columns\n\n\n\n\n# Separating features and target on test set\ny_test = test.Purchased\nX_test = test.drop([\"Purchased\", \"Gender\", \"User ID\"], axis = 1)\nX_test\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nMale\n\n\n\n\n0\n35\n20000\nTrue\n\n\n1\n26\n43000\nFalse\n\n\n2\n27\n57000\nFalse\n\n\n3\n32\n150000\nFalse\n\n\n4\n29\n80000\nTrue\n\n\n...\n...\n...\n...\n\n\n95\n49\n39000\nFalse\n\n\n96\n47\n34000\nTrue\n\n\n97\n60\n42000\nTrue\n\n\n98\n39\n59000\nFalse\n\n\n99\n51\n23000\nTrue\n\n\n\n\n100 rows √ó 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model = LogisticRegression()\nsklearn_model.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n\ny_pred_test = sklearn_model.predict(X_test)\nprint('Accuracy of logistic regression on test set : {:.4f}'.format(accuracy_score(y_test, y_pred_test )))\n\nAccuracy of logistic regression on test set : 0.8800\n\n\n\n9.2.2.1 Log transformation of salary\n\n\nsns.histplot(train.EstimatedSalary)\n\n\n\n\n\n\n\n\n\ntrain[\"log_salary\"] = np.log(train[\"EstimatedSalary\"])\nsns.histplot(train.log_salary)\n\n\n\n\n\n\n\n\nThe reason for such transformations have nothing to do with their distribution. Instead, the reason has to do with the functional form of the effect. Say we want to know the effect of the number of publications on the probability of getting tenure. It is reasonable to believe that getting an extra publication when one has only 1 publication has more impact compared with getting an extra publication when one has already published 50 articles. The log transformation is one way to capture such a (testable) assumption of diminishing returns.\n\ntest[\"log_salary\"] = np.log(test[\"EstimatedSalary\"])\n\n\n# Separating features and target\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\"], axis = 1)\n\n\nX_train\n\n\n\n\n\n\n\n\nAge\nMale\nlog_salary\n\n\n\n\n0\n36\nTrue\n10.404263\n\n\n1\n39\nFalse\n11.018629\n\n\n2\n36\nTrue\n11.678440\n\n\n3\n39\nTrue\n11.711776\n\n\n4\n26\nFalse\n11.678440\n\n\n...\n...\n...\n...\n\n\n295\n48\nFalse\n11.472103\n\n\n296\n42\nTrue\n11.911702\n\n\n297\n28\nTrue\n11.277203\n\n\n298\n51\nFalse\n11.805595\n\n\n299\n33\nFalse\n10.239960\n\n\n\n\n300 rows √ó 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model_log = LogisticRegression()\nsklearn_model_log.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Separating features and target for the test dataset\ny_test_log = test.Purchased\nX_test_log = test.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\"], axis = 1)\n\n\ny_log_pred_test = sklearn_model_log.predict(X_test_log)\nprint('Accuracy of logistic regression after log transformation of salary on test set : {:.4f}'.format(accuracy_score( y_test_log, y_log_pred_test)))\n\nAccuracy of logistic regression after log transformation of salary on test set : 0.8300\n\n\n\n\n\n9.2.3 By Binning Continous Variables\n\nsns.histplot(data=train.Age)\n\n\n\n\n\n\n\n\n\nbins = [train.Age.min()-1, 25, 35, 48, train.Age.max()]\nlabels = ['Big Kid', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Purchased\", data=train)\nplt.show()\n\n\n\n\n\n\n\n\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n10.404263\nAdult\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n11.018629\nAdult\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n11.678440\nAdult\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n11.711776\nAdult\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n11.678440\nYoung Adult\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\nFalse\n11.472103\nAdult\n\n\n296\n15701537\nMale\n42\n149000\n1\nTrue\n11.911702\nAdult\n\n\n297\n15807481\nMale\n28\n79000\n0\nTrue\n11.277203\nYoung Adult\n\n\n298\n15603942\nFemale\n51\n134000\n0\nFalse\n11.805595\nSenior\n\n\n299\n15690188\nFemale\n33\n28000\n0\nFalse\n10.239960\nYoung Adult\n\n\n\n\n300 rows √ó 8 columns\n\n\n\n\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \ntrain['AgeGroup']= label_encoder.fit_transform(train['AgeGroup']) \ntrain['AgeGroup'].unique() \n\narray([0, 3, 1, 2])\n\n\n\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\ntest['AgeGroup']= label_encoder.fit_transform(test['AgeGroup']) \n\n\ntrain\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\n15755018\nMale\n36\n33000\n0\nTrue\n10.404263\n0\n\n\n1\n15697020\nFemale\n39\n61000\n0\nFalse\n11.018629\n0\n\n\n2\n15796351\nMale\n36\n118000\n1\nTrue\n11.678440\n0\n\n\n3\n15665760\nMale\n39\n122000\n1\nTrue\n11.711776\n0\n\n\n4\n15794661\nFemale\n26\n118000\n0\nFalse\n11.678440\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n295\n15724536\nFemale\n48\n96000\n1\nFalse\n11.472103\n0\n\n\n296\n15701537\nMale\n42\n149000\n1\nTrue\n11.911702\n0\n\n\n297\n15807481\nMale\n28\n79000\n0\nTrue\n11.277203\n3\n\n\n298\n15603942\nFemale\n51\n134000\n0\nFalse\n11.805595\n2\n\n\n299\n15690188\nFemale\n33\n28000\n0\nFalse\n10.239960\n3\n\n\n\n\n300 rows √ó 8 columns\n\n\n\n\n# Separating features and target on train set\ny_train = train.Purchased\nX_train = train.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\", \"Age\"], axis = 1)\nX_train\n\n\n\n\n\n\n\n\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\nTrue\n10.404263\n0\n\n\n1\nFalse\n11.018629\n0\n\n\n2\nTrue\n11.678440\n0\n\n\n3\nTrue\n11.711776\n0\n\n\n4\nFalse\n11.678440\n3\n\n\n...\n...\n...\n...\n\n\n295\nFalse\n11.472103\n0\n\n\n296\nTrue\n11.911702\n0\n\n\n297\nTrue\n11.277203\n3\n\n\n298\nFalse\n11.805595\n2\n\n\n299\nFalse\n10.239960\n3\n\n\n\n\n300 rows √ó 3 columns\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nsklearn_model_bin = LogisticRegression()\nsklearn_model_bin.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Separating features and target on test set\ny_test = test.Purchased\nX_test_bin = test.drop([\"Purchased\", \"Gender\", \"User ID\", \"EstimatedSalary\", \"Age\"], axis = 1)\nX_test_bin\n\n\n\n\n\n\n\n\nMale\nlog_salary\nAgeGroup\n\n\n\n\n0\nTrue\n9.903488\n3\n\n\n1\nFalse\n10.668955\n3\n\n\n2\nFalse\n10.950807\n3\n\n\n3\nFalse\n11.918391\n3\n\n\n4\nTrue\n11.289782\n3\n\n\n...\n...\n...\n...\n\n\n95\nFalse\n10.571317\n2\n\n\n96\nTrue\n10.434116\n0\n\n\n97\nTrue\n10.645425\n2\n\n\n98\nFalse\n10.985293\n0\n\n\n99\nTrue\n10.043249\n2\n\n\n\n\n100 rows √ó 3 columns\n\n\n\n\ny_bin_pred_test = sklearn_model_bin.predict(X_test_bin)\nprint('Accuracy of logistic regression after age binning on test set : {:.4f}'.format(accuracy_score( y_test, y_bin_pred_test)))\n\nAccuracy of logistic regression after age binning on test set : 0.7100",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#reference",
    "href": "Logistic Regression - Decision Boundary and Feature Engineering (Nonlinear Features).html#reference",
    "title": "9¬† Logistic regression: Others",
    "section": "9.3 Reference",
    "text": "9.3 Reference\n\nhttps://www.linkedin.com/pulse/generating-non-linear-decision-boundaries-using-logistic-d-urso/\nhttps://jermwatt.github.io/machine_learning_refined/notes/10_Nonlinear_intro/10_4_Twoclass.html\nhttps://www.kaggle.com/code/lzs0047/logistic-regression-non-linear-decision-boundary/edit\nhttps://www.kaggle.com/code/ashishrane7/logistic-regression-non-linear-decision-boundary/notebook",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Logistic regression: Others</span>"
    ]
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "10¬† Cross-Validation",
    "section": "",
    "text": "10.1 Review: Train-Test Split and Its Limitations",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#review-train-test-split-and-its-limitations",
    "href": "cross_validation.html#review-train-test-split-and-its-limitations",
    "title": "10¬† Cross-Validation",
    "section": "",
    "text": "10.1.1 Train-Test Split Recap\nThroughout this course, we have used the train-test split approach to evaluate models:\n\nWe split the dataset into training and testing sets.\nThe training set is used to fit the model.\nThe testing set is used to evaluate performance on unseen data.\n\nThis approach provides a simple yet effective way to estimate out-of-sample performance. However, it has limitations:\n\n\n10.1.2 Limitations of Train-Test Split\n\nHigh Variance: The model‚Äôs performance depends on which specific observations end up in the training and testing sets. A different split might lead to different results.\nData Efficiency: A portion of the dataset is reserved for testing, meaning the model is not trained on all available data, which can be problematic for small datasets.\nInstability: A single split does not always provide a robust estimate of performance, especially when data is imbalanced or noisy.\n\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n\nfrom sklearn.datasets import make_classification\n\n%matplotlib inline\nplt.style.use('ggplot')\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 5, 4\n\n\n# Generate synthetic dataset for binary classification\nX, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n\nPlease execute the cell below multiple times. It will be evident that the accuracy score varies with each run due to different observations in both the training and test sets.\n\n# use train/test split \nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X, y)\n\n# Define the model\nlogit_model = LogisticRegression()\nlogit_model.fit(X_train_class, y_train_class)\ny_pred = logit_model.predict(X_test_class)\nprint(accuracy_score(y_test_class, y_pred))\n\n0.856",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-key-concepts",
    "href": "cross_validation.html#cross-validation-key-concepts",
    "title": "10¬† Cross-Validation",
    "section": "10.2 Cross-Validation: Key Concepts",
    "text": "10.2 Cross-Validation: Key Concepts\nTo address the limitations of train-test split, we introduce cross-validation, which provides a more reliable estimate of model performance by using multiple train-test splits.\n\n10.2.1 Steps for K-fold cross-validation\n\nSplit the dataset into K equal partitions (or ‚Äúfolds‚Äù).\nUse fold 1 as the testing set and the union of the other folds as the training set.\nCalculate testing accuracy.\nRepeat steps 2 and 3 K times, using a different fold as the testing set each time.\nUse the average testing accuracy as the estimate of out-of-sample accuracy.\n\nDiagram of 5-fold cross-validation:\n\n\n\n5-fold cross-validation\n\n\n\n# simulate splitting a dataset of 25 observations into 5 folds\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=False).split(range(25))\n\n# print the contents of each training and testing set\nprint('{} {:^61} {}'.format('Iteration', 'Training set observations', 'Testing set observations'))\nfor iteration, data in enumerate(kf, start=1):\n    print('{:^9} {} {:^25}'.format(iteration, data[0], str(data[1])))\n\nIteration                   Training set observations                   Testing set observations\n    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]       \n    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]       \n    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]     \n    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]     \n    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]     \n\n\nKey takeaways\n\nThe dataset consists of 25 observations (indexed from 0 to 24).\n\nWe use 5-fold cross-validation, meaning the process runs for 5 iterations (one per fold).\n\nIn each iteration:\n\nThe dataset is split into a training set and a testing set.\n\nEach observation is included in either the training set or the testing set, but never both simultaneously.\n\n\nOver the entire cross-validation process:\n\nEach observation appears in the testing set exactly once.\n\nEach observation is included in the training set for (K-1) = 4 iterations.\n\n\nThis ensures that every data point contributes to both model training and evaluation, improving the robustness of performance estimates.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-in-scikit-learn",
    "href": "cross_validation.html#cross-validation-in-scikit-learn",
    "title": "10¬† Cross-Validation",
    "section": "10.3 Cross-Validation in Scikit-Learn",
    "text": "10.3 Cross-Validation in Scikit-Learn\n\n10.3.1 cross_val_score\ncross_val_score is a function in Scikit-Learn that simplifies k-fold cross-validation for model evaluation. It automates the process of splitting the dataset, training the model, and computing performance metrics across multiple folds.\n\nBy default, it uses 5-fold cross-validation (cv=5).\nFor classification models, it evaluates performance using accuracy as the default scoring metric.\nFor regression models, it uses R¬≤ (coefficient of determination) by default.\nThe function returns an array of scores, one for each fold, providing a more reliable estimate of model performance than a single train-test split.\n\nUsing cross_val_score ensures a more robust evaluation by reducing variance and making better use of available data.\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(logit_model, X, y)\n\nprint(scores)\n\n[0.87  0.855 0.85  0.83  0.875]\n\n\nFinally, we compute the mean performance score across all folds to obtain a robust evaluation.\n\n# get the mean score\nscores.mean()\n\n0.8560000000000001\n\n\nBy default, cross_val_score in Scikit-Learn performs k-fold cross-validation using the default cross-validator for the given estimator type.\n\n10.3.1.1 Default k-fold cross-validation Settings\ncv=5 unless specified\n\n\n\n\n\n\n\n\n\nModel Type\nDefault Cross-Validator\nShuffling\nStratification\n\n\n\n\nClassification\nStratifiedKFold(n_splits=5)\n‚ùå No\n‚úÖ Yes\n\n\nRegression\nKFold(n_splits=5)\n‚ùå No\n‚ùå No\n\n\n\n\n\n10.3.1.2 How to Modify the Behavior\nIf you need shuffling or a different cross-validation strategy, specify a custom cross-validator.\n\n10.3.1.2.1 Enable Shuffling\n\n# Turn on the shuffle\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(logit_model, X, y, cv=kf, scoring='accuracy')\nscores\n\narray([0.83 , 0.875, 0.87 , 0.85 , 0.865])\n\n\n\n\n10.3.1.2.2 Stratified Splitting for Classification\nFor classification problems, stratified sampling is recommended for creating the folds\n\nEach response class should be represented with equal proportions in each of the K folds\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(logit_model, X, y, cv=skf, scoring='accuracy')\nscores\n\narray([0.83 , 0.84 , 0.875, 0.89 , 0.845])\n\n\n\n\n10.3.1.2.3 Use Leave-One-Out (LOO) Cross-Validation\nIf you don‚Äôt have much data, so any split from the full set to the training and validation set is going to result in really very few observations on which you can train. Leave-on-out cross validation (LOOCV) that might work better for cross-validation. Say you have 16 observations. Train on 15 and validate on the other one. Repeat this until you have trained on every set of 15 with the 16th sample left out.\n\nfrom sklearn.model_selection import LeaveOneOut\n\nloo = LeaveOneOut()\nscores = cross_val_score(logit_model, X, y, cv=loo, scoring='accuracy')\nprint(len(scores))\nscores[:10]\n\n1000\n\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\nTypes of Cross-Validation\n\n\n\n\n\n\n\n\nMethod\nDescription\nUse Case\n\n\n\n\nk-Fold CV\nSplits data into k folds, each used for training/testing\nMost common approach (e.g., k=5 or k=10)\n\n\nStratified k-Fold\nLike k-Fold, but preserves class proportions\nUseful for imbalanced classification\n\n\nLeave-One-Out (LOO-CV)\nUses each sample as a test set once\nFor very small datasets\n\n\nLeave-P-Out (LPO-CV)\nLeaves out p samples for testing in each iteration\nComputationally expensive\n\n\nTime Series CV\nEnsures training data precedes test data (rolling windows)\nTime-series forecasting problems\n\n\n\n\n\n\n10.3.1.3 Changing the Scoring Metric in cross_val_score\nwe can specify a different evaluation metric using the scoring parameter. Scikit-Learn provides various built-in metrics, including F1-score, precision, recall, and more.\nYou can specify different metrics based on the type of model:\n\n\n\n\n\n\n\n\nTask Type\nMetric Name (for scoring)\nDescription\n\n\n\n\nClassification\n'accuracy'\nDefault, ratio of correct predictions\n\n\nClassification\n'precision', 'recall'\nMeasure of correctness for positive class\n\n\nClassification\n'f1', 'f1_macro'\nHarmonic mean of precision & recall\n\n\nRegression\n'neg_mean_squared_error'\nMSE (lower is better)\n\n\nRegression\n'r2'\nDefault, measures variance explained\n\n\n\nYou can refer to the scikit-learn documentation. If a built-in metric doesn‚Äôt fit your needs, you can define a custom scoring function\nFor classification problems, especially imbalanced datasets, F1-score is a better metric than accuracy as it considers both precision and recall. Let‚Äôs use it as our metric next\n\n# create a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\nscores = cross_val_score(logit_model, X, y, cv=folds, scoring= \"f1\")\nscores \n\narray([0.875     , 0.81632653, 0.88151659, 0.87958115, 0.85869565])\n\n\n\n\n\n10.3.2 cross_validate in Scikit-Learn\nThe cross_validate function provides a more comprehensive evaluation of a model compared to cross_val_score.\nIts Key Feature includes:\n\nAllows multiple evaluation metrics to be specified at once.\nReturns a detailed dictionary containing:\n\nTraining scores and test scores across different folds.\nFit times (time taken to train the model for each fold).\nScore times (time taken to evaluate the model for each fold).\n\nUseful for:\n\nAnalyzing training/testing time variability across folds.\nComparing multiple performance metrics simultaneously to get a more complete picture of model performance.\n\n\nThis function is ideal when you need deeper insights into how your model behaves across different folds, beyond just performance scores.\n\nfrom sklearn.model_selection import cross_validate\n\nscores = cross_validate(logit_model, X, y, scoring=\"accuracy\", return_train_score=True)\n \ndf_scores = pd.DataFrame(scores)\ndf_scores\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_score\ntrain_score\n\n\n\n\n0\n0.007137\n0.001002\n0.870\n0.86125\n\n\n1\n0.006532\n0.000997\n0.855\n0.86750\n\n\n2\n0.004417\n0.000998\n0.850\n0.87375\n\n\n3\n0.001992\n0.001005\n0.830\n0.87000\n\n\n4\n0.001035\n0.000000\n0.875\n0.85750\n\n\n\n\n\n\n\nLet‚Äôs use multiple metrics\n\n# Define scoring metrics explicitly for multiclass\nscoring = ['accuracy', 'recall', 'precision', 'f1', 'roc_auc']\n\n# Perform cross-validation\nscores = cross_validate(logit_model, X, y, scoring=scoring, return_train_score=True)\n\n# Convert to DataFrame for better readability\ndf_scores = pd.DataFrame(scores)\ndf_scores\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_accuracy\ntrain_accuracy\ntest_recall\ntrain_recall\ntest_precision\ntrain_precision\ntest_f1\ntrain_f1\ntest_roc_auc\ntrain_roc_auc\n\n\n\n\n0\n0.003802\n0.000000\n0.870\n0.86125\n0.868687\n0.852500\n0.868687\n0.867684\n0.868687\n0.860025\n0.941394\n0.931588\n\n\n1\n0.004556\n0.004991\n0.855\n0.86750\n0.810000\n0.862155\n0.890110\n0.870886\n0.848168\n0.866499\n0.944400\n0.931368\n\n\n2\n0.003989\n0.004989\n0.850\n0.87375\n0.840000\n0.867168\n0.857143\n0.878173\n0.848485\n0.872636\n0.926300\n0.936212\n\n\n3\n0.003987\n0.004063\n0.830\n0.87000\n0.840000\n0.862155\n0.823529\n0.875318\n0.831683\n0.868687\n0.905200\n0.940987\n\n\n4\n0.000000\n0.011978\n0.875\n0.85750\n0.890000\n0.847118\n0.864078\n0.864450\n0.876847\n0.855696\n0.940200\n0.932781\n\n\n\n\n\n\n\n\n\n10.3.3 cross_val_predict in sklearn\ncross_val_predict is a function in Scikit-Learn that performs cross-validation but instead of returning evaluation scores, it returns predicted values for each instance in the dataset as if they were unseen.\n\n10.3.3.1 Process:\n\nThe dataset is split into k folds.\nThe model is trained on k-1 folds.\nPredictions are made on the remaining fold.\nThis process repeats until every instance has been predicted once, ensuring that each prediction is made on unseen data.\n\nThis approach mimics real-world predictions, making it useful for evaluating model performance with classification reports, confusion matrices, and ROC curves.\n\nfrom sklearn.model_selection import cross_val_predict\n\n# output the predicted probabilities\ny_pred_prob = cross_val_predict(logit_model, X, y, cv=5, method='predict_proba')[:,1]\nprint(y_pred_prob.shape)\nprint(y_pred_prob[:10])\n\n(1000,)\n[0.04211674 0.97024937 0.96688462 0.01087727 0.65634726 0.02306063\n 0.14247751 0.92610262 0.8783967  0.2408313 ]\n\n\n\n\n\n10.3.4 Key Differences Between cross_val_score, cross_validate, and cross_val_predict\n\n\n\n\n\n\n\n\n\n\n\nFunction\nReturns\nRequires scoring?\nSupports Multiple Metrics?\nCan Return Train Scores?\nPurpose\n\n\n\n\ncross_val_score\nArray of scores\n‚ùå No (defaults to accuracy for classification)\n‚ùå No\n‚ùå No\nEvaluates model performance using cross-validation\n\n\ncross_validate\nDictionary\n‚úÖ Yes (must specify)\n‚úÖ Yes (scoring={'accuracy', 'precision'})\n‚úÖ Yes (return_train_score=True)\nProvides detailed evaluation metrics, including training times\n\n\ncross_val_predict\nArray of predictions\n‚ùå No (uses predict or predict_proba)\n‚ùå No\n‚ùå No\nGenerates out-of-sample predictions for each instance\n\n\n\n\n\n10.3.5 Advantages and Disadvantages of Cross-Validation\n\n10.3.5.1 Advantages:\n\nReduces Variance: By averaging results over multiple folds, cross-validation provides a more stable estimate of model performance.\n\nBetter Use of Data: Every observation gets a chance to be in both the training and testing sets, improving data efficiency.\n\nMore Reliable Performance Metrics: The results are less dependent on a single random split, making the evaluation more robust.\n\n\n\n10.3.5.2 Disadvantages Compared to Train-Test Split:\n\nHigher Computational Cost: Instead of training the model once (as in a train-test split), cross-validation requires training the model k times, making it computationally expensive for large datasets.\n\nPotential Overfitting to Small Data: If k is too large (e.g., Leave-One-Out Cross-Validation), it can lead to high variance and make the model too sensitive to small changes in data.\n\nChoosing k: Typically, k=5 or k=10 is recommended as a balance between bias and variance.\n\nCross-validation is preferable for small-to-moderate datasets where stable performance estimates are important, while a simple train-test split is often sufficient for large datasets when computational efficiency is a priority.",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#cross-validation-for-hyperparameter-tuning",
    "href": "cross_validation.html#cross-validation-for-hyperparameter-tuning",
    "title": "10¬† Cross-Validation",
    "section": "10.4 Cross-Validation for Hyperparameter Tuning",
    "text": "10.4 Cross-Validation for Hyperparameter Tuning\nCross-validation is a powerful technique that can be used for:\n\nModel Performance Evaluation: Provides a more reliable estimate of how well a model generalizes to unseen data.\nHyperparameter Tuning: Helps find the optimal model parameters by evaluating different configurations.\n\nModel Selection: Compares multiple models to choose the one that performs best across different folds.\n\nFeature Selection: Assesses the impact of different feature subsets on model performance.\n\nIn this notebook, we focus exclusively on Hyperparameter Tuning with Cross-Validation.\nWe demonstrate how cross-validation can be used to systematically search for the best hyperparameters, ensuring better generalization and optimized model performance.\n\n10.4.1 Finding the optimal Degree in Polynomial Regression\n\n10.4.1.1 Background: Polynomial Regression\nYou already know simple linear regression:\n\\(y = \\beta_0 + \\beta_1 x_1\\)\nIn polynomial regression of degree \\(n\\), we fit a curve of the form:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_1^2 + \\beta_3x_1^3 ... + \\beta_nx_1^n\\)\nIn the experiment below, we fit polynomial models of varying degrees on simulated data to analyze their performance.\nWe build a linear regression model and use cross-validation to tune the polynomial degree (p).\nBy selecting the optimal degree, we aim to balance the trade-off between underfitting and overfitting, ensuring the model generalizes well to unseen data.\nSpecifically, we Will Cover:\n\nUsing cross_val_score for hyperparameter tuning to evaluate model performance across folds.\n\nUsing cross_validate to obtain detailed metrics, including training scores and fit times.\n\nApplying GridSearchCV to systematically search for the optimal polynomial degree.\n\n\n\n10.4.1.2 Finding the Optimal Degree with cross_val_score\n\n10.4.1.2.1 Step 1: Let‚Äôs begin by importing the required libraries.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n\n\n\n10.4.1.2.2 Step 2: Let‚Äôs generate input data\n\n# Simulate input data\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(360)])\nnp.random.seed(10)  #Setting seed for reproducibility\ny = np.sin(x) + np.random.normal(0,0.15,len(x))\ndata = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\nplt.plot(data['x'],data['y'],'.');\n\n\n\n\n\n\n\n\n\n\n10.4.1.2.3 Step 3: Train-Test Split\n\n# Split the data\nX = data['x'].values.reshape(-1, 1)\ny = data['y'].values\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n\n10.4.1.2.4 Step 4: Using cross_val_score to tune the p in polynomialFeatures\nIn sklearn, polynomial features can be generated using the PolynomialFeatures class. Also, to perform LinearRegression and PolynomialFeatures in tandem, we will use the module sklearn_pipeline - it basically creates the features and feeds the output to the model (in that sequence).\n\ndef cross_validation_score(X_train, y_train, max_degree=10, scoring='r2'):\n    \"\"\"\n    Perform cross-validation for polynomial regression models with different degrees.\n\n    Parameters:\n        X_train (array-like): Training feature data.\n        y_train (array-like): Training target data.\n        max_degree (int): Maximum polynomial degree to evaluate.\n        scoring (str): Scoring metric ('r2' for R¬≤, 'rmse' for root mean squared error).\n\n    Returns:\n        degrees (list): List of polynomial degrees evaluated.\n        scores_df (DataFrame): DataFrame of cross-validation scores across different degrees.\n    \"\"\"\n    degrees = range(1, max_degree + 1)\n    cv_scores = []\n\n    for degree in degrees:\n        # Create polynomial regression model\n        model = make_pipeline(\n            PolynomialFeatures(degree),\n            LinearRegression()\n        )\n\n        # Compute cross-validation scores\n        if scoring == 'rmse':\n            raw_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n            cv_score = np.sqrt(-raw_scores)  # Convert negative MSE to RMSE\n            score_label = \"RMSE (lower is better)\"\n        else:  # Default to R¬≤\n            cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n            score_label = \"R¬≤ (higher is better)\"\n\n        cv_scores.append(cv_score)\n\n    # Convert scores to a DataFrame\n    scores_df = pd.DataFrame(np.array(cv_scores), index=degrees, columns=[f'Fold {i+1}' for i in range(cv_scores[0].shape[0])])\n    \n    print(f\"Cross-validation scores ({score_label}):\")\n    return degrees, scores_df\n\n\n# Example Usage:\nscoring_metric = 'rmse'  # Change to 'r2' if needed\ndegrees, scores = cross_validation_score(X_train, y_train, scoring=\"rmse\")\n\n# Print the formatted score matrix\nprint(scores)\n\nCross-validation scores (RMSE (lower is better)):\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n1   0.500794  0.456810  0.472232  0.484231  0.461889\n2   0.500255  0.455897  0.475378  0.485450  0.461456\n3   0.164968  0.158555  0.156423  0.152829  0.161931\n4   0.165521  0.158619  0.156413  0.153359  0.162379\n5   0.145887  0.138576  0.143649  0.141445  0.152968\n6   0.145995  0.139056  0.143390  0.144567  0.153870\n7   0.151113  0.138186  0.144122  0.144211  0.153438\n8   0.150406  0.137350  0.143297  0.146944  0.154557\n9   0.151095  0.138084  0.143219  0.146828  0.154599\n10  0.152122  0.139420  0.145089  0.147192  0.154581\n\n\n\n# take mean of each row and add it as the last column of the scores dataframe\nscores['mean'] = scores.mean(axis=1)\nprint(scores)\n\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5      mean\n1   0.500794  0.456810  0.472232  0.484231  0.461889  0.475191\n2   0.500255  0.455897  0.475378  0.485450  0.461456  0.475687\n3   0.164968  0.158555  0.156423  0.152829  0.161931  0.158941\n4   0.165521  0.158619  0.156413  0.153359  0.162379  0.159258\n5   0.145887  0.138576  0.143649  0.141445  0.152968  0.144505\n6   0.145995  0.139056  0.143390  0.144567  0.153870  0.145376\n7   0.151113  0.138186  0.144122  0.144211  0.153438  0.146214\n8   0.150406  0.137350  0.143297  0.146944  0.154557  0.146511\n9   0.151095  0.138084  0.143219  0.146828  0.154599  0.146765\n10  0.152122  0.139420  0.145089  0.147192  0.154581  0.147681\n\n\n\n# plot the mean scores for each degree\nplt.figure(figsize=(12, 6))\nplt.plot(degrees, scores['mean'], 'o-', label='Cross-validation RMSE')\nplt.xlabel('Degree')\nplt.ylabel('Mean CV Score')\nplt.title('Polynomial Degree vs. Mean CV RMSE')\nplt.grid(True)\nplt.legend();\n\n\n\n\n\n\n\n\n\n# Find optimal degree based on the mean cross-validation scores\noptimal_degree = scores['mean'].idxmin()\nprint(f'Optimal polynomial degree: {optimal_degree}')\n\nOptimal polynomial degree: 5\n\n\n\n# use r2 as performance metric, note that r2 is default metric in the function defination\ndegrees, scores = cross_validation_score(X_train, y_train)\n\n# Print the formatted score matrix\nprint(scores)\n\nCross-validation scores (R¬≤ (higher is better)):\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5\n1   0.474906  0.587650  0.567349  0.584471  0.590389\n2   0.476036  0.589297  0.561564  0.582378  0.591156\n3   0.943020  0.950323  0.952529  0.958609  0.949655\n4   0.942638  0.950283  0.952535  0.958321  0.949376\n5   0.955439  0.962054  0.959965  0.964546  0.955074\n6   0.955373  0.961790  0.960110  0.962963  0.954543\n7   0.952190  0.962267  0.959701  0.963145  0.954797\n8   0.952636  0.962722  0.960162  0.961735  0.954136\n9   0.952201  0.962322  0.960205  0.961795  0.954111\n10  0.951549  0.961590  0.959159  0.961606  0.954121\n\n\n\n# take mean of each row and add it as the last column of the scores dataframe\nscores['mean'] = scores.mean(axis=1)\nprint(scores)\n\n      Fold 1    Fold 2    Fold 3    Fold 4    Fold 5      mean\n1   0.474906  0.587650  0.567349  0.584471  0.590389  0.560953\n2   0.476036  0.589297  0.561564  0.582378  0.591156  0.560086\n3   0.943020  0.950323  0.952529  0.958609  0.949655  0.950827\n4   0.942638  0.950283  0.952535  0.958321  0.949376  0.950631\n5   0.955439  0.962054  0.959965  0.964546  0.955074  0.959416\n6   0.955373  0.961790  0.960110  0.962963  0.954543  0.958956\n7   0.952190  0.962267  0.959701  0.963145  0.954797  0.958420\n8   0.952636  0.962722  0.960162  0.961735  0.954136  0.958278\n9   0.952201  0.962322  0.960205  0.961795  0.954111  0.958127\n10  0.951549  0.961590  0.959159  0.961606  0.954121  0.957605\n\n\n\n# plot the mean scores for each degree\nplt.figure(figsize=(12, 6))\nplt.plot(degrees, scores['mean'], 'o-', label='Cross-validation R¬≤')\nplt.xlabel('Degree')\nplt.ylabel('Mean CV Score')\nplt.title('Polynomial Degree vs. Mean CV Score')\nplt.grid(True)\nplt.legend();\n\n\n\n\n\n\n\n\n\n# Find optimal degree based on the mean cross-validation scores\noptimal_degree = scores['mean'].idxmax()\nprint(f'Optimal polynomial degree: {optimal_degree}')\n\nOptimal polynomial degree: 5\n\n\n\n\n10.4.1.2.5 Step 5: Fitting the Final Model with the Optimal Polynomial Degree\nAfter determining the optimal polynomial degree (p=5) using cross-validation, we now refit the model on the entire training dataset.\nThis ensures that the model fully utilizes all available training data for the best possible fit.\nSteps: 1. Build the final model using a polynomial transformation with the optimal degree (p=5). 2. Train the model on the full training dataset. 3. Generate predictions and visualize how well the polynomial regression fits the data.\nThe plot below shows the final polynomial regression fit, highlighting how the model captures patterns in the dataset.\n\n# Fit final model with optimal degree\nfinal_model = make_pipeline(\n    PolynomialFeatures(optimal_degree),\n    LinearRegression()\n)\nfinal_model.fit(X_train, y_train)\n\n# Plot final model predictions\nplt.figure(figsize=(12, 6))\nX_sorted = np.sort(X)\ny_pred = final_model.predict(X_sorted)\n\nplt.scatter(X, y, color='blue', alpha=0.5, label='Data points')\nplt.plot(X_sorted, y_pred, color='red', label=f'Polynomial degree {optimal_degree}')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Polynomial Regression Fit')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4.1.2.6 Step 6: Output the final model performance\n\n# Print final model performance metrics\nprint(\"\\nFinal Model Performance:\")\nprint(f\"Training R¬≤: {final_model.score(X_train, y_train):.4f}\")\nprint(f\"Test R¬≤: {final_model.score(X_test, y_test):.4f}\")\n\n\nFinal Model Performance:\nTraining R¬≤: 0.9613\nTest R¬≤: 0.9521\n\n\n\n\n\n10.4.1.3 Hyperparameter Tuning with cross_validate\nThe cross_validate function differs from cross_val_score in two ways:\n\nIt allows specifying multiple metrics for evaluation.\nIt returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score.\n\n\nfrom sklearn.model_selection import cross_validate\n\ndef cross_validation_full_results(X_train, y_train, max_degree=10):\n    \"\"\"\n    Perform cross-validation for polynomial regression models with different degrees.\n    Captures all cross-validation results including R¬≤, RMSE, fit time, score time, etc.\n\n    Parameters:\n        X_train (array-like): Training feature data.\n        y_train (array-like): Training target data.\n        max_degree (int): Maximum polynomial degree to evaluate.\n\n    Returns:\n        full_results_df (DataFrame): DataFrame containing all cross-validation metrics for each degree.\n    \"\"\"\n    degrees = []\n    results_dict = {\n        \"Fold\": [],\n        \"Fit Time\": [],\n        \"Score Time\": [],\n        \"R¬≤\": [],\n        \"RMSE\": []\n    }\n\n    for degree in range(1, max_degree + 1):\n        # Create polynomial regression model\n        model = make_pipeline(\n            PolynomialFeatures(degree),\n            LinearRegression()\n        )\n\n        # Perform cross-validation for both R¬≤ and RMSE, capturing additional metrics\n        cv_results = cross_validate(model, X_train, y_train, cv=5, \n                                    scoring=['r2', 'neg_root_mean_squared_error'], return_train_score=True)\n        \n        # Store each fold's results separately\n        for fold in range(5):  # 5 folds\n            degrees.append(degree)\n            results_dict[\"Fold\"].append(fold + 1)\n            results_dict[\"Fit Time\"].append(cv_results[\"fit_time\"][fold])\n            results_dict[\"Score Time\"].append(cv_results[\"score_time\"][fold])\n            results_dict[\"R¬≤\"].append(cv_results[\"test_r2\"][fold])\n            results_dict[\"RMSE\"].append(-cv_results[\"test_neg_root_mean_squared_error\"][fold])  # Convert negative RMSE to positive\n\n    # Convert to DataFrame\n    full_results_df = pd.DataFrame(results_dict)\n    full_results_df.insert(0, \"Degree\", degrees)  # Add Degree column at the front\n\n    print(\"Complete Cross-Validation Results (Higher R¬≤ is better, Lower RMSE is better, Fit Time & Score Time included):\")\n    return full_results_df\n\nscores_df = cross_validation_full_results(X_train, y_train, max_degree=10)\n\nscores_df\n\nComplete Cross-Validation Results (Higher R¬≤ is better, Lower RMSE is better, Fit Time & Score Time included):\n\n\n\n\n\n\n\n\n\nDegree\nFold\nFit Time\nScore Time\nR¬≤\nRMSE\n\n\n\n\n0\n1\n1\n0.001993\n0.002523\n0.474906\n0.500794\n\n\n1\n1\n2\n0.001994\n0.002033\n0.587650\n0.456810\n\n\n2\n1\n3\n0.000000\n0.000000\n0.567349\n0.472232\n\n\n3\n1\n4\n0.000996\n0.000996\n0.584471\n0.484231\n\n\n4\n1\n5\n0.000997\n0.001993\n0.590389\n0.461889\n\n\n5\n2\n1\n0.001482\n0.000000\n0.476036\n0.500255\n\n\n6\n2\n2\n0.006236\n0.002004\n0.589297\n0.455897\n\n\n7\n2\n3\n0.000996\n0.000997\n0.561564\n0.475378\n\n\n8\n2\n4\n0.000997\n0.000997\n0.582378\n0.485450\n\n\n9\n2\n5\n0.000997\n0.000997\n0.591156\n0.461456\n\n\n10\n3\n1\n0.000996\n0.000997\n0.943020\n0.164968\n\n\n11\n3\n2\n0.000997\n0.001250\n0.950323\n0.158555\n\n\n12\n3\n3\n0.001089\n0.000000\n0.952529\n0.156423\n\n\n13\n3\n4\n0.000000\n0.000000\n0.958609\n0.152829\n\n\n14\n3\n5\n0.000000\n0.000000\n0.949655\n0.161931\n\n\n15\n4\n1\n0.000000\n0.000000\n0.942638\n0.165521\n\n\n16\n4\n2\n0.000000\n0.000000\n0.950283\n0.158619\n\n\n17\n4\n3\n0.000996\n0.000996\n0.952535\n0.156413\n\n\n18\n4\n4\n0.000997\n0.000996\n0.958321\n0.153359\n\n\n19\n4\n5\n0.000997\n0.000997\n0.949376\n0.162379\n\n\n20\n5\n1\n0.000996\n0.000992\n0.955439\n0.145887\n\n\n21\n5\n2\n0.000997\n0.001001\n0.962054\n0.138576\n\n\n22\n5\n3\n0.000997\n0.000993\n0.959965\n0.143649\n\n\n23\n5\n4\n0.000996\n0.000997\n0.964546\n0.141445\n\n\n24\n5\n5\n0.000000\n0.000000\n0.955074\n0.152968\n\n\n25\n6\n1\n0.000000\n0.000000\n0.955373\n0.145995\n\n\n26\n6\n2\n0.000000\n0.000000\n0.961790\n0.139056\n\n\n27\n6\n3\n0.000000\n0.000000\n0.960110\n0.143390\n\n\n28\n6\n4\n0.000000\n0.000996\n0.962963\n0.144567\n\n\n29\n6\n5\n0.000000\n0.000997\n0.954543\n0.153870\n\n\n30\n7\n1\n0.000998\n0.001304\n0.952190\n0.151113\n\n\n31\n7\n2\n0.000997\n0.001003\n0.962267\n0.138186\n\n\n32\n7\n3\n0.000000\n0.000997\n0.959701\n0.144122\n\n\n33\n7\n4\n0.000000\n0.000993\n0.963145\n0.144211\n\n\n34\n7\n5\n0.000997\n0.000997\n0.954797\n0.153438\n\n\n35\n8\n1\n0.000997\n0.000842\n0.952636\n0.150406\n\n\n36\n8\n2\n0.000000\n0.000000\n0.962722\n0.137350\n\n\n37\n8\n3\n0.000000\n0.000000\n0.960162\n0.143297\n\n\n38\n8\n4\n0.010667\n0.000995\n0.961735\n0.146944\n\n\n39\n8\n5\n0.000000\n0.000000\n0.954136\n0.154557\n\n\n40\n9\n1\n0.000000\n0.000000\n0.952201\n0.151095\n\n\n41\n9\n2\n0.000000\n0.000000\n0.962322\n0.138084\n\n\n42\n9\n3\n0.000000\n0.000000\n0.960205\n0.143219\n\n\n43\n9\n4\n0.000996\n0.001993\n0.961795\n0.146828\n\n\n44\n9\n5\n0.000170\n0.000000\n0.954111\n0.154599\n\n\n45\n10\n1\n0.000000\n0.000000\n0.951549\n0.152122\n\n\n46\n10\n2\n0.000000\n0.000000\n0.961590\n0.139420\n\n\n47\n10\n3\n0.000000\n0.012748\n0.959159\n0.145089\n\n\n48\n10\n4\n0.000996\n0.001614\n0.961606\n0.147192\n\n\n49\n10\n5\n0.000000\n0.000000\n0.954121\n0.154581\n\n\n\n\n\n\n\n\nmean_results_df = scores_df.groupby(\"Degree\")[[\"R¬≤\", \"RMSE\"]].mean().reset_index()\nmean_results_df\n\n\n\n\n\n\n\n\nDegree\nR¬≤\nRMSE\n\n\n\n\n0\n1\n0.560953\n0.475191\n\n\n1\n2\n0.560086\n0.475687\n\n\n2\n3\n0.950827\n0.158941\n\n\n3\n4\n0.950631\n0.159258\n\n\n4\n5\n0.959416\n0.144505\n\n\n5\n6\n0.958956\n0.145376\n\n\n6\n7\n0.958420\n0.146214\n\n\n7\n8\n0.958278\n0.146511\n\n\n8\n9\n0.958127\n0.146765\n\n\n9\n10\n0.957605\n0.147681\n\n\n\n\n\n\n\n\n\n10.4.1.4 Grid Search Cross-Validation for Hyperparameter Tuning\nA common use of cross-validation is tuning the hyperparameters of a model. One of the most widely used techniques for this is grid search cross-validation.\nHow Grid Search Cross-Validation Works\n\nDefine a Grid of Hyperparameters:\nWe specify a set of hyperparameters and the possible values we want to evaluate for each.\nEvaluate All Combinations:\nEvery possible combination of hyperparameter values in the grid is systematically tested.\nCross-Validation for Model Evaluation:\nFor each combination, the model is trained and evaluated using cross-validation to assess performance.\nSelect the Best Hyperparameter Setting:\nThe combination that yields the best validation performance is chosen as the optimal set of hyperparameters.\n\nGrid search ensures that we explore multiple hyperparameter settings in a structured way, improving the model‚Äôs performance without manually adjusting parameters.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the pipeline\nmodel = make_pipeline(PolynomialFeatures(), LinearRegression())\n\n# Define the hyperparameter grid\nparam_grid = {'polynomialfeatures__degree': np.arange(1, 11)}\n\n# define a KFold cross-validation with shuffling\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# Perform Grid Search with Cross-Validation\ngrid_search = GridSearchCV(model, param_grid, cv=cv, scoring= 'r2', return_train_score=True, verbose=1, n_jobs=-1)\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\nGridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('linearregression',\n                                        LinearRegression())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             return_train_score=True, scoring='r2', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=5, random_state=42, shuffle=True),\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('linearregression',\n                                        LinearRegression())]),\n             n_jobs=-1,\n             param_grid={'polynomialfeatures__degree': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])},\n             return_train_score=True, scoring='r2', verbose=1) best_estimator_: PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=5) LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_polynomialfeatures__degree\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\n...\nmean_test_score\nstd_test_score\nrank_test_score\nsplit0_train_score\nsplit1_train_score\nsplit2_train_score\nsplit3_train_score\nsplit4_train_score\nmean_train_score\nstd_train_score\n\n\n\n\n0\n0.014676\n0.001534\n0.002452\n0.000697\n1\n{'polynomialfeatures__degree': 1}\n0.514869\n0.594851\n0.583781\n0.548702\n...\n0.543991\n0.043422\n9\n0.580311\n0.549217\n0.565220\n0.567494\n0.584039\n0.569256\n0.012343\n\n\n1\n0.014464\n0.001952\n0.002246\n0.000386\n2\n{'polynomialfeatures__degree': 2}\n0.516412\n0.596158\n0.585003\n0.539017\n...\n0.543109\n0.043391\n10\n0.580415\n0.549575\n0.565611\n0.569567\n0.584617\n0.569957\n0.012318\n\n\n2\n0.010744\n0.001704\n0.001196\n0.000399\n3\n{'polynomialfeatures__degree': 3}\n0.948330\n0.954229\n0.939908\n0.950587\n...\n0.948949\n0.004903\n7\n0.952702\n0.950599\n0.955068\n0.951851\n0.951932\n0.952430\n0.001481\n\n\n3\n0.003352\n0.001858\n0.001900\n0.000197\n4\n{'polynomialfeatures__degree': 4}\n0.948137\n0.954200\n0.938938\n0.950252\n...\n0.948634\n0.005233\n8\n0.952718\n0.950600\n0.955190\n0.951892\n0.951934\n0.952467\n0.001522\n\n\n4\n0.003586\n0.002212\n0.001844\n0.000487\n5\n{'polynomialfeatures__degree': 5}\n0.957618\n0.965540\n0.945282\n0.960822\n...\n0.958620\n0.007195\n1\n0.961771\n0.959014\n0.965001\n0.960735\n0.960368\n0.961378\n0.002015\n\n\n5\n0.002876\n0.001588\n0.001994\n0.000631\n6\n{'polynomialfeatures__degree': 6}\n0.957569\n0.965655\n0.943992\n0.960471\n...\n0.958278\n0.007659\n2\n0.961771\n0.959019\n0.965278\n0.960858\n0.960459\n0.961477\n0.002097\n\n\n6\n0.003966\n0.003833\n0.001792\n0.000374\n7\n{'polynomialfeatures__degree': 7}\n0.957219\n0.965979\n0.944374\n0.960850\n...\n0.958136\n0.007431\n3\n0.962026\n0.959143\n0.965312\n0.960954\n0.960862\n0.961659\n0.002046\n\n\n7\n0.002766\n0.000773\n0.001784\n0.000394\n8\n{'polynomialfeatures__degree': 8}\n0.957222\n0.966218\n0.944548\n0.960278\n...\n0.957822\n0.007242\n4\n0.962026\n0.959278\n0.965478\n0.961240\n0.961259\n0.961856\n0.002026\n\n\n8\n0.002394\n0.000487\n0.001599\n0.000487\n9\n{'polynomialfeatures__degree': 9}\n0.955364\n0.965595\n0.944424\n0.960261\n...\n0.957306\n0.007211\n5\n0.962294\n0.959391\n0.965482\n0.961240\n0.961259\n0.961933\n0.002006\n\n\n9\n0.002936\n0.000066\n0.001993\n0.000036\n10\n{'polynomialfeatures__degree': 10}\n0.955130\n0.965594\n0.944371\n0.959727\n...\n0.957090\n0.007175\n6\n0.962307\n0.959391\n0.965501\n0.961302\n0.961271\n0.961955\n0.002008\n\n\n\n\n10 rows √ó 21 columns\n\n\n\n\npd.set_option('display.float_format', '{:.6f}'.format) \ncv_results[[\"mean_test_score\", \"mean_train_score\"]]\n\n\n\n\n\n\n\n\nmean_test_score\nmean_train_score\n\n\n\n\n0\n0.543991\n0.569256\n\n\n1\n0.543109\n0.569957\n\n\n2\n0.948949\n0.952430\n\n\n3\n0.948634\n0.952467\n\n\n4\n0.958620\n0.961378\n\n\n5\n0.958278\n0.961477\n\n\n6\n0.958136\n0.961659\n\n\n7\n0.957822\n0.961856\n\n\n8\n0.957306\n0.961933\n\n\n9\n0.957090\n0.961955\n\n\n\n\n\n\n\n\n# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_polynomialfeatures__degree\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_polynomialfeatures__degree\"], cv_results[\"mean_train_score\"])\nplt.xlabel('degree')\nplt.ylabel('r-squared')\nplt.title(\"Optimal polynomial degree\")\nplt.legend(['test score', 'train score'], loc='upper left');\n\n\n\n\n\n\n\n\n\n# print out the best hyperparameters and the best score\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\nprint(grid_search.best_estimator_)\n\n{'polynomialfeatures__degree': 5}\n0.9586200484884486\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])\n\n\nBenefits of GridSearchCV\n\nNo need for manually writing nested for loops for hyperparameter tuning.\n\nAllows parallel processing using multiple CPU cores (n_jobs=-1), speeding up the search.\n\nGridSearchCV guarantees finding the best combination.\n\nHowever, GridSearchCV performs an exhaustive search over all possible hyperparameter combinations, ensuring that the best parameters are found. This can be slow, especially when dealing with multiple hyperparameters.\n\n\n10.4.1.5 GridSearchCV vs.¬†Faster Alternatives for Hyperparameter Tuning\nTwo Ways to Speed Up Hyperparameter Tuning\n\nRandomizedSearchCV\n\nInstead of checking every combination, it randomly samples a set number of hyperparameter values, significantly reducing computation time.\n\nSuitable when a rough estimate of the best hyperparameters is sufficient.\n\nShuffleSplit\n\nUnlike KFold, which ensures each sample is used exactly once as a test set, ShuffleSplit randomly selects train-test splits in each iteration.\n\nReduces redundant computations, making the process faster while maintaining good model performance.\n\n\nBoth approaches can be combined with n_jobs=-1 to leverage parallel processing for even faster results.\n\n\n\n10.4.2 Tuning the classification threshold\nBy default, classifiers use 0.5 as the threshold for classification. However, adjusting this threshold can improve precision, recall, or F1-score depending on the application. Let‚Äôs use cross_val_predict to tune the Classification Threshold\n\n10.4.2.1 Using TunedThresholdClassifierCV from sklearn ( &gt;= Version 1.5)\n\nfrom sklearn.model_selection import TunedThresholdClassifierCV\n\n# Define cross-validation strategy\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\n# Define TunedThresholdClassifierCV\ntuned_clf = TunedThresholdClassifierCV(\n    estimator=logit_model,\n    scoring=\"f1\",  # Optimize for F1-score\n    cv=cv\n)\n\n# Fit the model\ntuned_clf.fit(X_train_class, y_train_class)\n\n\n# Print the best threshold and the corresponding score\nprint(f\"Best threshold: {tuned_clf.best_threshold_:.2f}\")\nprint(f\"Best F1-score: {tuned_clf.best_score_:.4f}\")\n\nBest threshold: 0.39\nBest F1-score: 0.8757\n\n\n\n\n10.4.2.2 Using cross_val_predict\n\n# use the knn to tune the threshold for classification, the threshold is among np.arange(0.1, 0.9, 0.1), using 5 fold cross validation for tuning the threshold\nthresholds = np.arange(0.1, 0.9, 0.1)\nscores = []\nfor threshold in thresholds:\n    y_pred_class_prob = cross_val_predict(logit_model, X_train_class, y_train_class, cv=5, method='predict_proba')[:,1]\n    y_pred_class = (y_pred_class_prob &gt; threshold).astype(int)\n    scores.append(accuracy_score(y_train_class, y_pred_class))\nscores = pd.Series(scores, index=thresholds)\n\ndf_scores = pd.DataFrame({'threshold': thresholds, 'accuracy': scores}, )\nprint(df_scores.to_string(index=False))\n\n threshold  accuracy\n  0.100000  0.716000\n  0.200000  0.832000\n  0.300000  0.864000\n  0.400000  0.868000\n  0.500000  0.865333\n  0.600000  0.861333\n  0.700000  0.841333\n  0.800000  0.810667\n\n\n\n# print out the best threshold  and the cv score for the best threshold\nbest_threshold = scores.idxmax()\nprint(best_threshold)\nprint(scores.loc[best_threshold])\n\n0.4\n0.868",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Cross-Validation</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html",
    "href": "Regularization in Python.html",
    "title": "11¬† Regularization",
    "section": "",
    "text": "11.1 Why do we need regularization?",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#why-do-we-need-regularization",
    "href": "Regularization in Python.html#why-do-we-need-regularization",
    "title": "11¬† Regularization",
    "section": "",
    "text": "11.1.1 The Challenge of Overfitting and Underfitting\nWhen building machine learning models, we aim to find patterns in data that generalize well to unseen samples. However, models can suffer from two key issues:\n\nUnderfitting: The model is too simple to capture the underlying pattern in the data.\nOverfitting: The model is too complex and captures noise rather than generalizable patterns.\n\nRegularization is a technique used to address overfitting by penalizing overly complex models.\n\n\n11.1.2 Understanding the Bias-Variance Tradeoff\nA well-performing model balances two competing sources of error:\n\nBias: Error due to overly simplistic assumptions (e.g., underfitting).\nVariance: Error due to excessive sensitivity to training data (e.g., overfitting).\n\nA high-bias model (e.g., a simple linear regression) may not capture the underlying trend, while a high-variance model (e.g., a deep neural network with many parameters) may memorize noise instead of learning meaningful patterns.\nRegularization helps reduce variance while maintaining an appropriate level of model complexity.\n\n\n11.1.3 Visualizing Overfitting vs.¬†Underfitting\nTo better understand this concept, consider three different models:\n\nUnderfitting (High Bias): The model is too simple and fails to capture important trends.\nGood Fit (Balanced Bias & Variance): The model generalizes well to unseen data.\nOverfitting (High Variance): The model is too complex and captures noise, leading to poor generalization.\n\n\\[\n\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n\\]\nRegularization helps control variance by penalizing large coefficients, leading to a model that generalizes better.",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#simulating-data-for-an-overfitting-linear-model",
    "href": "Regularization in Python.html#simulating-data-for-an-overfitting-linear-model",
    "title": "11¬† Regularization",
    "section": "11.2 Simulating Data for an Overfitting Linear Model",
    "text": "11.2 Simulating Data for an Overfitting Linear Model\n\n11.2.1 Generating the data\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(360)])\nnp.random.seed(10)  #Setting seed for reproducibility\ny = np.sin(x) + np.random.normal(0,0.15,len(x))\ndata = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\nplt.plot(data['x'],data['y'],'.');\n\n\n\n\n\n\n\n\n\n# check how the data looks like\ndata.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0.000000\n0.199738\n\n\n1\n0.017453\n0.124744\n\n\n2\n0.034907\n-0.196911\n\n\n3\n0.052360\n0.051078\n\n\n4\n0.069813\n0.162957\n\n\n\n\n\n\n\nPolynomial features allow linear regression to model non-linear relationships. Higher-degree terms capture more complex patterns in the data. Let‚Äôs manually expands features, similar to PolynomialFeatures in sklearn.preprocessing. Using polynomial regression, we can evaluate different polynomial degrees and analyze the balance between underfitting and overfitting.\n\nfor i in range(2,16):  #power of 1 is already there\n    colname = 'x_%d'%i      #new var will be x_power\n    data[colname] = data['x']**i\ndata.head()\n\n\n\n\n\n\n\n\nx\ny\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\n\n\n\n\n0\n0.000000\n0.199738\n0.000000\n0.000000\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n1\n0.017453\n0.124744\n0.000305\n0.000005\n9.279177e-08\n1.619522e-09\n2.826599e-11\n4.933346e-13\n8.610313e-15\n1.502783e-16\n2.622851e-18\n4.577739e-20\n7.989662e-22\n1.394459e-23\n2.433790e-25\n4.247765e-27\n\n\n2\n0.034907\n-0.196911\n0.001218\n0.000043\n1.484668e-06\n5.182470e-08\n1.809023e-09\n6.314683e-11\n2.204240e-12\n7.694250e-14\n2.685800e-15\n9.375210e-17\n3.272566e-18\n1.142341e-19\n3.987522e-21\n1.391908e-22\n\n\n3\n0.052360\n0.051078\n0.002742\n0.000144\n7.516134e-06\n3.935438e-07\n2.060591e-08\n1.078923e-09\n5.649226e-11\n2.957928e-12\n1.548767e-13\n8.109328e-15\n4.246034e-16\n2.223218e-17\n1.164074e-18\n6.095079e-20\n\n\n4\n0.069813\n0.162957\n0.004874\n0.000340\n2.375469e-05\n1.658390e-06\n1.157775e-07\n8.082794e-09\n5.642855e-10\n3.939456e-11\n2.750259e-12\n1.920043e-13\n1.340443e-14\n9.358057e-16\n6.533156e-17\n4.561003e-18\n\n\n\n\n\n\n\nWhat This Code Does\n\nGenerates Higher-Degree Polynomial Features: Iterates over the range 2 to 15, computing polynomial terms (x¬≤, x¬≥, ..., x¬π‚Åµ).\nDynamically Creates Column Names: New feature names are automatically generated in the format x_2, x_3, ..., x_15.\nExpands the Dataset: Each polynomial-transformed feature is stored\n\n\n\n11.2.2 Splitting the Data\nNext, we will split the data into training and testing sets. As we‚Äôve learned, models tend to overfit when trained on a small dataset.\nTo intentionally create an overfitting scenario, we will: - Use only 10% of the data for training. - Reserve 90% of the data for testing.\nThis is not a typical train-test split but is deliberately done to demonstrate overfitting, where the model performs well on the training data but generalizes poorly to unseen data.\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.9)\n\n\nprint('Number of observations in the training data:', len(train))\nprint('Number of observations in the test data:',len(test))\n\nNumber of observations in the training data: 36\nNumber of observations in the test data: 324\n\n\n\n\n11.2.3 Splitting the target and features\n\nX_train = train.drop('y', axis=1).values\ny_train = train.y.values\nX_test = test.drop('y', axis=1).values\ny_test = test.y.values\n\n\n\n11.2.4 Building Models\n\n11.2.4.1 Building a linear model with only 1 predictor x\n\n# Linear regression with one feature\nindependent_variable_train = X_train[:, 0:1]\n\nlinreg = LinearRegression()\nlinreg.fit(independent_variable_train, y_train)\ny_train_pred = linreg.predict(independent_variable_train)\nrss_train = sum((y_train_pred-y_train)**2)/X_train.shape[0]\n\nindependent_variable_test = X_test[:, 0:1]\ny_test_pred = linreg.predict(independent_variable_test)\nrss_test = sum((y_test_pred-y_test)**2)/X_test.shape[0]\n\nprint(\"Training Error\", rss_train)\nprint(\"Testing Error\", rss_test)\n\nplt.plot(X_train[:, 0:1], y_train_pred)\nplt.plot(X_train[:, 0:1], y_train, '.');\n\nTraining Error 0.22398220582126424\nTesting Error 0.22151086120574928\n\n\n\n\n\n\n\n\n\n\n\n11.2.4.2 Building a linear regression model with three features x, x_2, x_3\n\nindependent_variable_train = X_train[:, 0:3]\nindependent_variable_train[:3]\n\narray([[ 1.36135682,  1.85329238,  2.52299222],\n       [ 2.30383461,  5.30765392, 12.22795682],\n       [ 1.51843645,  2.30564925,  3.50098186]])\n\n\n\ndef sort_xy(x,y):\n    idx = np.argsort(x)\n    x2,y2= x[idx] ,y[idx] \n    return x2,y2\n\n\n# Linear regression with 3 features\n\nlinreg = LinearRegression()\n\nlinreg.fit(independent_variable_train, y_train)\ny_train_pred = linreg.predict(independent_variable_train)\nrss_train = sum((y_train_pred-y_train)**2)/X_train.shape[0]\n\nindependent_variable_test = X_test[:, 0:3]\ny_test_pred = linreg.predict(independent_variable_test)\nrss_test = sum((y_test_pred-y_test)**2)/X_test.shape[0]\n\nprint(\"Training Error\", rss_train)\nprint(\"Testing Error\", rss_test)\n\nplt.plot(*sort_xy(X_train[:, 0], y_train_pred))\nplt.plot(X_train[:, 0], y_train, '.');\n\nTraining Error 0.02167114498970705\nTesting Error 0.028159311299747036\n\n\n\n\n\n\n\n\n\nLet‚Äôs define a helper function that dynamically builds and trains a linear regression model based on a specified number of features. It allows for flexibility in selecting features and automates the process for multiple models.\n\n# Define a function which will fit linear vregression model, plot the results, and return the coefficient\ndef linear_regression(train_x, train_y, test_x, test_y, features, models_to_plot):\n    \n    #fit the model\n    linreg = LinearRegression()\n    linreg.fit(train_x, train_y)\n    train_y_pred = linreg.predict(train_x)\n    test_y_pred = linreg.predict(test_x)\n    \n    #check if a plot is to be made for the entered features\n    if features in models_to_plot:\n        plt.subplot(models_to_plot[features])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x[:, 0], train_y_pred))\n        plt.plot(train_x[:, 0], train_y, '.')\n        \n        plt.title('Number of Predictors: %d'%features)\n        \n    #return the result in pre-defined format\n    rss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [rss_train]\n    \n    rss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([rss_test])\n    \n    ret.extend([linreg.intercept_])\n    ret.extend(linreg.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the results:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['Number_of_variable_%d'%i for i in range(1, 16)]\ncoef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1:231, 3:232, 6:233, 9:234, 12:235, 15:236}\n\n\nimport matplotlib.pyplot as plt\n# Iterate through all powers and store the results in a matrix form\nplt.figure(figsize = (12, 8))\nfor i in range(1, 16):\n    train_x = X_train[:, 0:i]\n    train_y = y_train\n    test_x = X_test[:, 0:i]\n    test_y = y_test\n    \n    coef_matrix_simple.iloc[i-1, 0:i+3] = linear_regression(train_x, train_y, test_x, test_y, features=i, models_to_plot=models_to_plot)\n\n\n\n\n\n\n\n\nKey Takeaways:\nAs we increase the number of features (higher-degree polynomial terms), we observe the following:\n- The model becomes more flexible, capturing intricate patterns in the training data.\n- The curve becomes increasingly wavy and complex, adapting too closely to the data points.\n- This results in overfitting, where the model performs well on the training set but fails to generalize to unseen data.\nOverfitting occurs because the model learns noise instead of the true underlying pattern, leading to poor performance on new data.\nTo better understand this phenomenon, let‚Äôs:\n- Evaluate model performance on both the training and test sets.\n- Output the model coefficients to analyze how feature coefficients changes with increasing complexity.\n\n# Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_simple\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nNumber_of_variable_1\n0.22\n0.22\n0.88\n-0.29\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_2\n0.22\n0.22\n0.84\n-0.25\n-0.0057\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_3\n0.022\n0.028\n-0.032\n1.7\n-0.83\n0.089\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_4\n0.021\n0.03\n-0.09\n2\n-1\n0.14\n-0.0037\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_5\n0.02\n0.025\n-0.019\n1.5\n-0.48\n-0.092\n0.037\n-0.0026\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_6\n0.019\n0.029\n-0.13\n2.4\n-1.9\n0.77\n-0.21\n0.031\n-0.0017\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_7\n0.017\n0.034\n-0.37\n4.7\n-6.5\n4.7\n-1.9\n0.4\n-0.044\n0.0019\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_8\n0.017\n0.035\n-0.42\n5.3\n-8\n6.4\n-2.9\n0.73\n-0.1\n0.0076\n-0.00022\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_9\n0.016\n0.036\n-0.57\n7.1\n-14\n16\n-9.9\n3.8\n-0.91\n0.13\n-0.011\n0.00037\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_10\n0.016\n0.036\n-0.51\n6.2\n-11\n9.7\n-4.5\n0.83\n0.11\n-0.087\n0.018\n-0.0017\n6.6e-05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_11\n0.014\n0.044\n0.17\n-4\n36\n-86\n1e+02\n-71\n31\n-8.8\n1.6\n-0.18\n0.012\n-0.00034\nNaN\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_12\n0.013\n0.049\n0.54\n-10\n67\n-1.6e+02\n2e+02\n-1.5e+02\n74\n-24\n5.4\n-0.8\n0.076\n-0.0041\n0.0001\nNaN\nNaN\nNaN\n\n\nNumber_of_variable_13\n0.0086\n0.065\n-0.56\n9.9\n-56\n2e+02\n-3.9e+02\n4.5e+02\n-3.2e+02\n1.6e+02\n-51\n11\n-1.7\n0.17\n-0.0093\n0.00023\nNaN\nNaN\n\n\nNumber_of_variable_14\n0.009\n0.062\n-0.075\n0.62\n7\n-5.1\n-12\n11\n9.4\n-21\n15\n-6.2\n1.6\n-0.27\n0.028\n-0.0016\n4.2e-05\nNaN\n\n\nNumber_of_variable_15\n0.0097\n0.061\n-0.3\n3.4\n-0.93\n-2\n-0.61\n1.2\n1.2\n-0.79\n-1.2\n1.6\n-0.81\n0.24\n-0.043\n0.0047\n-0.00029\n7.8e-06\n\n\n\n\n\n\n\nLet‚Äôs plot the training error versus the test error below and identify the overfitting\n\ncoef_matrix_simple[['mrss_train', 'mrss_test']].plot()\nax = plt.gca()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\n\n\n\n11.2.5 Overfitting Indicated by Training and Test MRSS Trends\nAs observed in the plot:\n- The Training Mean Residual Sum of Squares (MRSS) consistently decreases as the number of features increases.\n- However, after a certain point, the Test MRSS starts to rise, indicating that the model is no longer generalizing well to unseen data.\nThis trend suggests that while adding more features helps the model fit the training data better, it also causes the model to memorize noise, leading to poor performance on the test set.\nThis is a classic sign of overfitting, where the model captures excessive complexity in the data rather than the true underlying pattern.\nNext, let‚Äôs mitigate the overfitting issue using regularization",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#regularization-combating-overfitting",
    "href": "Regularization in Python.html#regularization-combating-overfitting",
    "title": "11¬† Regularization",
    "section": "11.3 Regularization: Combating Overfitting",
    "text": "11.3 Regularization: Combating Overfitting\nRegularization is a technique that modifies the loss function by adding a penalty term to control model complexity.\nThis helps prevent overfitting by discouraging large coefficients in the model.\n\n11.3.1 Regularized Loss Function\nThe regularized loss function is given by:\n\\(L_{reg}(\\beta) = L(\\beta) + \\alpha R(\\beta)\\)\nwhere: - \\(L(\\beta)\\) is the original loss function (e.g., Mean Squared Error in linear regression).\n- \\(R(\\beta)\\) is the regularization term, which penalizes large coefficients.\n- \\(\\alpha\\) is a hyperparameter that controls the strength of regularization.\n\n\n11.3.2 Regularization Does Not Penalize the Intercept\n\nThe intercept (bias term) captures the baseline mean of the target variable.\n\nPenalizing the intercept would shift predictions incorrectly instead of controlling complexity.\n\nThus, regularization only applies to feature coefficients, not the intercept.\n\n\n\n11.3.3 Types of Regularization\n\nL1 Regularization (Lasso Regression): Encourages sparsity by driving some coefficients to zero.\n\nL2 Regularization (Ridge Regression): Shrinks coefficients but keeps all of them nonzero.\n\nElastic Net: A combination of both L1 and L2 regularization.\n\nBy applying regularization, we obtain models that balance bias-variance tradeoff, leading to better generalization.\n\n\n11.3.4 Why Is Feature Scaling Required in Regularization?\n\n11.3.4.1 The Effect of Feature Magnitudes on Regularization\nRegularization techniques such as Lasso (L1), Ridge (L2), and Elastic Net apply penalties to the model‚Äôs coefficients. However, when features have vastly different scales, regularization disproportionately affects certain features, leading to:\n\nUneven shrinkage of coefficients, causing instability in the model.\nIncorrect feature importance interpretation, as some features dominate due to their larger numerical scale.\nSuboptimal performance, since regularization penalizes large coefficients more, even if they belong to more informative features.\n\n\n\n11.3.4.2 Example: The Need for Feature Scaling\nImagine a dataset with two features:\n\nFeature 1: Number of bedrooms (range: 1-5).\n\nFeature 2: House area in square feet (range: 500-5000).\n\nSince house area has much larger values, the model assigns smaller coefficients to compensate, making regularization unfairly biased toward smaller-scale features.\n\n\n11.3.4.3 How to Scale Features for Regularization\nTo ensure fair treatment of all features, apply feature scaling before training a regularized model:\n\n11.3.4.3.1 Standardization (Recommended)\n\\[\nx_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma}\n\\] - Centers the data around zero with unit variance. - Used in Lasso, Ridge, and Elastic Net.\n\n\n11.3.4.3.2 Min-Max Scaling\n\\[\nx_{\\text{scaled}} = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n\\] - Scales features to a fixed [0, 1] range. - Common for neural networks but less effective for regularization.\nLet‚Äôs use StandardScaler to scale the features\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Feature scaling on X_train\nX_train_std = scaler.fit_transform(X_train)\ncolumns = data.drop('y', axis=1).columns\nX_train_std = pd.DataFrame(X_train_std, columns = columns)\nX_train_std.head()\n# Feature scaling on X_test\nX_test_std = scaler.transform(X_test)\nX_test_std = pd.DataFrame(X_test_std, columns = columns)\nX_test_std.head()\n\n\n\n\n\n\n\n\nx\nx_2\nx_3\nx_4\nx_5\nx_6\nx_7\nx_8\nx_9\nx_10\nx_11\nx_12\nx_13\nx_14\nx_15\n\n\n\n\n0\n-0.96\n-1\n-0.89\n-0.78\n-0.69\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n1\n-0.061\n-0.34\n-0.49\n-0.55\n-0.57\n-0.56\n-0.53\n-0.5\n-0.47\n-0.45\n-0.42\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n2\n-1.8\n-1.2\n-0.95\n-0.8\n-0.7\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n3\n0.21\n-0.051\n-0.24\n-0.36\n-0.43\n-0.46\n-0.47\n-0.46\n-0.45\n-0.43\n-0.41\n-0.4\n-0.38\n-0.36\n-0.35\n\n\n4\n-1.7\n-1.2\n-0.95\n-0.8\n-0.7\n-0.62\n-0.57\n-0.52\n-0.48\n-0.45\n-0.43\n-0.4\n-0.38\n-0.37\n-0.35\n\n\n\n\n\n\n\nIn the next section, we will explore different types of regularization techniques and see how they help in preventing overfitting.\n\n\n\n\n11.3.5 Ridge Regression: L2 Regularization\nRidge regression is a type of linear regression that incorporates L2 regularization to prevent overfitting by penalizing large coefficients.\n\n11.3.5.1 Ridge Regression Loss Function\nThe regularized loss function for Ridge regression is given by:\n\\[\nL_{\\text{Ridge}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\beta^\\top x_i|^2 + \\alpha \\sum_{j=1}^{J} \\beta_j^2\n\\]\nwhere:\n\n\\(y_i \\text{ is the true target value for observation } i.\\)\n\\(x_i \\text{ is the feature vector for observation } i.\\)\n\\(\\beta \\text{ is the vector of model coefficients.}\\)\n\\(\\alpha \\text{ is the regularization parameter, which controls the penalty strength.}\\)\n\\(\\sum_{j=1}^{J} \\beta_j^2 \\text{ is the L2 norm (sum of squared coefficients).}\\)\n\nNote that \\(j\\) starts from 1, excluding the intercept from regularization.\nThe penalty term in Ridge regression,\n\\[\n\\sum_{j=1}^{J} \\beta_j^2 = ||\\beta||_2^2\n\\]\nis the squared L2 norm of the coefficient vector \\(\\beta\\).\nMinimizing this norm ensures that the model coefficients remain small and stable, reducing sensitivity to variations in the data.\nLet‚Äôs build a Ridge Regression model using scikit-learn, The alpha parameter controls the strength of the regularization:\n\nfrom sklearn.linear_model import Ridge\n# defining a function which will fit ridge regression model, plot the results, and return the coefficients\ndef ridge_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    ridgereg = Ridge(alpha=alpha)\n    ridgereg.fit(train_x, train_y)\n    train_y_pred = ridgereg.predict(train_x)\n    test_y_pred = ridgereg.predict(test_x)\n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([ridgereg.intercept_])\n    ret.extend(ridgereg.coef_)\n    \n    return ret\n\nLet‚Äôs experiment with different values of alpha in Ridge Regression and observe how it affects the model‚Äôs coefficients and performance.\n\n#initialize a dataframe to store the coefficient:\nalpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0, 10)]\ncoef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_ridge.iloc[i,] = ridge_regression(X_train_std, train_y, X_test_std, test_y, alpha_ridge[i], models_to_plot)\n\n\n\n\n\n\n\n\nAs we can observe, when increasing alpha, the model becomes simpler, with coefficients shrinking more aggressively due to stronger regularization. This reduces the risk of overfitting but may lead to underfitting if alpha is set too high.\nNext, let‚Äôs output the training error versus the test error and examine how the feature coefficients change with different alpha values.\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_ridge\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.0092\n0.059\n-0.072\n-2.8\n1.9e+02\n-1.3e+03\n-6e+03\n1.1e+05\n-5.6e+05\n1.5e+06\n-2e+06\n7.5e+05\n1.4e+06\n-1.4e+06\n-7.5e+05\n2e+06\n-1.3e+06\n2.8e+05\n\n\nalpha_1e-10\n0.016\n0.036\n-0.072\n12\n-1.3e+02\n7.1e+02\n-1.8e+03\n1.8e+03\n1.1e+03\n-2.8e+03\n-7.9e+02\n2.4e+03\n1.5e+03\n-1.4e+03\n-2.1e+03\n3.5e+02\n2.2e+03\n-1e+03\n\n\nalpha_1e-08\n0.017\n0.034\n-0.072\n8.1\n-70\n2.8e+02\n-5.6e+02\n4.2e+02\n1.7e+02\n-2.6e+02\n-1.8e+02\n1e+02\n1.8e+02\n34\n-1.1e+02\n-79\n64\n3.8\n\n\nalpha_0.0001\n0.019\n0.024\n-0.072\n2.9\n-8.2\n4.5\n-0.022\n-1.3\n0.31\n1.8\n2.1\n1.1\n-0.44\n-1.8\n-2.4\n-1.9\n-0.06\n3.1\n\n\nalpha_0.001\n0.019\n0.023\n-0.072\n2.5\n-5.6\n-0.5\n1.3\n1.6\n1.4\n0.84\n0.21\n-0.39\n-0.82\n-1\n-0.91\n-0.48\n0.28\n1.3\n\n\nalpha_0.01\n0.022\n0.024\n-0.072\n1.9\n-4\n-1.3\n0.73\n1.5\n1.3\n0.82\n0.23\n-0.26\n-0.58\n-0.69\n-0.6\n-0.31\n0.13\n0.72\n\n\nalpha_1\n0.089\n0.093\n-0.072\n0.08\n-0.61\n-0.48\n-0.22\n-0.011\n0.13\n0.2\n0.22\n0.21\n0.17\n0.12\n0.058\n-0.0066\n-0.072\n-0.14\n\n\nalpha_5\n0.12\n0.13\n-0.072\n-0.17\n-0.3\n-0.24\n-0.14\n-0.055\n0.0059\n0.046\n0.069\n0.08\n0.081\n0.077\n0.069\n0.057\n0.045\n0.031\n\n\nalpha_10\n0.14\n0.14\n-0.072\n-0.19\n-0.24\n-0.18\n-0.12\n-0.058\n-0.014\n0.018\n0.039\n0.053\n0.06\n0.063\n0.064\n0.062\n0.059\n0.054\n\n\nalpha_20\n0.16\n0.17\n-0.072\n-0.18\n-0.19\n-0.15\n-0.097\n-0.055\n-0.022\n0.0025\n0.021\n0.034\n0.043\n0.049\n0.053\n0.056\n0.057\n0.057\n\n\n\n\n\n\n\nTo better observe the pattern, let‚Äôs visualize how the coefficients change as we increase \\(\\alpha\\)\n\ndef plot_ridge_reg_coeff(train_x):\n    alphas = np.logspace(3,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        ridge = Ridge(alpha = a)\n        ridge.fit(train_x, train_y)\n        coefs.append(ridge.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Feature coefficient')\n    plt.legend(train_x.columns );    \nplot_ridge_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test.png\")\n\n\n\n\n\n\n\n\nAs we can see, as \\(\\alpha\\) increases, the coefficients become smaller and approach zero. Now, let‚Äôs examine the number of zero coefficients.\n\ncoef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15     0\nalpha_1e-10     0\nalpha_1e-08     0\nalpha_0.0001    0\nalpha_0.001     0\nalpha_0.01      0\nalpha_1         0\nalpha_5         0\nalpha_10        0\nalpha_20        0\ndtype: int32\n\n\nLet‚Äôs plot how the test error and training error change as we increase \\(\\alpha\\)\n\ncoef_matrix_ridge[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\nAs we can observe, as \\(ùúÜ\\) increases beyond a certain value, both the training MRSS and test MRSS begin to rise, indicating that the model starts underfitting.\n\n\n\n11.3.6 Lasso Regression: L1 Regularization\nLASSO stands for Least Absolute Shrinkage and Selection Operator. There are two key aspects in this name:\n- ‚ÄúAbsolute‚Äù refers to the use of the absolute values of the coefficients in the penalty term.\n- ‚ÄúSelection‚Äù highlights LASSO‚Äôs ability to shrink some coefficients to exactly zero, effectively performing feature selection.\nLasso regression performs L1 regularization, which helps prevent overfitting by penalizing large coefficients and enforcing sparsity in the model.\n\n11.3.6.1 Lasso Regression Loss Function\nIn standard linear regression, the loss function is the Mean Squared Error (MSE):\n\\[L_{\\text{MSE}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2\\]\nLASSO modifies this by adding an L1 regularization penalty, leading to the following regularized loss function:\n\\[L_{\\text{Lasso}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2 + \\alpha  \\sum_{j=1}^{J} |\\beta_j|\\]\nwhere:\n\n\\(y_i \\text{ is the true target value for observation } i.\\)\n\\(x_i \\text{ is the feature vector for observation } i.\\)\n\\(\\beta \\text{ is the vector of model coefficients.}\\)\n\\(\\alpha \\text{ is the regularization parameter, which controls the penalty strength.}\\)\n\\(\\sum_{j=1}^{J} |\\beta_j| \\text{ is the } \\mathbf{L_1} \\text{ norm (sum of absolute values of coefficients).}\\)\n\nThe penalty term in Lasso regression,\n\\[\n\\sum_{j=1}^{J} |\\beta_j| = ||\\beta||_1\n\\]\nis the L1 norm of the coefficient vector ( \\(\\beta\\) ).\nMinimizing this norm encourages sparsity, meaning some coefficients become exactly zero, leading to an automatically selected subset of features.\nNext, let‚Äôs build a Lasso Regression model. Similar to Ridge regression, we will explore a range of values for the regularization parameter alpha.\n\nfrom sklearn.linear_model import Lasso\nalpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 0.1, 1, 5]\n\n\n# Defining a function which will fit lasso regression model, plot the results, and return the coefficient\ndef lasso_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    if alpha == 0:\n        lassoreg = LinearRegression()    \n    else:\n        lassoreg = Lasso(alpha=alpha, max_iter=200000000, tol=0.01)\n    lassoreg.fit(train_x, train_y)\n    train_y_pred = lassoreg.predict(train_x)\n    test_y_pred = lassoreg.predict(test_x)\n        \n        \n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0:1], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([lassoreg.intercept_])\n    ret.extend(lassoreg.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the coefficient:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0, 10)]\ncoef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 0.1:236}\n\n\nmodels_to_plot\n\n{1e-10: 231, 1e-05: 232, 0.0001: 233, 0.001: 234, 0.01: 235, 0.1: 236}\n\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_lasso.iloc[i,] = lasso_regression(X_train_std, train_y, X_test_std, test_y, alpha_lasso[i], models_to_plot)\n\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_lasso\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-10\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-08\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-05\n0.019\n0.024\n-0.072\n2.8\n-6.8\n0.73\n1.7\n1.6\n0.93\n0.21\n0\n-0.6\n-0.68\n-0.55\n-0.3\n-0\n0.023\n0.71\n\n\nalpha_0.0001\n0.02\n0.024\n-0.072\n2.7\n-6.5\n0\n2.8\n1.4\n0\n0\n-0\n-0\n-0.68\n-0.43\n-0\n-0\n0\n0.35\n\n\nalpha_0.001\n0.024\n0.026\n-0.072\n2\n-4.5\n-0\n0\n2.6\n0\n0\n0\n-0\n-0\n-0\n-0\n-0.37\n-0\n-0\n\n\nalpha_0.01\n0.084\n0.088\n-0.072\n0.27\n-1.3\n-0\n-0\n0\n0\n0\n0.68\n0\n0\n0\n0\n0\n0\n-0\n\n\nalpha_0.1\n0.19\n0.2\n-0.072\n-0.22\n-0.27\n-0\n-0\n-0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0.12\n\n\nalpha_1\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\nalpha_5\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\n\n\n\n\n\n\ndef plot_lasso_reg_coeff(train_x):\n    alphas = np.logspace(1,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        laso = Lasso(alpha=a, max_iter = 100000)\n        laso.fit(train_x, train_y)\n        coefs.append(laso.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Standardized coefficient')\n    plt.legend(train_x.columns );    \nplot_lasso_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test1.png\")\n\n\n\n\n\n\n\n\n\ncoef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15      0\nalpha_1e-10      0\nalpha_1e-08      0\nalpha_1e-05      2\nalpha_0.0001     8\nalpha_0.001     11\nalpha_0.01      12\nalpha_0.1       12\nalpha_1         15\nalpha_5         15\ndtype: int32\n\n\n\ncoef_matrix_lasso[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test'])\n\n\n\n\n\n\n\n\nEffect of alpha in Lasso Regression - Small alpha (close to 0): The penalty is minimal, and Lasso behaves like ordinary linear regression. - Moderate alpha: Some coefficients shrink, and some become exactly zero, performing feature selection. - Large alpha: Many coefficients become zero, leading to a very simple model (potentially underfitting).\n\n\n\n11.3.7 Elastic Net Regression: Combining L1 and L2 Regularization\n\n11.3.7.1 Mathematical Formulation*\nElastic Net regression combines both L1 (Lasso) and L2 (Ridge) penalties, balancing feature selection and coefficient shrinkage. The regularized loss function for Elastic Net is given by:\n\\[\nL_{\\text{ElasticNet}}(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\beta^\\top x_i)^2 + \\alpha \\left( (1 - \\rho) \\frac{1}{2} \\sum_{j=1}^{p} \\beta_j^2 + \\rho \\sum_{j=1}^{p} |\\beta_j| \\right)\n\\]\nwhere:\n\n\\(y_i\\) is the true target value for observation \\(i\\).\n\\(x_i\\) is the feature vector for observation \\(i\\).\n\\(\\beta\\) is the vector of model coefficients.\n\\(\\alpha\\) is the regularization strength parameter in scikit-learn.\n\\(\\rho\\) is the l1_ratio parameter in scikit-learn, controlling the mix of L1 and L2 penalties.\n\\(\\sum_{j=1}^{p} |\\beta_j|\\) is the L1 norm, enforcing sparsity.\n\\(\\sum_{j=1}^{p} \\beta_j^2\\) is the L2 norm, ensuring coefficient stability.\n\n\n\n11.3.7.2 Elastic Net Special Cases\n\nWhen l1_ratio = 0, Elastic Net behaves like Ridge Regression (L2 regularization).\nWhen l1_ratio = 1, Elastic Net behaves like Lasso Regression (L1 regularization).\nWhen 0 &lt; l1_ratio &lt; 1, Elastic Net balances feature selection (L1) and coefficient shrinkage (L2).\n\nNow, let‚Äôs implement an Elastic Net Regression model using scikit-learn and explore how different values of alpha and l1_ratio affect the model.\n\nfrom sklearn.linear_model import ElasticNet\nalpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 0.1, 1, 5]\nl1_ratio=0.5\n\n\n# Defining a function which will fit lasso regression model, plot the results, and return the coefficient\ndef elasticnet_regression(train_x, train_y, test_x, test_y, alpha, models_to_plot={}):\n    \n    #fit the model\n    if alpha == 0:\n        model = LinearRegression()    \n    else:\n        model = ElasticNet(alpha=alpha, max_iter=20000, l1_ratio=l1_ratio)\n    model.fit(train_x, train_y)\n    train_y_pred = model.predict(train_x)\n    test_y_pred = model.predict(test_x)\n        \n        \n    \n    #check if a plot is to be made for the entered alpha\n    if alpha in models_to_plot:\n        plt.subplot(models_to_plot[alpha])\n        # plt_tight_layout()\n        plt.plot(*sort_xy(train_x.values[:, 0], train_y_pred))\n        plt.plot(train_x.values[:, 0:1], train_y, '.')\n        \n        plt.title('Plot for alpha: %.3g'%alpha)\n        \n    #return the result in pre-defined format\n    mrss_train = sum((train_y_pred - train_y)**2)/train_x.shape[0]\n    ret = [mrss_train]\n    \n    mrss_test = sum((test_y_pred - test_y)**2)/test_x.shape[0]\n    ret.extend([mrss_test])\n    \n    ret.extend([model.intercept_])\n    ret.extend(model.coef_)\n    \n    return ret\n\n\n#initialize a dataframe to store the coefficient:\ncol = ['mrss_train', 'mrss_test', 'intercept'] + ['coef_VaR_%d'%i for i in range(1, 16)]\nind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0, 10)]\ncoef_matrix_elasticnet = pd.DataFrame(index=ind, columns=col)\n\n\n# Define the number of features for which a plot is required:\nmodels_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 0.1:236}\n\n\nmodels_to_plot\n\n{1e-10: 231, 1e-05: 232, 0.0001: 233, 0.001: 234, 0.01: 235, 0.1: 236}\n\n\n\n#Iterate over the 10 alpha values:\nplt.figure(figsize=(12, 8))\nfor i in range(10):\n    coef_matrix_elasticnet.iloc[i,] = elasticnet_regression(X_train_std, train_y, X_test_std, test_y, alpha_lasso[i], models_to_plot)\n\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.488e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.395e-01, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\nc:\\Users\\lsi8012\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.279e-02, tolerance: 1.779e-03\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\n\n#Set the display format to be scientific for ease of analysis\npd.options.display.float_format = '{:,.2g}'.format\ncoef_matrix_elasticnet\n\n\n\n\n\n\n\n\nmrss_train\nmrss_test\nintercept\ncoef_VaR_1\ncoef_VaR_2\ncoef_VaR_3\ncoef_VaR_4\ncoef_VaR_5\ncoef_VaR_6\ncoef_VaR_7\ncoef_VaR_8\ncoef_VaR_9\ncoef_VaR_10\ncoef_VaR_11\ncoef_VaR_12\ncoef_VaR_13\ncoef_VaR_14\ncoef_VaR_15\n\n\n\n\nalpha_1e-15\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-10\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-08\n0.019\n0.024\n-0.072\n2.8\n-6.9\n0.83\n1.5\n1.6\n1\n0.34\n-0.23\n-0.57\n-0.66\n-0.57\n-0.35\n-0.058\n0.27\n0.6\n\n\nalpha_1e-05\n0.019\n0.024\n-0.072\n2.7\n-6.6\n0.37\n1.7\n1.7\n1\n0.27\n-0.13\n-0.61\n-0.69\n-0.57\n-0.33\n-0.019\n0.15\n0.66\n\n\nalpha_0.0001\n0.02\n0.023\n-0.072\n2.4\n-5.5\n-0.6\n1.3\n2.2\n1.2\n0.056\n-0\n-0.36\n-0.83\n-0.68\n-0.26\n-0\n0\n0.71\n\n\nalpha_0.001\n0.028\n0.03\n-0.072\n1.6\n-3.3\n-0.6\n0\n0.99\n1.1\n0.54\n0\n0\n-0\n-0\n-0.21\n-0.25\n-0.11\n-0\n\n\nalpha_0.01\n0.076\n0.081\n-0.072\n0.31\n-1.1\n-0.48\n-0\n0\n0.14\n0.35\n0.33\n0.14\n0\n0\n0\n-0\n-0\n-0.067\n\n\nalpha_0.1\n0.15\n0.16\n-0.072\n-0.19\n-0.41\n-0.0068\n-0\n-0\n0\n0\n0\n0\n0\n0\n0.04\n0.062\n0.073\n0.075\n\n\nalpha_1\n0.48\n0.52\n-0.072\n-0.013\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\nalpha_5\n0.49\n0.53\n-0.072\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n-0\n\n\n\n\n\n\n\n\ndef plot_elasticnet_reg_coeff(train_x):\n    alphas = np.logspace(1,-3,200)\n    coefs = []\n    #X_train_std, train_y\n    for a in alphas:        \n        model = ElasticNet(alpha=a, max_iter=20000, l1_ratio=l1_ratio)\n        model.fit(train_x, train_y)\n        coefs.append(model.coef_)\n    #Visualizing the shrinkage in ridge regression coefficients with increasing values of the tuning parameter lambda\n    plt.xlabel('xlabel', fontsize=18)\n    plt.ylabel('ylabel', fontsize=18)\n    plt.plot(alphas, coefs)\n    plt.xscale('log')\n    plt.xlabel(r'$\\alpha$')\n    plt.ylabel('Standardized coefficient')\n    plt.legend(train_x.columns );    \nplot_elasticnet_reg_coeff(X_train_std.iloc[:,:5])\nplt.savefig(\"test2.png\")\n\n\n\n\n\n\n\n\n\ncoef_matrix_elasticnet.apply(lambda x: sum(x.values==0),axis=1)\n\nalpha_1e-15      0\nalpha_1e-10      0\nalpha_1e-08      0\nalpha_1e-05      0\nalpha_0.0001     3\nalpha_0.001      6\nalpha_0.01       7\nalpha_0.1        8\nalpha_1         14\nalpha_5         15\ndtype: int32\n\n\n\ncoef_matrix_elasticnet[['mrss_train', 'mrss_test']].plot()\nplt.xlabel('Features')\nplt.ylabel('MRSS')\nplt.xticks(rotation=90)\nplt.legend(['train', 'test']);\n\n\n\n\n\n\n\n\nElasticNet is controlled by these key parameters:\n\nalpha (float, default=1.0):\nThe regularization strength multiplier. Higher values increase regularization.\nl1_ratio (float, default=0.5):\nThe mixing parameter between L1 and L2 penalties:\n\nl1_ratio = 0: Pure Ridge regression\n\nl1_ratio = 1: Pure Lasso regression\n\n0 &lt; l1_ratio &lt; 1: ElasticNet with mixed penalties\n\n\n\n\n\n11.3.8 RidgeCV, LassoCV, and ElasticNetCV in Scikit-Learn\nIn Scikit-Learn, RidgeCV, LassoCV, and ElasticNetCV are cross-validation (CV) versions of Ridge, Lasso, and Elastic Net regression models, respectively. These versions automatically select the best regularization strength (alpha) by performing internal cross-validation.\n\n11.3.8.1 Overview of RidgeCV, LassoCV, and ElasticNetCV**\n\n\n\n\n\n\n\n\n\nModel\nRegularization Type\nPurpose\nHow Alpha is Chosen?\n\n\n\n\nRidgeCV\nL2 (Ridge)\nShrinks coefficients to handle overfitting, but keeps all features.\nUses cross-validation to select the best alpha.\n\n\nLassoCV\nL1 (Lasso)\nShrinks coefficients, but also removes some features by setting coefficients to zero.\nUses cross-validation to find the best alpha.\n\n\nElasticNetCV\nL1 + L2 (Elastic Net)\nBalances Ridge and Lasso.\nUses cross-validation to find the best alpha and l1_ratio.\n\n\n\n\n\n11.3.8.2 How to Use RidgeCV, LassoCV, andElasticNetCV\nEach model automatically selects the optimal alpha value through internal cross-validation without using a loop through the alpha values\n\nfrom sklearn.linear_model import LassoCV\n\nalpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 5]\n\n# Initialize LassoCV with cross-validation\nlasso_cv = LassoCV(alphas=alpha_lasso, cv=5, max_iter=2000000)\n\n# Fit the model using training data\nlasso_cv.fit(X_train_std, train_y)\n\n# Make predictions\ntrain_y_pred = lasso_cv.predict(X_train_std)\ntest_y_pred = lasso_cv.predict(X_test_std)\n\n# Get the best alpha chosen by cross-validation\nbest_alpha = lasso_cv.alpha_\nprint(f\"Best alpha selected by LassoCV: {best_alpha}\")\n\n# Check if a plot should be made for the selected alpha\nif best_alpha in models_to_plot:\n    plt.subplot(models_to_plot[best_alpha])\n    plt.plot(*sort_xy(train_x[:, 0], train_y_pred))\n    plt.plot(train_x[:, 0:1], train_y, '.', label=\"Actual Data\")\n    \n    plt.title(f'Plot for alpha: {best_alpha:.3g}')\n    plt.legend();",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Regularization in Python.html#reference",
    "href": "Regularization in Python.html#reference",
    "title": "11¬† Regularization",
    "section": "11.4 Reference",
    "text": "11.4 Reference\nhttps://www.linkedin.com/pulse/tutorial-ridge-lasso-regression-subhajit-mondal/?trk=read_related_article-card_title",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Regularization</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html",
    "href": "Feature Selection.html",
    "title": "12¬† Feature Selection",
    "section": "",
    "text": "12.1 Filter Methods (Statistical approaches)\nFeature selection is a crucial step in machine learning that helps improve model performance, reduce overfitting, and speed up training time by selecting the most relevant features from the dataset.\nThere are three main types of feature selection methods:\nWhy feature selection?\nFilter methods evaluate the relevance of features based on statistical measures before training a model. These methods rank features according to their correlation with the target variable and select the most relevant ones.",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html#filter-methods-statistical-approaches",
    "href": "Feature Selection.html#filter-methods-statistical-approaches",
    "title": "12¬† Feature Selection",
    "section": "",
    "text": "12.1.1 Characteristics:\n\nIndependent of the model: Feature selection is performed before training the model.\nFast and computationally efficient: Since no model training is required, they are suitable for large datasets.\nProne to selecting irrelevant features: They do not consider interactions between features.\nCommon techniques:\n\nPearson correlation coefficient\nMutual information\nChi-square test\nANOVA (Analysis of Variance)\nVariance thresholding\n\n\n\n\n\n\n\n\n\n12.1.2 Methods\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nPython Function/Class\nNotes\n\n\n\n\nVariance Threshold\nRemoves low-variance numeric features\nVarianceThreshold() (scikit-learn)\nOnly works on numerical features\n\n\nCorrelation Filter\nRemoves highly correlated features\nCustom implementation + df.corr()\nRequires manual analysis (pandas) or correlation_threshold methods\n\n\nChi-Square Test\nSelects categorical features for classification\nchi2() + SelectKBest()\nFor categorical targets, requires non-negative features\n\n\nMutual Information\nMeasures dependency for classification/regression\nmutual_info_classif()/regression()\nUsed with SelectKBest() or SelectPercentile()\n\n\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n\n# Load the data\ndf = pd.read_csv('./Datasets/Credit.csv', index_col=0)\n\n# Separate features and target\n# Let's use 'Balance' as our target variable\nX = df.drop('Balance', axis=1)\ny = df['Balance']\n\n# Preprocessing: Encode categorical variables\nX_encoded = pd.get_dummies(X, columns=['Gender', 'Student', 'Married', 'Ethnicity'])\n\n# 1. Variance Threshold Feature Selection\ndef variance_threshold_selection(X, threshold=0.1):\n    # Create a VarianceThreshold selector\n    selector = VarianceThreshold(threshold=threshold)\n    \n    # Fit and transform the data\n    X_selected = selector.fit_transform(X)\n    \n    # Get the selected feature names\n    selected_features = X.columns[selector.get_support()]\n    \n    return X_selected, selected_features\n\n# Apply Variance Threshold\nX_var_selected, var_selected_features = variance_threshold_selection(X_encoded)\n\nprint(\"1. Variance Threshold Feature Selection:\")\nprint(\"Selected features:\", list(var_selected_features))\nprint(\"Number of features reduced from\", X_encoded.shape[1], \"to\", len(var_selected_features))\n\n1. Variance Threshold Feature Selection:\nSelected features: ['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender_ Male', 'Gender_Female', 'Married_No', 'Married_Yes', 'Ethnicity_African American', 'Ethnicity_Asian', 'Ethnicity_Caucasian']\nNumber of features reduced from 15 to 13\n\n\n\n# 2. Correlation-based Feature Removal\ndef remove_highly_correlated_features(X, threshold=0.8):\n    # Compute the correlation matrix\n    corr_matrix = X.corr().abs()\n    \n    # Create a mask of the upper triangle of the correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    \n    # Find features to drop\n    to_drop = [column for column in upper.columns if any(upper[column] &gt; threshold)]\n    \n    # Drop highly correlated features\n    X_reduced = X.drop(columns=to_drop)\n    \n    return X_reduced, to_drop\n\n# Apply Correlation-based Feature Removal\nX_corr_removed, dropped_features = remove_highly_correlated_features(X_encoded)\n\nprint(\"\\n3. Correlation-based Feature Removal:\")\nprint(\"Dropped features:\", dropped_features)\nprint(\"Number of features reduced from\", X_encoded.shape[1], \"to\", X_corr_removed.shape[1])\n\n# Visualization of Correlation Matrix\nplt.figure(figsize=(20,16))\ncorrelation_matrix = X_encoded.corr()\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\", square=True)\nplt.title('Feature Correlation Matrix')\nplt.tight_layout();\n\n\n3. Correlation-based Feature Removal:\nDropped features: ['Rating', 'Gender_Female', 'Student_Yes', 'Married_Yes']\nNumber of features reduced from 15 to 11\n\n\n\n\n\n\n\n\n\n\n# 3. Correlation-based Feature Selection (with target)\ndef correlation_with_target_selection(X, y, threshold=0.2):\n    # Calculate correlation with target\n    correlations = X.apply(lambda col: col.corr(y))\n    \n    # Select features above the absolute correlation threshold\n    selected_features = correlations[abs(correlations) &gt;= threshold].index\n    \n    return X[selected_features], selected_features\n\n# Apply Correlation with Target Selection\nX_corr_target, corr_target_features = correlation_with_target_selection(X_encoded, y)\n\nprint(\"\\n2. Correlation with Target Feature Selection:\")\nprint(\"Selected features:\", list(corr_target_features))\nprint(\"Number of features reduced from\", X_encoded.shape[1], \"to\", len(corr_target_features))\n\n\n2. Correlation with Target Feature Selection:\nSelected features: ['Income', 'Limit', 'Rating', 'Student_No', 'Student_Yes']\nNumber of features reduced from 15 to 5\n\n\n\n# Calculate correlations with target\ncorrelations = X_encoded.apply(lambda col: col.corr(y))\n\n# Sort correlations by absolute value in descending order\ncorrelations_sorted = correlations.abs().sort_values(ascending=False)\n\n# Prepare the visualization\nplt.figure(figsize=(15, 10))\ncorrelations_sorted.plot(kind='bar')\nplt.title('Feature Correlations with Balance (Absolute Values)', fontsize=16)\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Absolute Correlation', fontsize=12)\nplt.xticks(rotation=90)\nplt.tight_layout()\n\n# Print out the sorted correlations\nprint(\"Correlations with Balance (sorted by absolute value):\")\nfor feature, corr in correlations_sorted.items():\n    print(f\"{feature}: {corr}\")\n\nCorrelations with Balance (sorted by absolute value):\nRating: 0.863625160621495\nLimit: 0.8616972670153951\nIncome: 0.46365645701575736\nStudent_No: 0.2590175474501476\nStudent_Yes: 0.2590175474501476\nCards: 0.08645634741861911\nGender_ Male: 0.021474006717338588\nGender_Female: 0.021474006717338588\nEthnicity_African American: 0.013719801718047214\nEthnicity_Asian: 0.00981223592782398\nEducation: 0.00806157645355343\nMarried_Yes: 0.005673490217239985\nMarried_No: 0.005673490217239973\nEthnicity_Caucasian: 0.003288321172097514\nAge: 0.0018351188590736563\n\n\n\n\n\n\n\n\n\n\n# 4. SelectKBest Feature Selection\ndef select_k_best_features(X, y, k=5):\n    # Create a SelectKBest selector\n    selector = SelectKBest(score_func=f_regression, k=k)\n    \n    # Fit and transform the data\n    X_selected = selector.fit_transform(X, y)\n    \n    # Get the selected feature names\n    selected_features = X.columns[selector.get_support()]\n    \n    return X_selected, selected_features\n\n# Apply SelectKBest\nX_k_best, k_best_features = select_k_best_features(X_encoded, y)\n\nprint(\"\\n4. SelectKBest Feature Selection:\")\nprint(\"Selected features:\", list(k_best_features))\nprint(\"Number of features reduced from\", X_encoded.shape[1], \"to\", len(k_best_features))\n\n\n4. SelectKBest Feature Selection:\nSelected features: ['Income', 'Limit', 'Rating', 'Student_No', 'Student_Yes']\nNumber of features reduced from 15 to 5",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html#wrapper-methods",
    "href": "Feature Selection.html#wrapper-methods",
    "title": "12¬† Feature Selection",
    "section": "12.2 Wrapper Methods",
    "text": "12.2 Wrapper Methods\nWrapper methods evaluate subsets of features by actually training and testing a model on different feature combinations. These methods optimize feature selection based on model performance.\n\n12.2.1 Characteristics:\n\nModel-dependent: They rely on training a model to assess feature usefulness.\nComputationally expensive: Since multiple models need to be trained, they can be slow, especially for large datasets.\nMore accurate than filter methods: They account for feature interactions and can find the optimal subset.\nRisk of overfitting: Since they optimize for a specific dataset, the selected features may not generalize well to new data.\nCommon techniques:\n\nRecursive Feature Elimination (RFE)\nSequential Feature Selection (SFS) (Forward/Backward Selection)\n\nWrapper methods can be explained with the help of following graphic:\n\n\n\n\n\n\n\n\n12.2.2 Recursive Feature Elimination (RFE)\nThis is how RFE works:\n\nStart with all features in the dataset.\nTrain a model (e.g., Random Forest, SVM, Logistic Regression).\nRank feature importance using model coefficients (coef_) or feature importance scores (feature_importances_).\nRemove the least important feature(s).\nRepeat steps 2-4 recursively until reaching the desired number of features.\nReturn the best subset of features.\n\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Load dataset\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n# Initialize a model\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Apply Recursive Feature Elimination (RFE)\nrfe_selector = RFE(model, n_features_to_select=5)  # Select top 5 features\nX_rfe_selected = rfe_selector.fit_transform(X, y)\n\n# Get selected feature names\nrfe_selected_features = X.columns[rfe_selector.support_]\nprint(\"Selected features using RFE:\", rfe_selected_features.tolist())\n\nSelected features using RFE: ['mean concave points', 'worst radius', 'worst perimeter', 'worst area', 'worst concave points']\n\n\n\n\n12.2.3 Recursive Feature Elimination with Cross-Validation (RFECV)\nRFECV is an extension of Recursive Feature Elimination (RFE) that automatically selects the optimal number of features using cross-validation.\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\n# Load dataset\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n# Initialize model (Random Forest)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Define cross-validation strategy\ncv = StratifiedKFold(n_splits=5)\n\n# Apply RFECV\nrfecv_selector = RFECV(estimator=model, step=1, cv=cv, scoring='accuracy', n_jobs=-1)\nX_rfecv_selected = rfecv_selector.fit_transform(X, y)\n\n# Get selected feature names\nrfecv_selected_features = X.columns[rfecv_selector.support_]\nprint(\"Optimal number of features:\", rfecv_selector.n_features_)\nprint(\"Selected features using RFECV:\", rfecv_selected_features.tolist())\n\n# Print feature rankings\nfeature_ranking = pd.DataFrame({'Feature': X.columns, 'Ranking': rfecv_selector.ranking_})\nprint(feature_ranking.sort_values(by='Ranking'))\n\nOptimal number of features: 16\nSelected features using RFECV: ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean concavity', 'mean concave points', 'radius error', 'area error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points']\n                    Feature  Ranking\n0               mean radius        1\n22          worst perimeter        1\n23               worst area        1\n13               area error        1\n24         worst smoothness        1\n25        worst compactness        1\n10             radius error        1\n21            worst texture        1\n26          worst concavity        1\n7       mean concave points        1\n6            mean concavity        1\n3                 mean area        1\n2            mean perimeter        1\n1              mean texture        1\n27     worst concave points        1\n20             worst radius        1\n28           worst symmetry        2\n5          mean compactness        3\n4           mean smoothness        4\n29  worst fractal dimension        5\n12          perimeter error        6\n16          concavity error        7\n15        compactness error        8\n14         smoothness error        9\n18           symmetry error       10\n17     concave points error       11\n19  fractal dimension error       12\n11            texture error       13\n8             mean symmetry       14\n9    mean fractal dimension       15\n\n\n\n\n12.2.4 Sequential Feature Selection\nSFS iteratively selects the most relevant features by adding them one at a time (forward selection) or removing them one at a time (backward elimination) while evaluating model performance at each step.\nThe selection strategy is controlled using: - direction='forward' for feature addition - direction='backward' for feature removal\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Initialize a model\nmodel = LogisticRegression(max_iter=1000)\n\n# Apply Sequential Feature Selection (SFS)\nsfs_selector = SequentialFeatureSelector(model, n_features_to_select=5, direction='forward', n_jobs=-1)\nX_sfs_selected = sfs_selector.fit_transform(X, y)\n\n# Get selected feature names\nsfs_selected_features = X.columns[sfs_selector.get_support()]\nprint(\"Selected features using SFS:\", sfs_selected_features.tolist())\n\nSelected features using SFS: ['mean radius', 'radius error', 'worst texture', 'worst perimeter', 'worst compactness']\n\n\nLet‚Äôs use them on the credit dataset\n\n\n# Load the data\ndf = pd.read_csv('./Datasets/Credit.csv', index_col=0)\n\n# Separate features and target\nX = df.drop('Balance', axis=1)\ny = df['Balance']\n\n# Preprocessing\n# One-hot encode categorical variables\nX_encoded = pd.get_dummies(X, columns=['Gender', 'Student', 'Married', 'Ethnicity'])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 1. Recursive Feature Elimination with Cross-Validation (RFECV)\ndef perform_rfecv(X_train, y_train, X_test, y_test):\n    # Use both Linear Regression and Random Forest\n    estimators = [\n        ('Linear Regression', LinearRegression()),\n        ('Random Forest', RandomForestRegressor(n_estimators=100, random_state=42))\n    ]\n    \n    results = {}\n    \n    for name, estimator in estimators:\n        # RFECV with cross-validation\n        rfecv = RFECV(\n            estimator=estimator, \n            step=1, \n            cv=5, \n            scoring='neg_mean_squared_error'\n        )\n        \n        # Fit RFECV\n        rfecv.fit(X_train, y_train)\n        \n        # Get selected features\n        selected_features = X_train.columns[rfecv.support_]\n        \n        # Prepare reduced datasets\n        X_train_reduced = X_train[selected_features]\n        X_test_reduced = X_test[selected_features]\n        \n        # Fit the model on reduced dataset\n        estimator.fit(X_train_reduced, y_train)\n        \n        # Predict and evaluate\n        y_pred = estimator.predict(X_test_reduced)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        \n        results[name] = {\n            'Selected Features': list(selected_features),\n            'Number of Features': len(selected_features),\n            'MSE': mse,\n            'R2': r2\n        }\n    \n    return results\n\n# 2. Sequential Feature Selection\ndef perform_sequential_feature_selection(X_train, y_train, X_test, y_test):\n    # Use both Linear Regression and Random Forest\n    estimators = [\n        ('Linear Regression', LinearRegression()),\n        ('Random Forest', RandomForestRegressor(n_estimators=100, random_state=42))\n    ]\n    \n    results = {}\n    \n    for name, estimator in estimators:\n        # Sequential Forward Selection\n        sfs_forward = SequentialFeatureSelector(\n            estimator=estimator,\n            n_features_to_select='auto',  # will choose based on cross-validation\n            direction='forward',\n            cv=5,\n            scoring='neg_mean_squared_error'\n        )\n        \n        # Fit SFS\n        sfs_forward.fit(X_train, y_train)\n        \n        # Get selected features\n        selected_features = X_train.columns[sfs_forward.get_support()]\n        \n        # Prepare reduced datasets\n        X_train_reduced = X_train[selected_features]\n        X_test_reduced = X_test[selected_features]\n        \n        # Fit the model on reduced dataset\n        estimator.fit(X_train_reduced, y_train)\n        \n        # Predict and evaluate\n        y_pred = estimator.predict(X_test_reduced)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        \n        results[name] = {\n            'Selected Features': list(selected_features),\n            'Number of Features': len(selected_features),\n            'MSE': mse,\n            'R2': r2\n        }\n    \n    return results\n\n# Run feature selection methods\nprint(\"1. Recursive Feature Elimination with Cross-Validation (RFECV):\")\nrfecv_results = perform_rfecv(X_train, y_train, X_test, y_test)\n\nprint(\"\\n2. Sequential Feature Selection:\")\nsfs_results = perform_sequential_feature_selection(X_train, y_train, X_test, y_test)\n\n# Print detailed results\ndef print_feature_selection_results(results):\n    for model_name, result in results.items():\n        print(f\"\\n{model_name}:\")\n        print(f\"Number of Selected Features: {result['Number of Features']}\")\n        print(\"Selected Features:\", result['Selected Features'])\n        print(f\"Mean Squared Error: {result['MSE']:.4f}\")\n        print(f\"R2 Score: {result['R2']:.4f}\")\n\nprint(\"\\nRFECV Results:\")\nprint_feature_selection_results(rfecv_results)\n\nprint(\"\\nSequential Feature Selection Results:\")\nprint_feature_selection_results(sfs_results)\n\n1. Recursive Feature Elimination with Cross-Validation (RFECV):\n\n2. Sequential Feature Selection:\n\nRFECV Results:\n\nLinear Regression:\nNumber of Selected Features: 15\nSelected Features: ['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender_ Male', 'Gender_Female', 'Student_No', 'Student_Yes', 'Married_No', 'Married_Yes', 'Ethnicity_African American', 'Ethnicity_Asian', 'Ethnicity_Caucasian']\nMean Squared Error: 7974.8564\nR2 Score: 0.9523\n\nRandom Forest:\nNumber of Selected Features: 5\nSelected Features: ['Income', 'Limit', 'Rating', 'Student_No', 'Student_Yes']\nMean Squared Error: 9124.1126\nR2 Score: 0.9454\n\nSequential Feature Selection Results:\n\nLinear Regression:\nNumber of Selected Features: 7\nSelected Features: ['Income', 'Limit', 'Rating', 'Cards', 'Age', 'Student_No', 'Student_Yes']\nMean Squared Error: 8058.6421\nR2 Score: 0.9518\n\nRandom Forest:\nNumber of Selected Features: 7\nSelected Features: ['Income', 'Limit', 'Rating', 'Gender_Female', 'Student_No', 'Student_Yes', 'Ethnicity_African American']\nMean Squared Error: 9953.4559\nR2 Score: 0.9404",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html#embeded-methods",
    "href": "Feature Selection.html#embeded-methods",
    "title": "12¬† Feature Selection",
    "section": "12.3 Embeded methods",
    "text": "12.3 Embeded methods\nEmbedded methods integrate feature selection directly into the model training process. These methods learn which features are important while building the model, offering a balance between filter and wrapper methods.\n\n12.3.1 Characteristics:\n\nMore efficient than wrapper methods: Feature selection is built into the model training, avoiding the need for multiple iterations.\nLess prone to overfitting than wrapper methods: Regularization techniques help prevent overfitting.\nModel-dependent: The selected features are specific to the chosen model.\nCommon techniques:\n\nLasso (L1 Regularization): Shrinks less important feature coefficients to zero.\nDecision Tree-based methods: Feature importance scores from Random Forest, XGBoost, or Gradient Boosting.\nElastic Net: A combination of L1 and L2 regularization.\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Load data\ndf = pd.read_csv('./Datasets/Credit.csv').drop(columns=df.columns[0])  # Drop index column\n\n# Clean categorical variables (trim whitespace)\ncategorical_cols = ['Gender', 'Student', 'Married', 'Ethnicity']\nfor col in categorical_cols:\n    df[col] = df[col].str.strip()\n\n# Convert categorical variables to dummy variables\ndf = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n# Separate features and target\nX = df.drop(columns=['Balance'])\ny = df['Balance']\n\n# Split data into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardize features for Lasso\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Lasso Regression for feature selection\nlasso = LassoCV(cv=5, random_state=42)\nlasso.fit(X_train_scaled, y_train)\n\n# Get non-zero Lasso coefficients\nlasso_feat = pd.Series(lasso.coef_, index=X.columns)\nselected_lasso = lasso_feat[lasso_feat != 0].sort_values(ascending=False)\n\n# Random Forest for feature importance\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Get feature importances\nrf_feat = pd.Series(rf.feature_importances_, index=X.columns)\nselected_rf = rf_feat.sort_values(ascending=False)\n\n# Display results\nprint(\"Lasso Selected Features (Non-Zero Coefficients):\\n\", selected_lasso)\nprint(\"\\nRandom Forest Feature Importances:\\n\", selected_rf)\n\nLasso Selected Features (Non-Zero Coefficients):\n Limit                  408.156450\nStudent_Yes            106.371166\nCards                   36.834149\nEthnicity_Caucasian      6.009746\nGender_Male             -0.254150\nAge                    -28.253276\ndtype: float64\n\nRandom Forest Feature Importances:\n Limit                  0.458433\nRating                 0.418695\nStudent_Yes            0.046555\nAge                    0.022970\nUnnamed: 0             0.018321\nEducation              0.014302\nCards                  0.010239\nMarried_Yes            0.003657\nGender_Male            0.002616\nEthnicity_Caucasian    0.002256\nEthnicity_Asian        0.001956\ndtype: float64",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html#comparison-of-feature-selection-methods",
    "href": "Feature Selection.html#comparison-of-feature-selection-methods",
    "title": "12¬† Feature Selection",
    "section": "12.4 Comparison of Feature Selection Methods",
    "text": "12.4 Comparison of Feature Selection Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nModel Dependency\nComputational Cost\nHandles Feature Interactions\nRisk of Overfitting\n\n\n\n\nFilter\nNo\nLow\nNo\nLow\n\n\nWrapper\nYes\nHigh\nYes\nHigh\n\n\nEmbedded\nYes\nMedium\nYes\nMedium",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Feature Selection.html#conclusion",
    "href": "Feature Selection.html#conclusion",
    "title": "12¬† Feature Selection",
    "section": "12.5 Conclusion",
    "text": "12.5 Conclusion\n\nUse Filter methods when working with large datasets or when speed is a priority.\nUse Wrapper methods when accuracy is more important and computational cost is not a major concern.\nUse Embedded methods when leveraging models that inherently perform feature selection, such as Lasso regression or tree-based models.\n\nBy understanding these techniques, you can make better decisions when selecting features for machine learning models. üöÄ\n\n12.5.1 Reference:\n\nhttps://xavierbourretsicotte.github.io/subset_selection.html\nhttps://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection\nhttps://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/",
    "crumbs": [
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html",
    "href": "Assignment_1.html",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#instructions",
    "href": "Assignment_1.html#instructions",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nDo not write your name on the assignment.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nThere is a bonus question worth 15 points.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student‚Äôs identity‚Äîe.g.¬†printouts of the working directory should not be included in the final submission. (1 point)\nThere aren‚Äôt excessively long outputs of extraneous information (e.g.¬†no printouts of entire data frames without good reason, there aren‚Äôt long printouts of which iteration a loop is on, there aren‚Äôt long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 100 + 15 (bonus question) + 5 (proper formatting) = 120 out of 100. There is no partial credit for some parts of the bonus question.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#case-studies-regression-vs-classification-and-prediction-vs-inference-16-points",
    "href": "Assignment_1.html#case-studies-regression-vs-classification-and-prediction-vs-inference-16-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "1) Case Studies: Regression vs Classification and Prediction vs Inference (16 points)",
    "text": "1) Case Studies: Regression vs Classification and Prediction vs Inference (16 points)\nFor each case below, explain (1) whether it is a classification or a regression problem and (2) whether the main purpose is prediction or inference. You need justify your answers for credit.\n\n1a)\nYou work for a company that is interested in conducting a marketing campaign. The goal of your project is to identify individuals who are likely to respond positively to a marketing campaign, based on observations of demographic variables (such as age, gender, income etc.) measured on each individual. (2+2 points)\n\n\n1b)\nFor the same company, now you are working on a different project. This one is focused on understanding the impact of advertisements in different media types on the company sales. For example, you are interested in the following question: ‚ÄòHow large of an increase in sales is associated with a given increase in radio and TV advertising?‚Äô (2+2 points)\n\n\n1c)\nA company is selling furniture and they are interested in the finding the association between demographic characteristics of customers (such as age, gender, income etc.) and if they would purchase a particular company product. (2+2 points)\n\n\n1d)\nWe are interested in forecasting the % change in the USD/Euro exchange rate using the weekly changes in the stock markets of a number of countries. We collect weekly data for all of 2023. For each week, we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. (2+2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#examples-for-different-regression-metrics-rmse-vs-mae-8-points",
    "href": "Assignment_1.html#examples-for-different-regression-metrics-rmse-vs-mae-8-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "2) Examples for Different Regression Metrics: RMSE vs MAE (8 points)",
    "text": "2) Examples for Different Regression Metrics: RMSE vs MAE (8 points)\n\n2a)\nDescribe a regression problem, where it will be more proper to evaluate the model performance using the root mean squared error (RMSE) metric as compared to the mean absolute error (MAE) metric. You need to justify your answer for credit. (4 points)\nNote: You are not allowed to use the datasets and examples covered in the lectures.\n\n\n2b)\nDescribe a regression problem, where it will be more proper to evaluate the model performance using the mean absolute error (MAE) metric as compared to the root mean squared error (RMSE) metric. You need to justify your answer for credit. (4 points)\nNote: You are not allowed to use the datasets and examples covered in the lectures.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#simple-linear-regression-formulation-3-points",
    "href": "Assignment_1.html#simple-linear-regression-formulation-3-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "3) Simple Linear Regression: Formulation (3 points)",
    "text": "3) Simple Linear Regression: Formulation (3 points)\nWhen asked to state the simple linear regression model, a students wrote it as follows: \\(E(Y_i) = \\beta_0 + \\beta_1X_i + \\epsilon_i\\). Is this correct (1 point)? Justify your answer (2 points).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#modeling-the-petrol-consumption-in-u.s.-states-58-points",
    "href": "Assignment_1.html#modeling-the-petrol-consumption-in-u.s.-states-58-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "4) Modeling the Petrol Consumption in U.S. States (58 points)",
    "text": "4) Modeling the Petrol Consumption in U.S. States (58 points)\nRead petrol_consumption_train.csv. Assume that each observation is a U.S. state. For each observation, the data has the following variables as its five columns:\nPetrol_tax: Petrol tax (cents per gallon)\nPer_capita_income: Average income (dollars)\nPaved_highways: Paved Highways (miles)\nProp_license: Proportion of population with driver‚Äôs licenses\nPetrol_consumption: Consumption of petrol (millions of gallons)\n\n4a)\nCreate a pairwise plot of all the variables in the dataset. (1 point) Print the correlation matrix of all the variables as well. (1 point) Which variable has the highest linear correlation with Petrol_consumption? (1 point)\nNote: Remember that a pairwise plot is a visualization tool that you can find in the seaborn library.\n\n\n4b)\nFit a simple linear regression model to predict Petrol_consumption using the column you found in part a as the only predictor. Print the model summary. (3 points)\n\n\n4c)\nWhen asked for a point estimate of the expected petrol consumption for a state where the proportion of population with driver‚Äôs license is 54.4%, a person gave the estimate 488 million gallons because that is the mean value of Petrol_consumption for the two observations of Prop_license = 0.544 pieces in the dataset. Is there an issue with this approach? Explain. (2 points) If there is an issue, then suggest a better approach and use it to estimate the expected petrol consumption for a state where the proportion of population with driver‚Äôs license is 54.4%. (2 points)\n\n\n4d)\nWhat is the increase in petrol consumption for an increase of 0.05 in the predictor? (3 points)\n\n\n4e)\nDoes petrol consumption have a statistically significant relationship with the predictor? You need to justify your answer for credit. (3 points)\n\n\n4f)\nHow much of the variation in petrol consumption can be explained by its linear relationship with the predictor? (2 points)\n\n\n4g)\nPredict the petrol consumption for a state in which 50% of the population has a driver‚Äôs license. (2 points) What are the confidence interval (2 points) and the prediction interval (2 points) for your prediction? Which interval is wider? (1 points) Why? (2 points)\n\n\n4h)\nPredict the petrol consumption for a state in which 10% of the population has a driver‚Äôs license. (3 points) Are you getting a reasonable outcome? (1 point) Why or why not? (2 points)\n\n\n4i)\nWhat is the residual standard error of the model? (3 points)\n\n\n4j)\nUsing the trained model, predict the petrol consumption of the observations in petrol_consumption_test.csv (2 points) and find the RMSE. (2 points) What is the unit of this RMSE value? (1 point)\n\n\n4k)\nBased on the answers to part i and part j, do you think the model is overfitting? You need to justify your answer for credit. (3 points)\n\n\n4l)\nMake a scatterplot of Petrol_consumption vs.¬†the predictor using petrol_consumption_test.csv. (1 point) Over the scatterplot, plot the regression line (1 point), the prediction interval (2 points), and the confidence interval. (2 points)\nMake sure that regression line, prediction interval lines, and confidence interval lines have different colors. (1 point) Display a legend that correctly labels the lines as well. (1 point) Note that you need two lines of the same color to plot an interval.\n\n\n4m)\nThe dataset consists of 40 US States. If you combine this data with the data of the remaining 10 US States, are you likely to obtain narrower confidence and prediction intervals in the plot developed in the previous question, for the same level of confidence? Justify your answer. (2 points).\nIf yes, then can you gaurantee that the width of these intervals will reduce? Justify your answer. If no, then can you gaurantee that the width of these intervals will not reduce? Justify your answer. (2 points)\n\n\n4n)\nFind the correlation between Petrol_consumption and the rest of the variables in petrol_consumption_train.csv. Which column would have the lowest R-squared value when used as the predictor for a Simple Linear Regression model to predict Petrol_consumption? Note that you can directly answer this question from the correlation values and do not need to develop any more linear regression models. (2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#reproducing-the-results-with-scikit-learn-15-points",
    "href": "Assignment_1.html#reproducing-the-results-with-scikit-learn-15-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "5) Reproducing the Results with Scikit-Learn (15 points)",
    "text": "5) Reproducing the Results with Scikit-Learn (15 points)\n\n5a)\nUsing the same datasets, same response and the same predictor as Question 4, reproduce the following outputs with scikit-learn:\n\nModel RMSE for test data (3 points)\nR-squared value of the model (3 points)\nResidual standard error of the model (3 points)\n\nNote that you are only allowed to use scikit-learn, pandas, and numpy tools for this question. Any other libraries will not receive any credit.\n\n\n5b)\nWhich of the model outputs from Question 4 cannot be reproduced using scikit-learn? Give two answers. (2+2 points) What does this tell about scikit-learn? (2 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_1.html#bonus-question-15-points",
    "href": "Assignment_1.html#bonus-question-15-points",
    "title": "Appendix A ‚Äî Assignment 1: Statistical Learning and Simple Linear Regression",
    "section": "6) Bonus Question (15 points)",
    "text": "6) Bonus Question (15 points)\nPlease note that the bonus question requires you to look more into the usage of the tools we covered in class and it will be necessary to do your own research. We strongly suggest attempting it after you are done with the rest of the assignment.\n\n6a)\nFit a simple linear regression model to predict Petrol_consumption based on the predictor in Question 4, but without an intercept term. (5 points - no partial credit)\nWithout an intercept means that the equation becomes \\(Y = \\beta_1X\\). The intercept term, \\(\\beta_0\\), becomes 0.\nNote: You must answer this part correctly to qualify for the bonus points in the following parts.\n\n\n6b)\nPredict the petrol consumption for the observations in petrol_consumption_test.csv using the model without an intercept and find the RMSE. (1+2 points) Then, print the summary and find the R-squared. (2 points)\n\n\n6c)\nThe RMSE for the models with and without the intercept are similar, which indicates that both models are almost equally good. However, the R-squared for the model without intercept is much higher than the R-squared for the model with the intercept. Why? Justify your answer. (5 points - no partial credit)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Assignment 1: Statistical Learning and Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html",
    "href": "Assignment_2.html",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "",
    "text": "Instructions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html#instructions",
    "href": "Assignment_2.html#instructions",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "",
    "text": "You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity.\nWrite your code in the Code cells and your answers in the Markdown cells of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\nUse Quarto to render the .ipynb file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: quarto render filename.ipynb --to html. Submit the HTML file.\nFive points are properly formatting the assignment. The breakdown is as follows:\n\nMust be an HTML file rendered using Quarto (1 point). If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.\nNo name can be written on the assignment, nor can there be any indicator of the student‚Äôs identity‚Äîe.g.¬†printouts of the working directory should not be included in the final submission. (1 point)\nThere aren‚Äôt excessively long outputs of extraneous information (e.g.¬†no printouts of entire data frames without good reason, there aren‚Äôt long printouts of which iteration a loop is on, there aren‚Äôt long sections of commented-out code, etc.) (1 point)\nFinal answers to each question are written in the Markdown cells. (1 point)\nThere is no piece of unnecessary / redundant code, and no unnecessary / redundant text. (1 point)\n\nThe maximum possible score in the assigment is 100 + 5 (proper formatting) = 105 out of 100.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html#multiple-linear-regression-24-points",
    "href": "Assignment_2.html#multiple-linear-regression-24-points",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "1) Multiple Linear Regression (24 points)",
    "text": "1) Multiple Linear Regression (24 points)\nA study was conducted on 97 male patients with prostate cancer who were due to receive a radical prostatectomy (complete removal of the prostate). The prostate.csv file contains data on 9 measurements taken from these 97 patients. Each row (observation) represents a patient and each column (variable) represents a measurement. The description of variables can be found here: https://rafalab.github.io/pages/649/prostate.html\n\n1a)\nFit a linear regression model with lpsa as the response and all the other variables as the predictors. Print its summary. (2 points) Write down the optimal equation that predicts lpsa using the predictors. (2 points)\n\n\n1b)\nIs the overall regression statistically significant? In other words, is there a statistically significant relationship between the response and at least one predictor? You need to justify your answer for credit. (2 points)\n\n\n1c)\nWhat does the optimal coefficient of svi tell us as a numeric output? Make sure you include the predictor, (svi) the response (lpsa) and the other predictors in your answer. (2 points)\n\n\n1d)\nCheck the \\(p\\)-values of gleason and age. Are these predictors statistically significant? You need to justify your answer for credit. (2 points)\n\n\n1e)\nCheck the 95% Confidence Interval of age. How can you relate it to its p-value and statistical significance, which you found in the previous part? (2 points)\n\n\n1f)\nThis question requires some thinking, and bringing your 303-1 and 303-2 knowledge together.\nFit a simple linear regression model on lpsa against gleason and check the \\(p\\)-value of gleason using the summary. (2 point) Did the statistical significance of gleason change in the absence of other predictors? (1 point) Why or why not? (3 points)\nHints:\n\nYou need to compare this model with the Multiple Linear Regression model you created above.\nPrinting a correlation matrix of all the predictors should be useful.\n\n\n\n1g)\nPredict the lpsa of a 65 year old man with lcavol = 1.35, lweight = 3.65, lbph = 0.1, svi = 0.22, lcp = -0.18, gleason = 6.75, and pgg45 = 25. Find the 95% confidence and prediction intervals as well. (2 points)\n\n\n1h)\nIn the Multiple Linear Regression model with all the predictors, you should see a total of five predictors that appear to be statistically insignificant. Why is it not a good idea to directly conclude that all of them are statistically insignificant? (2 points) Implement the additional test that concludes the statistical insignificance of all five predictors. (2 points)\nHint: f_test() method",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html#multiple-linear-regression-with-variable-transformations-23-points",
    "href": "Assignment_2.html#multiple-linear-regression-with-variable-transformations-23-points",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "2) Multiple Linear Regression with Variable Transformations (23 points)",
    "text": "2) Multiple Linear Regression with Variable Transformations (23 points)\nThe infmort.csv file has the infant mortality data of different countries in the world. The mortality column represents the infant mortality rate with ‚Äúdeaths per 1000 infants‚Äù as the unit. The income column represents the per capita income in USD. The other columns should be self-explanatory. (This is an old dataset, as can be seen from some country names.)\n\n2a)\nStart your analysis by creating (i) a boxplot of log(mortality) for each region and (ii) a boxplot of income for each region. Note that the region column has the continent names. (3 points)\nNote: You need to use np.log, which is the natural log. This is to better distinguish the mortality values.\n\n\n2b)\nIn the previous part, you should see that Europe has the lowest infant mortality rate on average, but it also has the highest per capita income on average. Our goal is to see if Europe still has the lowest mortality rate if we remove the effect of income. We will try to find an answer for the rest of this question.\nCreate four scatter plots: (i) mortality against income, (ii) log(mortality) against income, (iii) mortality against log(income), and (iv) log(mortality) against log(income). (4 points)\nBased on the plots, create an appropriate model to predict the mortality rate as a function of per capita income. Print the model summary. (2 points)\n\n\n2c)\nUpdate the model you created in the previous part by adding region as a predictor. Print the model summary. (2 points)\n\n\n2d)\nUse the model developed in the previous part to compute a new adjusted_mortality variable for each observation in the data. (5 points) Adjusted mortality rate is the mortality rate after removing the estimated effect of income. You need to calculate it with the following steps:\n\nMultiply the (transformed) income column with its optimal coefficient. This is the estimated effect of income.\nSubtract the product from the (transformed) response column. This removes the estimated effect of income.\nYou may need to do a inverse transformation to calculate the actual adjusted mortality rate values.\n\nMake a boxplot of log(adjusted_mortality) for each region. (2 points)\n\n\n2e)\nUsing the plots in parts a and d, answer the following questions:\n\nDoes Europe still have the lowest mortality rate on average after removing the effect of income?\nHow did the distribution of values among different continents change after removing the effect of income? How did the comparison of different continents change? Does any non-European country have a lower mortality rate than all the European countries after removing the effect of income?\n\n(5 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html#variable-transformations-and-interactions-33-points",
    "href": "Assignment_2.html#variable-transformations-and-interactions-33-points",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "3) Variable Transformations and Interactions (33 points)",
    "text": "3) Variable Transformations and Interactions (33 points)\nThe soc_ind.csv dataset contains many social indicators of a number of countries. Each row is a country and each column is a social indicator. The column names should be clear on what the variables represent. The GDP per capita will be the response variable throughout this question.\n\n3a)\nUsing correlations, find out the most useful predictor for a simple linear regression model with gdpPerCapita as the response. You can ignore categorical variables for now. Let that predictor be \\(P\\). (2 points)\n\n\n3b)\nCreate a scatterplot of gdpPerCapita vs \\(P\\). Does the relationship between gdpPerCapita and \\(P\\) seem linear or non-linear? (2 points)\n\n\n3c)\nIf the relationship in the previous part is non-linear, create three models:\n\nOnly with \\(P\\)\nWith \\(P\\) and its quadratic term\nWith \\(P\\), its quadratic term and its cubic term\n\n(2x3 = 6 points)\nCompare the \\(R\\)-squared values of the models. (2 points)\n\n\n3d)\nOn the same figure:\n\ncreate the scatterplot in part b.\nplot the linear regression line (only using \\(P\\))\nplot the polynomial regression curve that includes the quadratic and cubic terms.\nadd a legend to distinguish the linear and cubic fits.\n\n(3 points)\n\n\n3e)\nDevelop a model to predict gdpPerCapita using \\(P\\) and continent as predictors. (No higher-order terms.)\n\nWhich continent creates the baseline? Write down its equation. (2 points)\nFor a given value of \\(P\\), are there any continents that do not have a statistically significant difference of predicted gdpPerCapita from the baseline continent? If yes, then which ones, and why? If no, then why not? You need to justify your answers for credit. (4 points)\n\n\n\n3f)\nThe model developed in the previous part has a limitation. It assumes that the increase in predicted gdpPerCapita with a unit increase in \\(P\\) does not depend on the continent.\nEliminate this limitation by including the interaction of continent with \\(P\\) in the model. Print the model summary of the model with interactions. (2 points) Which continent has the closest increase in predicted gdpPerCapita to the baseline continent with a unit increase in \\(P\\). Which continent has the furthest? You need to justify your answers for credit. (5 points)\n\n\n3g)\nUsing the model developed in the previous part, plot the regression lines of all the continents on the same figure. Put gdpPerCapita on the y-axis and \\(P\\) on the x-axis. (4 points) Use a legend to color-code the continents. (1 point)\nHint: For each continent, you need to construct the regression line using the appropriate coefficients from your interaction model. Remember that Africa is the baseline (reference) continent, so its line uses only the intercept and main lifeFemale coefficient. For other continents, you need to add their specific interaction terms to both the intercept and slope. Use np.linspace() to create x-values and plt.plot() for each continent with different colors and labels.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Assignment_2.html#prediction-with-sklearn-20-points",
    "href": "Assignment_2.html#prediction-with-sklearn-20-points",
    "title": "Appendix B ‚Äî Assignment 2: MLR",
    "section": "4) Prediction with Sklearn (20 points)",
    "text": "4) Prediction with Sklearn (20 points)\n\nInstructions\n\nRead the soc_ind.csv dataset and use the Index column as the index of the dataframe.\nDrop the geographic_location and country columns since they are not useful for our prediction.\nWe will use only sklearn and pandas libraries in this task.\ngdpPerCapita is the response (target) variable.\n(2 points)\n\nUse only lifeFemale and continent as predictors to match the tasks in the previous problem.\n(2 points)\nBuild an sklearn Pipeline that includes:\n\nA ColumnTransformer that applies PolynomialFeatures(degree=2, include_bias=False) to lifeFemale\nOne-hot encoding for continent\nA LinearRegression model (6 points)\n\nSplit the dataset into a training set (90%) and a test set (10%).\n\nSet random_state=3 for reproducibility.\n(3 points)\n\n\nFit the pipeline on the training set, then compute R-squared and RMSE on both the training set and the test set.\n(4 points)\n\nCompare training vs test R-squared and RMSE and discuss whether your model shows signs of overfitting or underfitting.\n(3 points)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Assignment 2: MLR</span>"
    ]
  },
  {
    "objectID": "Datasets.html",
    "href": "Datasets.html",
    "title": "Appendix C ‚Äî Datasets, assignment and project files",
    "section": "",
    "text": "Datasets used in the book, assignment files, project files, and prediction problems report tempate can be found here",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>¬† <span class='chapter-title'>Datasets, assignment and project files</span>"
    ]
  }
]